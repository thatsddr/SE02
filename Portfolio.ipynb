{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "diagnostic-function",
   "metadata": {},
   "source": [
    "# SE_02 Portfolio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "novel-intention",
   "metadata": {},
   "source": [
    "Created By: Domenico Di Ruocco"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arbitrary-basin",
   "metadata": {},
   "source": [
    "# Table of Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "returning-rebecca",
   "metadata": {},
   "source": [
    "- [Introduction to Theory](#introductionT)\n",
    "    - [Time Complexity](#timeComplexity)\n",
    "    - [Space Complexity](#spaceComplexity)\n",
    "    - [Asymptotic Notation](#asymptoticNotation)\n",
    "        - [Big O](#bigO)\n",
    "        - [Big Ω](#bigOmega)\n",
    "        - [Big Θ](#bigTheta)\n",
    "        - [Working with the Asymptotic Notation](#workingWithNotation)\n",
    "    - [Order of Dominance in the Asymptotic Limit](#orderOfDominance)\n",
    "- [Introduction to Data Structures](#introductionDS)\n",
    "    - [Arrays](#arrays)\n",
    "    - [Linked Lists](#linkedLists)\n",
    "    - [Stacks](#stacks)\n",
    "    - [Queues](#queues)\n",
    "    - [Hash Tables](#hashTables)\n",
    "    - [Trees](#trees)\n",
    "        - [Binary Trees](#binaryTrees)\n",
    "            - [Binary Search Trees](#binarySearchTrees)\n",
    "            - [Red-Black Trees](#redBlackTrees)\n",
    "            - [Ropes](#ropes)\n",
    "        - [Heaps](#heaps)\n",
    "    - [Graphs](#graphs)\n",
    "- [Introduction to Algorithms](#introductionA)\n",
    "    - [Searching Algorithms](#searching)\n",
    "        - [Linear Search](#linearSearch)\n",
    "        - [Binary Search](#binarySearch)\n",
    "    - [Sorting Algorithms](#sorting)\n",
    "        - [Selection Sort](#selectionSort)\n",
    "        - [Bubble Sort](#bubbleSort)\n",
    "        - [QuickSort](#quickSort)\n",
    "        - [MergeSort](#mergeSort)\n",
    "        - [HeapSort](#heapSort)\n",
    "        - [Counting Sort](#countingSort)\n",
    "    - [Graph Algorithms](#graphAlgorithms)\n",
    "        - [Depth-First Search](#dfs)\n",
    "        - [Breadth-First Search](#bfs)\n",
    "        - [Dijkstra's Algorithms](#dijkstras)\n",
    "- [Sources](#sources)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "anonymous-master",
   "metadata": {},
   "source": [
    "<div id=\"introductionT\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "flexible-airfare",
   "metadata": {},
   "source": [
    "# Introduction To Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coordinate-holly",
   "metadata": {},
   "source": [
    "In Computer Science, an algorithm is a set of instructions that must be followed in a fixed order to calculate an answer to a mathematical problem. Since it is common to find more than one algorithm that has been developed to solve the same problem, we need a way to analyze and compare them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alien-greene",
   "metadata": {},
   "source": [
    "<div id=\"timeComplexity\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "composed-tiger",
   "metadata": {},
   "source": [
    "## Time Complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "raised-studio",
   "metadata": {},
   "source": [
    "One way to compare two algorithms that solve the same problem, assuming that the solutions provided by both are correct, is to compare the time it takes each of them to get to the solution. The problem with this method is that it depends on the hardware were the algorithm runs. It is for this reason that for machine-independent algorithm design we consider our algorithm to be running on a hypothetical machine called the \"Random Access Machine\" or RAM.\n",
    "\n",
    "On the RAM, we consider each simple operation (+, *, -, =, if, call) and each memory access to take one time step, while loops and subroutines are considered to be the composition of many single-step operation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "basic-essence",
   "metadata": {},
   "source": [
    "<div id=\"spaceComplexity\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "naughty-tamil",
   "metadata": {},
   "source": [
    "## Space Complexity\n",
    "Another way to compare two algorithms is to compare the total space it takes them get to the solution. The total space includes the size of the input and auxiliary space, which is the extra or temporary space used by the algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "satellite-meditation",
   "metadata": {},
   "source": [
    "<div id=\"asymptoticNotation\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sought-luxury",
   "metadata": {},
   "source": [
    "## Asymptotic Notation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "engaging-privilege",
   "metadata": {},
   "source": [
    "We can use the RAM model to determine the number of steps it will take an algorithm to end with an input we choose, but estimating the worst, average, and best case runtime scenerio with the RAM model can be unconvenint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bizarre-liquid",
   "metadata": {},
   "outputs": [],
   "source": [
    "def even_numbers_avg(array):\n",
    "    '''\n",
    "    This function returns either the average of the sum of even numbers,\n",
    "    or None.\n",
    "    '''\n",
    "    even_sum = 0                    #1 time step\n",
    "    even_count = 0                  #1 time step\n",
    "                                    #n times:\n",
    "    for n in array:                    #1 time step\n",
    "        if n % 2 == 0:                 #1 time step\n",
    "            even_sum += n                  #1 time step\n",
    "            even_count +=1                 #1 time step\n",
    "            \n",
    "    if even_count > 0:              #1 time step\n",
    "        return even_sum/even_count     #1 time step\n",
    "    else:                           #1 time step\n",
    "        return None                    #1 time step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nutritional-transcription",
   "metadata": {},
   "source": [
    "In the example above, we can try to generalize the time complexity of this algorithm by counting every step, and we will find that in the worst case its time complexity will be: $T(n) = 5n + 6$. Its space complexity will be the size of the array n, plus the two variables we initialize."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extreme-hometown",
   "metadata": {},
   "source": [
    "The problem with the notation we used above is that is difficult to work precisely with it. In the example above we can see that both return statements have been counted as well as every step in the for loop, which is correct for the worst but not for the average and best case scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pending-banner",
   "metadata": {},
   "source": [
    "Since we can approximate an algorithm to a mathematical function, we can also determine its growth as a function of the input and define an upper bound function (Big O), a lower bound function (Big Ω), or both (Big Θ) in order to understand how it grows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "heard-defendant",
   "metadata": {},
   "source": [
    "In Asymptotic Notation, we only consider the fastest growing term without any multiplicative constant. E.g.: in the example above $T(n) = 5n + 6$ is $O(n)$ and not $O(5n)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ordinary-journey",
   "metadata": {},
   "source": [
    "<div id=\"bigO\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tamil-massachusetts",
   "metadata": {},
   "source": [
    "### Big O"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "constitutional-reform",
   "metadata": {},
   "source": [
    "The Big O of a function is its asymptotic upper bound. This means that the running time of a function $T$ will be always shorter than that of $f$. To generalize we can say that a funciton $T(n)$ is $O(f(n))$ if there is a constant $k$ such that $T(n) < k·f(n)$ for large enough $n$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "meaning-sculpture",
   "metadata": {},
   "source": [
    "<figure>\n",
    "    <img src=\"pics/Big O.png\" alt=\"Big O\"/>\n",
    "    <figcaption>\n",
    "        Big O. Image source: <a href=\"https://www.khanacademy.org/computing/computer-science/algorithms/asymptotic-notation/a/big-o-notation\">Khan Academy</a>\n",
    "    </figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "turned-basis",
   "metadata": {},
   "source": [
    "<div id=\"bigOmega\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "verified-drain",
   "metadata": {},
   "source": [
    "### Big Ω"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proprietary-short",
   "metadata": {},
   "source": [
    "The Big Ω of a function is its asymptotic lower bound. This means that the running time of a function $T$ will always be longer than that of $f$. To generalize we can say that a funciton $T(n)$ is $Ω(f(n))$ if there is a constant $k$ such that $T(n) > k·f(n)$ for large enough $n$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "middle-atmosphere",
   "metadata": {},
   "source": [
    "<figure>\n",
    "    <img src=\"pics/Big Omega.png\" alt=\"Big Omega\"/>\n",
    "    <figcaption>\n",
    "        Big Ω. Image source: <a href=\"https://www.khanacademy.org/computing/computer-science/algorithms/asymptotic-notation/a/big-big-omega-notation\">Khan Academy</a>\n",
    "    </figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "romantic-montreal",
   "metadata": {},
   "source": [
    "<div id=\"bigTheta\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "packed-anatomy",
   "metadata": {},
   "source": [
    "### Big Θ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lovely-gilbert",
   "metadata": {},
   "source": [
    "The Big Θ of a function is its asymptotic tight bound. This Means that the funciton always runs in a time comprised between the run time of the two asymptotic bounds. To generalize we can say that a funciton $T(n)$ is $Θ(f(n))$ if there are two constants $k_{1} and k_{2}$\n",
    "such that $T(n) ≥ k_{1}·f(n)$ and $T(n) ≤ k_{2}·f(n)$ for large enough $n$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "favorite-patrol",
   "metadata": {},
   "source": [
    "<figure>\n",
    "    <img src=\"pics/Big Theta.png\" alt=\"Big Theta\"/>\n",
    "    <figcaption>\n",
    "        Big Θ. Image source: <a href=\"https://www.khanacademy.org/computing/computer-science/algorithms/asymptotic-notation/a/big-big-theta-notation\">Khan Academy</a>\n",
    "    </figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "religious-search",
   "metadata": {},
   "source": [
    "<div id=\"workingWithNotation\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incoming-things",
   "metadata": {},
   "source": [
    "### Working with the Asymptotic Notation\n",
    "\n",
    "1. Addition\n",
    "\n",
    "    We can sum two functions together and the result will be the dominant one:\n",
    "    $f(n) + g(n) = Θ(max(f(n), g(n)))$\n",
    "2. Multiplication\n",
    "\n",
    "    Multiplying a function by a constant will result in the constant to be ignored. If instead we are multiplying two functions, we proceed as follows: $Θ(f(n))·Θ(g(n)) = Θ(f(n)·g(n))$\n",
    "    \n",
    "\n",
    "**The same rules also apply to Big O and Big Ω**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expected-government",
   "metadata": {},
   "source": [
    "<div id=\"orderOfDominance\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coastal-rocket",
   "metadata": {},
   "source": [
    "## Order of Dominance in the Asymptotic Limit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "patent-county",
   "metadata": {},
   "source": [
    "Let's consider some common asymptotic growths, valid for both space and time complexity:\n",
    "- Constant: $O(1)$\n",
    "- Logarithmic: $O(log(n))$\n",
    "- Linear: $O(n)$\n",
    "- Quasilinear: $O(n·log(n))$\n",
    "- Quadratic: $O(n^{2})$\n",
    "- Exponential: $O(2^{n})$\n",
    "- Factorial: $O(n!)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "toxic-vatican",
   "metadata": {},
   "source": [
    "To understand the difference between some of the most common time complexities, take a look at thi graph below, were the x-axys represents the size of the input of the functions and the y-axys represents the result of the functions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "permanent-start",
   "metadata": {},
   "source": [
    "<img src=\"pics/Comparison.png\" alt=\"Functions Compared\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "little-classroom",
   "metadata": {},
   "source": [
    "We can see that the dominance order of these functions is: $$n! >> 2^n >> n^2 >> n·logn >> n >> log(n) >> 1$$\n",
    "\n",
    "It is also clear that an efficient algorithm can really make the difference in terms of time and space efficiency, especially as the input size grows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "north-intellectual",
   "metadata": {},
   "source": [
    "<div id=\"introductionDS\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opponent-boundary",
   "metadata": {},
   "source": [
    "# Introduction to Data Structures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alien-hydrogen",
   "metadata": {},
   "source": [
    "Data Structures are constructs that allow you to store and manage data values and provide you with methods to access or manipulate such data. There are different kinds of data structures available, each with its pros and cons. It is important to understand the strengths and weaknesses of different Data Structures in order to choose the right one for your use case and make your software more efficient.\n",
    "\n",
    "Data structures can be classified in \"contiguos\" and \"linked\". The former are based upon arrays and the latter on pointers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sharp-johnston",
   "metadata": {},
   "source": [
    "<div id=\"arrays\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baking-annual",
   "metadata": {},
   "source": [
    "## Arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complex-atlantic",
   "metadata": {
    "tags": []
   },
   "source": [
    "An Array is an example of contiguos data structure. They are collections of data of fixed size allocated in contiguos memory locations, which make accessing the data values by index really efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "atlantic-sierra",
   "metadata": {},
   "source": [
    "<figure>\n",
    "    <img src=\"pics/Array.png\" alt=\"Example of array in memory\" width=\"450\"/>\n",
    "    <figcaption>\n",
    "        Array. Image source: <a href=\"https://www.geeksforgeeks.org/array-data-structure\">Geeks for Geeks</a>\n",
    "    </figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "micro-throw",
   "metadata": {},
   "source": [
    "Analysis of common operations on arrays:\n",
    "\n",
    "- Access:\n",
    "\n",
    "    Since the values are indexed and this is a data structure contiguos in memory it is possible to access data in the array with a worst-case time complexity of $O(1)$ (constant time complexity);\n",
    "- Search:\n",
    "\n",
    "    The data stored in an array is not always sorted, so we need to assume that every memory slot could be searched before finding the element we are looking for, therefore the time complexity of this operation is $O(n)$ (linear time complexity);\n",
    "- Insertion:\n",
    "\n",
    "    To insert an element at a specific index we might, in the worst case scenerio, need to shift all the other elements in the array, so the time complexity for this operation will be $O(n)$;\n",
    "- Deletion:\n",
    "\n",
    "    To delete an element in an array we may run into the same issue of insertion, hence the time complexity will be, again, $O(n)$.\n",
    "\n",
    "The space efficiency of arrays is also one of its major strengths, since arrays are only made up of pure data, no space is wasted.\n",
    "\n",
    "Another advantage of arrays is their memory locality, which takes full advantage of the speed of cahce memory.\n",
    "\n",
    "Their main disadvantage is the impossibility of changing their size while the program is running. It is however possible to avoid this limitation using dynamic arrays, arrays whose size doubles every time they are full.\n",
    "\n",
    "Arrays are very common data structures, and they can be used to store basically every kind of data. Other data structures, like Hash Tables, Graphs, and Heaps, are also based on them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "virtual-receipt",
   "metadata": {},
   "source": [
    "<div id=\"linkedLists\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "valued-czech",
   "metadata": {},
   "source": [
    "## Linked Lists"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interested-excerpt",
   "metadata": {},
   "source": [
    "Linked lists are an example of linked data structures. They are collections of data not allocated in contiguos memory locations but in which the elements are linked using a pointer to the next value in the case of a Singly Linked List or to both the previous and the next in the case of a Doubly Linked List.\n",
    "Linked lists are made up of a \"Nodes\", the first one of which is called Head. Every node has a pointer that points to the next node (or to a null value in case it is the last element), while in case they are doubly linked list they also have a pointer to the previous value (or to a null value in case of the first element)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jewish-friday",
   "metadata": {},
   "source": [
    "<figure>\n",
    "    <img src=\"pics/LinkedList.png\" alt=\"Representation of singly linked lists\" width=\"500\"/>\n",
    "    <figcaption>\n",
    "        Singly Linked List. Image source: <a href=\"https://www.geeksforgeeks.org/data-structures/linked-list/\">Geeks for Geeks</a>\n",
    "    </figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "virtual-drawing",
   "metadata": {},
   "source": [
    "<figure>\n",
    "    <img src=\"pics/DoublyLinkedList.png\" alt=\"Representation of doubly linked lists\" width=\"500\"/>\n",
    "    <figcaption>\n",
    "        Doubly Linked List. Image source: <a href=\"https://www.geeksforgeeks.org/doubly-linked-list/\">Geeks for Geeks</a>\n",
    "    </figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "another-admission",
   "metadata": {},
   "source": [
    "Analysis of common operations on linked lists:\n",
    "\n",
    "- Access:\n",
    "\n",
    "    Linked Lists (both Singly and Dobly Linked) are not indexed data structures. This mean that we cannot access one value directly, but we need to start from the Head (or also from the last element if we are in a Doubly Linked List) and follow the pointers until we find the element we are looking for or we find a null value. For this reason, the time complexity of this operation is $O(n)$;\n",
    "- Search:\n",
    "\n",
    "    The same concept from the access operation applies here;\n",
    "- Insertion:\n",
    "\n",
    "    To insert a new node \"B\" in a Singly Linked List, between two nodes \"A\" and \"C\", we just need to make sure that the node \"A\" points to node \"B\" and that node \"B\" points to node \"C\". If the list is a Doubly Linked one, we also need to make sure that the backward pointer of node \"C\" and \"B\" is set correctly. \n",
    "    We can also add an element at the beginning of a Linked List by simply making it the new head and set its pointer to the previous head, and in case it is a Doubly Linked List we also need to point the backward pointer of the previous Head to the new Head. The time complexity of this operation, after you know the position where you want to insert a new element is $O(1)$, since you only need to change the value of the pointer(s);\n",
    "- Deletion:\n",
    "\n",
    "    The same concept from the insertion operation applies here, except that insteaf of changing the pointer(s) to include a new element we change them to exclude one. For this reason the time complexity of this operation is also $O(1)$.\n",
    "\n",
    "Linked lists are not really space efficient since they need to store the pointers and the extra space needed will be $O(n)$.\n",
    "\n",
    "As we have seen, their main disadvantages are that they are slow in searching and occupy more memory than arrays. Moreover, another disadvantage is that they don't benefit from the speed of cache memory since they are not stored contiguously.\n",
    "\n",
    "Lists, just like arrays, are also very common data structures. They are mainly used where dynamic memory allocation is required. Trees are an example of data structure implemented using a modified version of linked lists."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "manual-detection",
   "metadata": {},
   "source": [
    "<div id=\"stacks\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complex-violence",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Stacks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comfortable-sixth",
   "metadata": {},
   "source": [
    "A Stack is a linear data structure. They allow two operations: insertion at the top (push) and read and removal at the top (pop). For this reason stacks follow the \"Last In First Out\" or \"LIFO\" order."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "physical-vocabulary",
   "metadata": {},
   "source": [
    "<figure>\n",
    "    <img src=\"pics/Stack.png\" alt=\"Representation of a stack\" width=\"500\"/>\n",
    "    <figcaption>\n",
    "        Stacks. Image source: <a href=\"https://www.geeksforgeeks.org/stack-data-structure/\">Geeks for Geeks</a>\n",
    "    </figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "supreme-finance",
   "metadata": {},
   "source": [
    "Analysis of common operations on Stacks:\n",
    "    \n",
    "- Access:\n",
    "\n",
    "    Access an element in a Stack entails that we need to read and remove the top item until we reach the one we're looking for or we are left with an empty Stack. Because of this, the time complexity of this operation will depend on its size and will therefore be $O(n)$;\n",
    "- Search:\n",
    "\n",
    "    The search operation in a Stack is the same as the access operation.\n",
    "- Insertion:\n",
    "\n",
    "    We can only insert an element at the top of the Stack, and for this reason this operation will have a time complexity of $O(1)$ (constant time);\n",
    "- Deletion:\n",
    "\n",
    "    Just like insertion, we can only delete the top element of a Stack and thus the time complexity of this operation will also be $O(1)$.\n",
    "    \n",
    "\n",
    "Depending on how Stacks are implemented they can be more or less space efficient.\n",
    "\n",
    "Their main advantage is that they are easy to implement and that that the insertion and deletion operations are really time efficient.\n",
    "\n",
    "Uses of Stacks include situations in which the order in which the elements are inserted/deleted is not important, or when you specifically need to retrieve the last elements first. An example of implementation could be a social media feed or a chat app, because in both cases you need to retrieve the latest information first."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "federal-cookie",
   "metadata": {},
   "source": [
    "<div id=\"queues\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cardiac-sport",
   "metadata": {},
   "source": [
    "## Queues"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "emerging-operator",
   "metadata": {},
   "source": [
    "A Queue is a linear data structure similar to the Stack but that supports a different set of operations, Enqueue (inserting at item at the rear of the queue) and Dequeue (reading and deleting an item from the fron of the queue). Instead of the LIFO, Queues follow the FIFO order (First In, First Out)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "documentary-ireland",
   "metadata": {},
   "source": [
    "<figure>\n",
    "    <img src=\"pics/Queue.png\" alt=\"Representation of a queue\" width=\"500\"/>\n",
    "    <figcaption>\n",
    "        Queues. Image source: <a href=\"https://www.geeksforgeeks.org/queue-data-structure/\">Geeks for Geeks</a>\n",
    "    </figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "final-drunk",
   "metadata": {},
   "source": [
    "Analysis of common operations on Queues:\n",
    "    \n",
    "- Access:\n",
    "\n",
    "    Access an element in a Queue is similar to the access operation in a Stack, we need to read and remove (dequeue) the front item until we find the one we are looking for or no elements are left in the Queue. The time complexity of this operation will henve be $O(n)$;\n",
    "- Search:\n",
    "\n",
    "    The search operation in a Queue is the same as the access operation.\n",
    "- Insertion:\n",
    "\n",
    "    We can only insert an element at the rear of the Queue, and for this reason this operation will have a time complexity of $O(1)$ (constant time);\n",
    "- Deletion:\n",
    "\n",
    "    We can only delete the front element of a Queue and thus the time complexity of this operation will also be $O(1)$.\n",
    "\n",
    "The space complexity of stacks also depends on how they are implemented.\n",
    "\n",
    "Just like Stacks they are also easy to implement and that the insertion and deletion operations are really time efficient.\n",
    "\n",
    "Queues are useful in situations in which the order in which the elements are retrieved matters. An example of implementation is every service that needs to handle the users' requests in the order that they were made (like for online payments)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sharp-bearing",
   "metadata": {},
   "source": [
    "<div id=\"hashTables\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "senior-triumph",
   "metadata": {},
   "source": [
    "## Hash Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tribal-vacation",
   "metadata": {},
   "source": [
    "A Hash Table is data structure in which a key-value data pair is stored. It is usually implemented with an array and it works by hashing (generating an unique value, integer in this case) the key and storing the kay-value data at the index returned by the hash function. In this way, given the key, it is really efficient to locate its value.\n",
    "\n",
    "The main challenge of Hash Tables is to remain memory efficient while avoiding collisions (having 2 or more elements at the same index)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indirect-species",
   "metadata": {},
   "source": [
    "<figure>\n",
    "    <img src=\"pics/HashTable.png\" alt=\"Representation of a hash table\" width=\"400\"/>\n",
    "    <figcaption>\n",
    "        Hash Table. Image source: <a href=\"https://en.wikipedia.org/wiki/Hash_table#/media/File:Hash_table_3_1_1_0_1_0_0_SP.svg/\">Wikipedia</a>\n",
    "    </figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enhanced-nightmare",
   "metadata": {},
   "source": [
    "Analysis of common operations on Hash Tables:\n",
    "\n",
    "- Search / Access:\n",
    "\n",
    "    Since the data stored in a Hash Table is indexed, it will take constant time to search or access it ($O(1)$), even if depending on the hashing algorithm, it could depend on the size of the key. Other edge cases include collision, and we will take such situation into acccount later;\n",
    "\n",
    "- Insertion:\n",
    "\n",
    "    To insert an element in a Hash table, we just need to execute the hash function and insert the data in the array, which happens, depending on the hashing funciton, wither with a time complexity of $O(1)$ (constant time), or with a time complexity dependent on the key size. This can change in case of collisions;\n",
    "- Deletion:\n",
    "\n",
    "    Deleting an element in a Hash Table is a process similar to searching for it, except that insted of reading it, it gets deleted. Normally this process happend with a time complexity of $O(1)$, but also this operation can be slowed down by collisions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arctic-joint",
   "metadata": {},
   "source": [
    "**Collisions** are more frequent as the array fills up because the empty slots are less. It is common to use the variable $a=n/m$ (where $n$ is the number of elements and $m$ the length of the array) to measure how full is an array. Once a collision happen there are different ways to deal with it:\n",
    "- Chaining:\n",
    "\n",
    "    We add more than 1 key value pair to the same index. In this way inserting a new element has always a time complexity of $O(1)$ while searching for an element a time complexity of $(O(1+a))$;\n",
    "    \n",
    "- Open Addressing:\n",
    "\n",
    "    We store the elements in same array without using additional data structures. Ways to find a new index include:\n",
    "    \n",
    "    - Linear Probing:\n",
    "\n",
    "        We store the colliding element in the first available slot in the array. This method, however, can be really inefficient. Inserting and searching for an element can have a time complexity of $O(n)$;\n",
    "    - Quadratic Probing:\n",
    "        \n",
    "        We find a new index by adding an arbitrary number that increases quadratically. Just like Linear probing, this method can be really inefficient with inserting and sorting operations that can have a time complexity of $O(n)$;\n",
    "    - Double Hashing:\n",
    "\n",
    "        We generate a new hash if a collision is detected. We can chose between different function to implement this technique, but generally this method is more time efficient compared to the other 2, especially in searching."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fixed-force",
   "metadata": {},
   "source": [
    "Hash Table are not really space efficient.\n",
    "\n",
    "Their main advantage is the efficienty with which insert,\n",
    "\n",
    "Hash Tables are usually implemented with built-in data structures like python dictionaries and can be used for a variety of things, from the implementation of an actual dictionary to entries of a NoSQL database."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fixed-import",
   "metadata": {},
   "source": [
    "<div id=\"trees\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stopped-necklace",
   "metadata": {},
   "source": [
    "## Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "optical-gates",
   "metadata": {},
   "source": [
    "Trees are non linear data structures that can be considered an extension of a linked list. In a Tree, each node points to some \"children\" nodes or a null value, creating an hierarchical data structure.\n",
    "\n",
    "Trees have a lot of properties, understand them we are going to take into consideration the Binary Tree in th eimage below (a Binary Tree is a Tree in which each node has at most 2 children)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "technical-degree",
   "metadata": {},
   "source": [
    "<figure>\n",
    "    <img src=\"pics/BinaryTree.png\" alt=\"Representation of a binary tree\" width=\"500\"/>\n",
    "    <figcaption>\n",
    "        Binary Tree. Image source: <a href=\"https://www.geeksforgeeks.org/binary-tree-data-structure//\">GeeksforGeeks</a>\n",
    "    </figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "frequent-container",
   "metadata": {},
   "source": [
    "**Properties and Terminology of Trees:**\n",
    "\n",
    "- **Node**: an element of the Tree, contains data and pointers to its children;\n",
    "- **Edge**: The \"link\" between 2 nodes, every Tree has a maximum of $N-1$ edges, where $N$ is the number of nodes;\n",
    "- **Parent Node**: the predecessor of a node, or the node that points to another. In the example above, among the others, \"1\" is the parent of \"2\" and \"3\", and \"2\" is the parent of \"4\" and \"5\";\n",
    "- **Child node**: the descendant of a node. In the example above, among the others, \"11\" is a child of \"2\" and \"3\" is a child of \"1\";\n",
    "- **Root Node**: The first element of the Tree and the only node without a parent node, in the example above the node is \"1\";\n",
    "- **Siblings Nodes**: nodes that are children of the same parent node. In the example above, \"4\" and \"5\" and \"8\" and \"9\" are examples of siblings;\n",
    "- **Leaf**: a node without children, like \"11\" or \"14\" in the example above.\n",
    "- **Internal Node**: a node with at least 1 child. \"7\", \"4\" and \"1\" are internal nodes in the example above;\n",
    "- **Degree**: the nummber of children that a node has. In a binary tree, this number is never greater than 2. The \"Degree of Tree\" is the Degree of the node with the highest Degree;\n",
    "- **Level**: the \"distance\" of a Node from the Root Node, starting at 1. In the example above, the level of \"1\" is 1, the degree of \"3\" is 2, the degree of \"5\" is 3 and the degree of \"9\" is 4;\n",
    "- **Height**: is the \"distance\" between the futhest descending leaf and a node, starting at 0 for the leaves. In the example above, the height of \" 10\" is 0, the height of \"4\" is 1, the height of \"3\" is 2 and the Height of \"1\" is \"4\". The Height of the Root Node is also the Height of Tree.\n",
    "- **Depth**: the number of edges between a Node and the Root Node. For example, it is 0 for \"1\" and 2 for \"7\" in the Tree above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excited-interference",
   "metadata": {},
   "source": [
    "Analysis of common operations on trees:\n",
    "\n",
    "- Access:\n",
    "    \n",
    "    Since a Tree is not an indexed data structure, in the worst case it is possible that we need to look trough all the nodes until we find the one we are looking for. For this reason the time complexity of this operation is $O(n)$;\n",
    "- Search:\n",
    "    \n",
    "    Trees are not ordered data structures, or at least not all of them. For this reason searching in a tree could also mean that we need to search trough all teh other nodes, either with a Depth-First-Search or a Breadth-First-Search approach (we will look at both of these algorithms in the algorithms section of this portfolio) with a time complexity that in both cases is of $O(n)$;\n",
    "- Insertion:\n",
    "    \n",
    "    Inserting a Node in a tree may require changing the positions of the other nodes as well to keep the properties of the tree. For this reason this operation also has a time complexity of $O(n)$;\n",
    "- Deletion:\n",
    "\n",
    "    Deletion, just like insertion, may require to rearrange all the other Nodes and so this operation also happens in $O(n)$ time complexity.\n",
    "\n",
    "The space complexity of trees is $O(n)$, since they need to store a number of pointers which grows linearly with the number of nodes.\n",
    "\n",
    "Given that there are a lot of subcategories of trees, they can be used in a lot of different ways. We will now look at some specific types of trees and discuss the real world implementations of each."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "promising-wagner",
   "metadata": {},
   "source": [
    "<div id=\"binaryTrees\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "variable-mission",
   "metadata": {},
   "source": [
    "### Binary Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "closing-charlotte",
   "metadata": {},
   "source": [
    "As I mentioned before, a Binary Tree is a Tree in which is node can have a maximum of 2 children, therefore each node cointains some data, a pointer to its left child and a pointer to its right child."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "successful-portal",
   "metadata": {},
   "source": [
    "A binary tree is not an ordered tree by definition, so the time complexity of common operations is the same as that of a normal tree.\n",
    "\n",
    "There are, however, a lot of different implementations of Binary Trees that make them more efficient in those common operations:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "naked-delight",
   "metadata": {},
   "source": [
    "<div id=\"binarySearchTrees\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tested-architecture",
   "metadata": {},
   "source": [
    "#### Binary Search Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spectacular-hollow",
   "metadata": {},
   "source": [
    "Binary Search Trees (BST) are Binary Trees in which the left child of a node (and all of its children) have a value smaller than that of the parent node and the right child of a node (and all of its children) have a value bigger than that of the parent node. As the name suggests, BSTs are used to implement the  Binary Search algorithm on them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confused-lewis",
   "metadata": {},
   "source": [
    "<figure>\n",
    "    <img src=\"pics/BST.png\" alt=\"Representation of a binary search tree\" width=\"400\"/>\n",
    "    <figcaption>\n",
    "        Binary Tree. Image source: <a href=\"https://www.geeksforgeeks.org/binary-search-tree-data-structure/\">GeeksforGeeks</a>\n",
    "    </figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alpha-ethnic",
   "metadata": {},
   "source": [
    "Analysis of common operations on BSTs:\n",
    "\n",
    "- Search / Access:\n",
    "    \n",
    "    Binary Search Trees are not indexed data structure, so to access an element we need to search for it. BSTs are ordered data structures that work really well with the Binary Search algorithm (an analysis of this algorithm can be found in the algorithms part of this portfolio). Because of the fact that we can use Binary Search with this data structure, we can find an element with a time complexity of $O(h)$, where $h$ is also the height of the tree;\n",
    "    \n",
    "- Insertion:\n",
    "    \n",
    "    Inserting a Node in a BST may require us to change the order of all the other nodes, so in the worst case scenario the time complexity of this operation will be $O(n)$, but in the average case the time complexity of this operation will depend on the height of the tree, so $Θ(h)$;\n",
    "- Deletion:\n",
    "\n",
    "    Deleting a Node has the same impact as inserting a Node. the time complexity of this operation will therefore be $Θ(h)$ in the average case and $O(n)$ in the worst.\n",
    "    \n",
    "An implementation of a BST could be using it together with a binary search algorithm.\n",
    "\n",
    "The main problems with BSTs is that they can be unbalanced when the nodes skew to one of the sides of the tree. In that case the height of the tree is $n$ and the operations in the tree become inefficient. To avoid this and have an height of $log(n)$ we can use a self balancing Binary Tree."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "floating-shame",
   "metadata": {},
   "source": [
    "<div id=\"redBlackTrees\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "owned-shelter",
   "metadata": {},
   "source": [
    "#### Red-Black Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dynamic-demand",
   "metadata": {},
   "source": [
    "Red-Black trees are a common kind of self-balancing Binary Trees in which: \n",
    "- Each node has a color property (either red or black); \n",
    "- The root is always black; \n",
    "- A Node cannot have the parent or children of its the same color, except if one or both of its children are leaves; \n",
    "- Its leaves have a null value and are considered black; \n",
    "- And in every path from a node to any of its null descendants contains the same number of black nodes.\n",
    "\n",
    "Red-Black Trees, just like BSTs, are used to implement the Binary Search algorithm on them, but these are usually more efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "touched-shipping",
   "metadata": {},
   "source": [
    "<figure>\n",
    "    <img src=\"pics/RedBlack.png\" alt=\"Representation of a red black tree\" width=\"500\"/>\n",
    "    <figcaption>\n",
    "        Red Black Tree. Image source: <a href=\"https://en.wikipedia.org/wiki/Red–black_tree#/media/File:Red-black_tree_example.svg\">Wikipedia</a>\n",
    "    </figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tropical-stone",
   "metadata": {},
   "source": [
    "Analysis of common operations on Red-Black Trees:\n",
    "\n",
    "- Search / Access:\n",
    "    \n",
    "    It is possible to perform a Binary Search on a Red-Black Tree, which as we have seen before allows us to search an element with a time complexity of $O(h)$. Since Red-Black trees are a balanced data structure, $h$ will be $log(n)$ and therefore this operation will happen with a time complexity of $O(log(n))$;\n",
    "    \n",
    "- Insertion:\n",
    "    \n",
    "    Since a Red Black Tree is a balanced Tree, and the insertion operation depends on the height of the tree, we can say that this operation will have a worst-case time complexity of $O(log(n))$;\n",
    "- Deletion:\n",
    "\n",
    "    Deleting a node can have the same impact as inserting a node, and this operation depends on the height of the Tree too. Since the height of a Red Black tree is $O(log(n))$, this operation will have a time complexity of $O(log(n))$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rough-scheduling",
   "metadata": {},
   "source": [
    "<div id=\"ropes\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "modified-feature",
   "metadata": {},
   "source": [
    "#### Ropes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vietnamese-search",
   "metadata": {},
   "source": [
    "A Rope is a Binary Tree used for string manipulation. In a rope, each leaf holds a substring and each inner node the total length of the substrings that are descendants of its left child.\n",
    "\n",
    "In the example below, the root node has as value the total length of the string, which is not a mandatory feature but can be useful in common operations, as we will see below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "together-alarm",
   "metadata": {},
   "source": [
    "<figure>\n",
    "    <img src=\"pics/Rope.jpg\" alt=\"Representation of a rope\" width=\"500\"/>\n",
    "    <figcaption>\n",
    "        Rope. Image source: <a href=\"https://www.geeksforgeeks.org/ropes-data-structure-fast-string-concatenation/\">GeeksForGeeks</a>\n",
    "    </figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brave-budapest",
   "metadata": {},
   "source": [
    "Common operations that can be done on a rope are different than those done on other trees. These operations include:\n",
    "\n",
    "- Index:\n",
    "\n",
    "    Searching an element by their index is a very common operation in string manipulation, and thanks to the value of the inner nodes this operation can be done on Ropes with time complexity of $O(log(n))$. \n",
    "    \n",
    "    To understand how that is possible let's consider an example, finding the character with at the position i=8 in the example above. We start by comparing i to the value of the root (which in this case holds the value of the total length), and we quickly determine if the index is part of the string. Since is smaller than the value of A, we move to A's left child (B). we compare i to the value of B and since 8 is smaller than 9 we move to B's left child. We compare i to the value of C (6) and since 8 is bigger, we move to  C's right child (F) and since we're moving to the right we update i to b i-C (8-6 = 2), and since F is a leaf we access the child at position i (which is now 2) of the substring (y), which is the 8th character of the whole string.\n",
    "\n",
    "- Concat\n",
    "\n",
    "    To concatenate 2 Ropes we just need to assign them to a new common root node with the value equal to the sum of the length of the substrings that descend from its new left child. This operation can be made in $O(1)$ time complexity, but computing the value for the new root node is an operation that has a time complexity of $O(log(n))$;\n",
    "\n",
    "- Split\n",
    "\n",
    "    When splitting a string starting from an index we need to make a distinction between 2 major cases: we need to start splitting after the last character of a leaf, or we need to start splitting starting from a middle character of a leaf. If our case is the latter, we assign 2 children to the leaf (which becomes an inner node), the left one containing the character that we don't need to split, and the right one containing the characters that we need to split. \n",
    "    \n",
    "    After finding the leaf from which we need to split the Rope, we separate the nodes at the right of that leaf and we fix the weight of the inner nodes that were ancestors of the nodes we separated. We then assign the split nodes to a new common root node.\n",
    "    \n",
    "    At this point, it may be necessary to rebalance both Ropes. \n",
    "    \n",
    "    This operation has a time complexity of $O(log(n))$ since it is the sum of the time complexities of the operations that are needed to complete this operation;\n",
    "\n",
    "- Insert\n",
    "\n",
    "    Inserting a Rope in the middle of another Rope is an operation that can be done by splitting the original Rope, concatenate the Rope we need to insert, and then concatenate the right part of the node we originally split. Rebalancing the tree may be also required. The time complexity of this operation will be the sum of the time complexities of 1 split operation and 2 concatenation operations $(O(log(n))$;\n",
    "\n",
    "- Delete\n",
    "\n",
    "    To delete a substring at the middle of a Rope, we need to split the original rope starting at the first character that we want to delete. We then split the resulting right Rope starting after the last character that we need to delete, and we finally concatenate the left rope of the first split operation with the right Rope of the last split operation. This operation will also have a time complexity given by the sum of the operations that it uses, which will result in a time complexity of $O(n)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ranking-segment",
   "metadata": {},
   "source": [
    "Ropes are widely used in text editors and email clients because of their performances in managing strings, especially compared with a traditional string implementation (an array of characters that also requires continuos memory allocation)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "absent-senegal",
   "metadata": {},
   "source": [
    "<div id=\"heaps\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "editorial-execution",
   "metadata": {},
   "source": [
    "### Heaps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "occasional-calvin",
   "metadata": {},
   "source": [
    "Heaps are trees in which the parent node always stores a value smaller than that of its children (in the case of a Min Heap) or bigger than that of its children (in case of a Max Heap). In this way the root node always stores the smallest value (in a Min Heap) or the biggest value (in a Max Heap).\n",
    "\n",
    "Heaps do not need to be Binary Trees, but they need to be complete (every level should have the maximum amount of nodes) andif they are not, new elements are added to the incomplete lavel from left to right. Because of this last property, Heaps are usually stored as arrays."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cross-vampire",
   "metadata": {},
   "source": [
    "<figure>\n",
    "    <img src=\"pics/Heap.png\" alt=\"Representation of a heap\" width=\"500\"/>\n",
    "    <figcaption>\n",
    "        Heap. Image source: <a href=\"https://www.geeksforgeeks.org/heap-data-structure\">GeeksForGeeks</a>\n",
    "    </figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cooperative-boulder",
   "metadata": {},
   "source": [
    "Analysis of common operations on Heaps:\n",
    "\n",
    "- Search / Access:\n",
    "\n",
    "    Searching an element that is not the root node (the node with the max value in a Max Heap or the node with a min value in a Min Heap), we may need to search through all the nodes to find the one we're looking for. Because of this, this operation will have a time complexity of $O(n)$;\n",
    "- Insertion:\n",
    "    \n",
    "    \n",
    "   To insert an element in its correct position in a Heap we need to start by appending it to the last level, which can be done with an average time complexity of $O(1)$ (if the heap is stored in an array). We then need to switch it with its parent node (in case the parent node is smaller and our heap is a Max Heap or in case the parent node is bigger and our Heap is a Min Heap) until it satisfies the properties of the heap. This second operation has a time complexity of $O(log(n))$;\n",
    "- Deletion:\n",
    "   \n",
    "   \n",
    "   To delete an element from a Heap we may also need to rearrange its nodes until the properties of the heap are satisfied. To do this we may need to switch an element for every level of the Heap and since the number of levels is given by $log(n)$ the time complexity of this operation will be $O(log(n))$.\n",
    "   \n",
    "\n",
    "Heaps are mainly used as an auxiliary data structure in various algorithm, like the Heapsort algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "passive-support",
   "metadata": {},
   "source": [
    "<div id=\"graphs\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "divine-header",
   "metadata": {},
   "source": [
    "## Graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alpine-sender",
   "metadata": {},
   "source": [
    "Graphs are non-linear data structures made of vertices (or nodes) that store data and edges (that can also store data). Graphs are used to represent the relationships between its nodes or vertices. We have already examined a subset of Graphs, Trees."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interstate-destiny",
   "metadata": {},
   "source": [
    "<figure>\n",
    "    <img src=\"pics/Graph.png\" alt=\"Representation of a graph\" width=\"500\"/>\n",
    "    <figcaption>\n",
    "        Graph. Image source: <a href=\"https://www.geeksforgeeks.org/graph-data-structure-and-algorithms/\">GeeksForGeeks</a>\n",
    "    </figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "weird-cabin",
   "metadata": {},
   "source": [
    "Graphs terminology:\n",
    "\n",
    "- **Vertex** (or Node), an element of the graph that always contains some data;\n",
    "- **Edge**, the relationship between 2 vertices, can also contain information;\n",
    "- **Adjacency**, two nodes connected via an edge;\n",
    "- **Path**, a sequence of edges between 2 vertices;\n",
    "- **Eulerian Path**, a path that visits every edge once (but can visit vertices more than once) and ends up in a vertex which is not the starting one;\n",
    "- **Eulerian Cycle**: a path that visits every edge once (but can visit vertices more than once) and ends up in the starting vertex;\n",
    "- **Hamiltonian Path**, a path that visits every vertex only once (but can visit edges more than once) and ends up in a vertex which is not the starting one;\n",
    "- **Hamiltonina Cycle**: a path that visits every vertex only once (but can visit edges more than once) and ends up in the starting vertex;\n",
    "- **Parallel Edges**, two or more edges that connect the same vertices;\n",
    "- **Loop**, an edge that connect a node to itself."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "associate-guide",
   "metadata": {},
   "source": [
    "Types of Graphs:\n",
    "\n",
    "- **Finite**. A finite Graph contains a finite number of edges and vertices;\n",
    "- **Infinite**. An infinite Graph contains an infinite number of vertices and edges;\n",
    "- **Trivial**. A trivial Graph contains only one vertex and no edges;\n",
    "- **Simple**. A simple Graph contains only one edge between a pair of vertices;\n",
    "- **Non Simple**. A non simple Graph contains more than one edge between a pair of vertices;\n",
    "- **Multi-Graph**. A multi-graph contains some parallel edges but no loops;\n",
    "- **Pseudo-Graph**. a pseudo-graph is a graph with at least a loop and a parallel edge;\n",
    "- **Null**. A null graph contains vertices but no edges;\n",
    "- **Complete** (or Full Graph). In a complete graph every vertex is adjacent to all the others;\n",
    "- **Unweighted**. In an unweighted graph the edges do not store data;\n",
    "- **Weighted**. In a weighted graph the edges store data;\n",
    "- **Directed**. In a directed graph the edges connect 2 vertices only in one direction;\n",
    "- **Undirected**. In an undirected graph the edges connect 2 vertices in both directions;\n",
    "- **Topological**. In a Topological Graphs, the vertices are represented by distinct points in space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "homeless-career",
   "metadata": {},
   "source": [
    "Ways of representing a graph:\n",
    "\n",
    "- **Adjacency List:**\n",
    "\n",
    "    With an Adjacency List, we use an array to store information about the graph. In a Adjacency List, the array element with the same index as the id of a vertex contains information about its adjacent vertices. An Adjacency List for the graph in the image above will look like this: ```[[1,4],[0,2,3,4],[1,3],[1,2,4],[0,1,3]]```.\n",
    "    \n",
    "    Analysis of common operations on Adjacency Lists:\n",
    "    \n",
    "    - Storage:\n",
    "    \n",
    "        Storing a graph as an Adjacency List can be done with a time complexity of $O(|V| + |E|)$, where $V$ is the number of vertices and also the length of the array, and $E$ is the number of edges;\n",
    "        \n",
    "    - Add Vertex:\n",
    "    \n",
    "        Adding a vertex to a graph represented as an Adjacency List can be done with a time complexity of $O(1)$ since we just need to store a new element to the list;\n",
    "    \n",
    "    - Add Edge:\n",
    "    \n",
    "        Adding an edge to a graph represented as an Adjacency List can be done with a time complexity of $O(1)$, since we just need to add to the arrays representing the two nodes 1 value;\n",
    "    \n",
    "    - Remove Vertex:\n",
    "    \n",
    "        Removing a vertex from a graph represented as an Adjacency List can be done with a time complexity of $O(|V|+|E|)$, since we need to remove the edges to that vertex from all the other vertices as well;\n",
    "    \n",
    "    - Remove Edge:\n",
    "    \n",
    "        Removing an edge from a graph represented as an Adjacency List can be done with a time complexity of $O(|E|)$, since we need to search and remove the edge from the list of edges of the two nodes that it connects.\n",
    "    \n",
    "    \n",
    "    The space complexity of an Adjacency List is $O(|V|+|E|)$.\n",
    "    \n",
    "    \n",
    "- **Adjacency Matrix:**\n",
    "\n",
    "    With an Adjacency Matrix, we use a 2D matrix to store information about the graph. In a Adjacency Matrix, the array element with the same index as the id of a vertex contains an array that indicates if a vertex is connected to another or not (1 if it is, 0 if it is not). An Adjacency Matrix for the graph in the image above will look like this:\n",
    "    \n",
    "    ```\n",
    "    [\n",
    "    [0,1,0,0,1],\n",
    "    [1,0,1,1,1],\n",
    "    [0,1,0,1,0],\n",
    "    [0,1,1,0,1],\n",
    "    [1,1,0,1,0]\n",
    "    ]\n",
    "    ```\n",
    "    \n",
    "    Analysis of common operations on Adjacency Matrices:\n",
    "    \n",
    "    - Storage:\n",
    "    \n",
    "        Storing a graph as an Adjacency matrix can be done with a time complexity of $O(|V|^{2})$, where $V$ is the number of vertices, the number of arrays in te matrix, and the length of each array;\n",
    "\n",
    "    - Add Vertex:\n",
    "    \n",
    "        Adding a vertex to a graph represented as an Adjacency Matrix can be done with a time complexity of $O(|V|^{2})$ since we need to update all the arrays of which the matrix is made up as well as adding a new array;\n",
    "        \n",
    "    - Add Edge:\n",
    "        \n",
    "        Adding an edge to a graph represented as an Adjacency Matrix can be done with a time complexity of $O(1)$, since we just need to add to the update 2 values in the matrix;\n",
    "   \n",
    "    - Remove Vertex:\n",
    "    \n",
    "        Removing a vertex from a graph represented as an Adjacency Matrix can be done with a time complexity of $O(|V|^{2})$, because we need to update all the other arrays of which the matrix is made up;\n",
    "\n",
    "    - Remove Edge:\n",
    "    \n",
    "        Removing an edge from a graph represented as an Adjacency Matrix can be done with a time complexity of $O(1)$, since we need to just update 2 values in the matrix.\n",
    "    \n",
    "    \n",
    "    The space complexity of an Adjacency Matrix is $O(|V|^{2})$.\n",
    "\n",
    "\n",
    "Because of their space complexity, it makes sense to use Adjacency Matrices either for small Graphs or Graphs with a lot of edges.\n",
    "\n",
    "Graphs can be used everywhere we need to store relationships of any kind between elements. Since Trees are a subset of Graphs, all the real worl implementation of Trees are also real-world implementations of graphs. Another possible real-world implementation of Graphs is for road maps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coupled-buyer",
   "metadata": {},
   "source": [
    "<div id=\"introductionA\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thick-truth",
   "metadata": {},
   "source": [
    "# Introduction to Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "floppy-darkness",
   "metadata": {},
   "source": [
    "We have seen what is an algorithm and how to analyze one in the first part of the portfolio. In this section, we will analyze in depth some common algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bigger-business",
   "metadata": {},
   "source": [
    "<div id=\"searching\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "communist-usage",
   "metadata": {},
   "source": [
    "## Searching Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "former-heart",
   "metadata": {},
   "source": [
    "Searching Algorithms are algorithms designed to find an element in the data structure for which the algorithm has been designed. We can find 2 main categories of Searching Algorithms:\n",
    "\n",
    "- **Sequential Searching Algorithms**, designed to search for an item in unsorted data structures, they check every item in the data structure;\n",
    "- **Interval Searching Algorithms**, designed to search for an item in sorted data structures, they only check some items of the data structure. This allows them to be more efficienct than Sequential Searching Algorithms.\n",
    "\n",
    "These algorithms are correct if they can correct find an item in the data structure for which they were designed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chief-brooks",
   "metadata": {},
   "source": [
    "<div id=\"linearSearch\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "heavy-ticket",
   "metadata": {},
   "source": [
    "### Linear Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "graduate-element",
   "metadata": {},
   "source": [
    "The Linear Search Algorithm is a Sequential Searching Algorithm.\n",
    "\n",
    "It takes in input an array and a value to search and iterates through the array to search for the value. It usually return the index of the element (if it is in the array) or $-1$ (if the element is not in the array).\n",
    "\n",
    "Here is a python example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "employed-aerospace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "def linear_search(arr, val):\n",
    "    for i in range(len(arr)):\n",
    "        if (arr[i] == val):\n",
    "            return i\n",
    "    return -1\n",
    "\n",
    "array = [9,6,3,5,3,0,2,4,3,7,8,2,6,1]\n",
    "print(linear_search(array, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "double-floor",
   "metadata": {},
   "source": [
    "This algorithm terminates for every input, either when it finds the element it is looking for, ot when it iterates through the whole array."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reliable-taylor",
   "metadata": {},
   "source": [
    "<figure>\n",
    "    <img src=\"pics/LinearSearch.gif\" alt=\"Linear Search Animation\" width=\"400\"/>\n",
    "    <figcaption>\n",
    "        Linear Search Animation. Gif source: <a href=\"https://www.tutorialspoint.com/data_structures_algorithms/linear_search_algorithm.htm\">tutorialspoint</a>\n",
    "    </figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "breeding-worse",
   "metadata": {},
   "source": [
    "**Time Complexity Analysis**\n",
    "\n",
    "We have seen that this algorithm searches an element by iterating through the array, so in the case that the element is not present or is the last element of the array, it needs to go through the whole array of $n$ elements. In the average case, the number of elements through which it needs to iterate also depend on $n$. Its time complexity in the average and worst case will therefore be $Θ = O(n)$.\n",
    "\n",
    "In the best case, the element to search will be the first one and thus the time complexity of this algorithm in the best case will be $Ω(1)$.\n",
    "\n",
    "**Space Complexity Analysis**\n",
    "\n",
    "No auxiliary data structures are required by this algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "external-speed",
   "metadata": {},
   "source": [
    "<div id=\"binarySearch\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inclusive-november",
   "metadata": {},
   "source": [
    "### Binary Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "active-pantyhose",
   "metadata": {},
   "source": [
    "The Binary Search Algorithm is an Interval Searching Algorithm.\n",
    "\n",
    "It takes in input a sorted array and the value to search in it. It works by finding the median point of the array and returning the index of that point if that element contains the value we're searching for or recursively calls itself on the half of the array which could contain the element (the right part if the value of the mian element was smaller thatn that of the value we are looking for or the left part if the value of the value of the median element was greater). The process is recursively repeated until the element is found or there are no subarrays left to search.\n",
    "\n",
    "Here is a python example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "abstract-reception",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "def binary_search (arr, left, right, val):\n",
    "    # check that the subarray is not empty\n",
    "    if right >= left:\n",
    "        # find the index of the median value\n",
    "        mid = left + (right - left) // 2\n",
    "        # return the median value if it is the one we were searching for\n",
    "        if arr[mid] == val:\n",
    "            return mid \n",
    "        # if the median value is greater, recursively search the left part of the array.\n",
    "        elif arr[mid] > val: \n",
    "            return binary_search(arr, left, mid-1, val)\n",
    "        # otherwise recursively search the right part.\n",
    "        else: \n",
    "            return binary_search(arr, mid + 1, right, val)\n",
    "    # if the subarray is empty return -1\n",
    "    else:  \n",
    "        return -1\n",
    "array = [0, 2, 5, 7, 8, 11, 12, 13, 19, 23, 26, 32, 41]\n",
    "print(binary_search(array, 0, len(array), 26))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sweet-ethernet",
   "metadata": {},
   "source": [
    "This algorithm terminates for every input, either when it finds the element or when it tries to search an empty subarray."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "painted-importance",
   "metadata": {},
   "source": [
    "<figure>\n",
    "    <img src=\"pics/BinarySearch.gif\" alt=\"Binary Search Animation\" width=\"400\"/>\n",
    "    <figcaption>\n",
    "        Binary Search Animation. Gif source: <a href=\"https://brilliant.org/wiki/binary-search/\">Brilliant</a>\n",
    "    </figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "furnished-replica",
   "metadata": {},
   "source": [
    "**Time Complexity Analysis**\n",
    "\n",
    "Because at each recursive call the size $n$ of the array is split in half, it will take $log(n)$ recursive calls to have an array of 1 element (which either contains the element we were searching for, or will result in a recursive call on an empty array). Because of this, the time complexity of this algorithm in its average and worst case is $Θ=O(log(n))$.\n",
    "\n",
    "In the best case, the element we are looking for is at the middle of the array and thus will be the first value we will search. So the time complexity of this algorithm in the best case will be $Ω(1)$.\n",
    "\n",
    "**Space Complexity Analysis**\n",
    "\n",
    "No auxiliary data structures are required by this algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "animated-uncertainty",
   "metadata": {},
   "source": [
    "<div id=\"sorting\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thousand-vinyl",
   "metadata": {},
   "source": [
    "## Sorting Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polar-cross",
   "metadata": {},
   "source": [
    "Sorting Algorithms are algorithms specifically designed to order element, in ascending or descending order, in the data structure for which the algorithm has been designed. \n",
    "\n",
    "Sorting algorithms may vary a lot for both time and space efficiency, but also for their behavior in edge cases.\n",
    "\n",
    "We can also distinguish between stable and unstable Sorting Algorithms: Stable Algorithms maintain the relative order of elements with the same value, while Unstable Algorithms do not. Every Unstable Algorithm can become Stable if we add the initail index of the element as second sorting key.\n",
    "\n",
    "We can say that a sorting algorithm is correct if it is capable of sorting the items in the data structure for which it has been designed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "essential-slide",
   "metadata": {},
   "source": [
    "<div id=\"selectionSort\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blank-paraguay",
   "metadata": {},
   "source": [
    "### Selection Sort"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "respected-bolivia",
   "metadata": {},
   "source": [
    "The Selection Sort Algorithm is an in-place, iterative sorting algorithm. \n",
    "\n",
    "It works by comparing the first item in a array to all the other items with a greater index and swaps it with the smallest it founds. It then moves to the next element and repeats the process until it gets to the last item of the array. In this way, after each iteration one more item is sorted.\n",
    "\n",
    "Here we can see a simple implementation in python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "forward-accreditation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 2, 3, 3, 3, 4, 5, 6, 6, 7, 8, 9]\n"
     ]
    }
   ],
   "source": [
    "def selection_sort(arr):\n",
    "    for i in range(len(arr)-1):\n",
    "        # find the index of the smallest element\n",
    "        min_number_index = i\n",
    "        for j in range(i, len(arr)):\n",
    "            if arr[min_number_index] > arr[j]:\n",
    "                min_number_index = j\n",
    "        # swap the elements\n",
    "        arr[i], arr[min_number_index] = arr[min_number_index], arr[i]\n",
    "\n",
    "array = [9,6,3,5,3,0,2,4,3,7,8,2,6,1]\n",
    "selection_sort(array)\n",
    "print(array)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unexpected-moderator",
   "metadata": {},
   "source": [
    "Because this implementation of the algorithm swaps the elements in their correct positions, it does not maintain the relative order of the elements and it is therefore unstable.\n",
    "\n",
    "We can see that this algorithms terminates for every input, since its loops will stop at the end of the array."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "academic-rwanda",
   "metadata": {},
   "source": [
    "<figure>\n",
    "    <img src=\"pics/SelectionSort.gif\" alt=\"Selection Sort Animation\" width=\"500\"/>\n",
    "    <figcaption>\n",
    "        Selection Sort Animaiton. Blue = current index, Red = current minimum, Yellow = already sorted. Gif source: <a href=\"https://rcsole.gitbooks.io/apprenticeship/content/year-one/data-structures-and-algorithms/02-sorting-algorithms.html\">GitBooks</a>\n",
    "    </figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "directed-miller",
   "metadata": {},
   "source": [
    "**Time Complexity Analysis**\n",
    "\n",
    "As we can see from the code, the algorithm needs to run two nested loops, the outermost one will call the other one $n-1$ times, and the operations that the innermost loop will perform each time are $n-i$, where $i$ is the current iteration of the outermost loop.\n",
    "\n",
    "If we multiply the amount of opeartions carried out by the two loops we will find out that the asymptotic growth of this function is quadratic, therefore, the average, best, and worst case-scenario time complexity will be $Θ = Ω = O(n^{2})$.\n",
    "\n",
    "This means that this algorithm will take the same amount of steps also in every edge case (array already sorted, array sorted backwards, array where all the items are the same).\n",
    "\n",
    "**Space Complexity Analysis**\n",
    "\n",
    "Because this is an in-place algorithm, it does not require any auxiliary space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "separated-northern",
   "metadata": {},
   "source": [
    "<div id=\"bubbleSort\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spoken-physiology",
   "metadata": {},
   "source": [
    "### Bubble Sort"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pressing-august",
   "metadata": {},
   "source": [
    "The Bubble Sort Algorithm is another example of in-place, iterative sorting algorithm. \n",
    "\n",
    "Here is how this algorithm worls: it starts by comparing the first element in the array to the next. If the first element is bigger than the next, it swaps them before moving to the next element and repeats the process, until it gets to the end of the array. It then starts a new iteration with one less item to be compared, until the array is sorted.\n",
    "\n",
    "Here we can see a simple implementation in python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "serial-career",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bubble_sort(arr):\n",
    "    # check if any items have been swapped\n",
    "    has_swapped = True\n",
    "    # keep track of the index\n",
    "    i = 0\n",
    "    while(has_swapped and i<len(arr)):\n",
    "        has_swapped = False\n",
    "        for j in range(len(arr) - i - 1):\n",
    "            if arr[j] > arr[j+1]:\n",
    "                arr[j], arr[j+1] = arr[j+1], arr[j]\n",
    "                has_swapped = True\n",
    "        i += 1\n",
    "    \n",
    "array = [9,6,3,5,3,0,2,4,3,7,8,2,6,1]\n",
    "bubble_sort(array)\n",
    "print(array)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enabling-installation",
   "metadata": {},
   "source": [
    "Because at each iteration this algorithm moves the item with the largest value at the end, the relative order of elements with the same value remains the same and thus is algorithm is stable.\n",
    "\n",
    "This algorithm terminates for every input, as soon as there is an iteration in which no elements of the array need to be sorted."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "latin-simple",
   "metadata": {},
   "source": [
    "<figure>\n",
    "    <img src=\"pics/BubbleSort.gif\" alt=\"Bubble Sort Animation\" width=\"300\"/>\n",
    "    <figcaption>\n",
    "        Bubble Sort Animation. Gif source: <a href=\"https://medium.com/madhash/bubble-sort-in-a-nutshell-how-when-where-4965e77910d8\">Medium</a>\n",
    "    </figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arctic-record",
   "metadata": {},
   "source": [
    "**Time Complexity Analysis**\n",
    "\n",
    "This algorithm needs to run 2 nested loops, of $n-1$ and $n-i-1$ iterations, where $n$ is the length of the array and $i$ the number of iteration of the outermost loop.\n",
    "\n",
    "If we multiply the amount of opeartions carried out by the two loops we will find out that the asymptotic growth of this algorithm is quadratic. The average and worst case-scenario time complexity will be $Θ = O(n^{2})$.\n",
    "\n",
    "Since the algorith checks if any swaps have been performed at each iteration, in the case that the input array is already sorted or all the items in the array are the same (no item is greater than the previous one), the time complexity of this algorithm will be $Ω(n)$.\n",
    "\n",
    "If instead the input array is sorted backwards, the time complexity will be $O(n^2)$, since it will need to perform swaps at each operation.\n",
    "\n",
    "**Space Complexity Analysis**\n",
    "\n",
    "Because this is an in-place algorithm, it does not require any auxiliary space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gross-decrease",
   "metadata": {},
   "source": [
    "<div id=\"quickSort\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stretch-reconstruction",
   "metadata": {},
   "source": [
    "### QuickSort"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "joined-country",
   "metadata": {},
   "source": [
    "The QuickSort Algorithm is a recursive sorting algorithm.\n",
    "\n",
    "The algorithms starts by chosing a \"pivot\" (there are different ways to do this) and puts all the elements smaller than the pivot at its left and the elements bigger than the pivot at its right. In this way the pivot is in the correct position and the operation is recursively repeated on the 2 sub-arrays, with new pivots. The operations continues until the subarrays contain only one element.\n",
    "\n",
    "Here we can see an example of implementation in python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "blessed-humor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 2, 3, 3, 3, 4, 5, 6, 6, 7, 8, 9]\n"
     ]
    }
   ],
   "source": [
    "def quicksort(arr):\n",
    "    # auxiliary arrays\n",
    "    less = []\n",
    "    equal = []\n",
    "    greater = []\n",
    "    if len(arr) > 1:\n",
    "        # pick the first element as pivot\n",
    "        pivot = arr[0]\n",
    "        for x in arr:\n",
    "            if x < pivot:\n",
    "                less.append(x)\n",
    "            elif x == pivot:\n",
    "                equal.append(x)\n",
    "            elif x > pivot:\n",
    "                greater.append(x)\n",
    "        # recursively repeat the operation\n",
    "        return quicksort(less)+equal+quicksort(greater)\n",
    "    else:\n",
    "        return arr\n",
    "    \n",
    "array = [9,6,3,5,3,0,2,4,3,7,8,2,6,1]\n",
    "print(quicksort(array))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "medieval-affairs",
   "metadata": {},
   "source": [
    "In this implementation, the pivot is always the first item of the array/subarray. Other ways of chosing the pivot include:\n",
    "- picking always the last item in the array;\n",
    "- picking a random value;\n",
    "- picking three values (using one of the methods above) and choosing the median value as pivot.\n",
    "\n",
    "Because in this implementation we store the elements with the same value in a third array, we pick the pivot to be the first element of the subarrays, the relative order of the elements with the same value will not change. This specific implementation of QuickSort is stable, but it is not always the case.\n",
    "\n",
    "This algorithm terminates for every input as soon as the subarrays contain 1 or 0 elements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "located-musical",
   "metadata": {},
   "source": [
    "**Time Complexity Analysis**\n",
    "\n",
    "We know that this algorithm will need to go through all the elements in the array, split it in half, and repeat the process on the halved arrays until it creates array of only one element. Assuming that the subarrays are of equal size, we can recursively split them in half $log(n)$ times.\n",
    "<figure>\n",
    "    <img src=\"pics/QuickSort.png\" alt=\"Representation of QuickSort\" width=\"400\"/>\n",
    "    <figcaption>\n",
    "        Quick Sort. Image Source: <a href=\"https://deepai.org/machine-learning-glossary-and-terms/quicksort-algorithm\">DeepAI</a>\n",
    "    </figcaption>\n",
    "</figure>\n",
    "We can see in the image above how an array of 9 elements is recursively divided 3 times ($≈log_{2}(9) $) before it is sorted. The number of comparisons in every iteration decreases as the size of the subarrays decreases, but we can see that it depends on the size of the array ($n$). Therefore, we will have in the average case a total number of operations, and thus its time complexity, that will grow in a quasilinear way ($Θ(n·log(n))$).\n",
    "<br/><br/>\n",
    "\n",
    "In case that the input array contains only elements with the same value, our implementation will only need to go through the array once, and since no element is bigger or smaller it will return two recursive functions called on empty array and the array of elements with the same value as the pivot. Its best-case time complexity will therefore be $Ω(n)$, but depending on how the algorithm is implemented, this may not be true in every case.\n",
    "\n",
    "\n",
    "If instead the input array is already sorted, or sorted backwards, the time complexity of the algorithm will be $O(n^2)$, at least if the pivot is chosen to be always the first ot always the last value of the array, since we will have a situation similar to that of the image below, where after each \"iteration\" $i$, the array is not split enough but in an array $n-i-1$ elements:\n",
    "<figure>\n",
    "    <img src=\"pics/QuickSortWorst.png\" alt=\"Representation of QuickSort in worst case scenario\" width=\"400\"/>\n",
    "    <figcaption>\n",
    "        Quick Sort in worst case time complexity. Image Source: <a href=\"https://www.khanacademy.org/computing/computer-science/algorithms/quick-sort/a/analysis-of-quicksort\">Khan Academy</a>\n",
    "    </figcaption>\n",
    "</figure>\n",
    "This situation can be avoided by picking a random pivot (which does not guarantee that this situation will not happen in other cases, although extremely unlikely especially for large enough inputs) or picking a median input.\n",
    "<br/><br/>\n",
    "\n",
    "**Space Complexity Analysis**\n",
    "\n",
    "In the worst-case scenario, the QuickSort Algorithm will require an auxiliary space of $O(n)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "swiss-reform",
   "metadata": {},
   "source": [
    "<div id=\"mergeSort\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "choice-collapse",
   "metadata": {},
   "source": [
    "### MergeSort"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adult-string",
   "metadata": {},
   "source": [
    "The MergeSort Algorithm is another example of recursive sorting algorithm.\n",
    "\n",
    "This algorithm works in a way that is conceptually really simple: it recursively divide an array in half until only arrays containing 1 element are left. It then start to merge these arrays together while sorting them.\n",
    "\n",
    "Here we can see a python implementation of this algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "entire-allen",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 2, 3, 3, 3, 4, 5, 6, 6, 7, 8, 9]\n"
     ]
    }
   ],
   "source": [
    "def merge(left, right):\n",
    "    '''\n",
    "    helper function to merge the array in an ordered way\n",
    "    '''\n",
    "    result = []\n",
    "    i = j = 0\n",
    "    while i < len(left) and j < len(right):\n",
    "        if left[i] <= right[j]:\n",
    "            result.append(left[i])\n",
    "            i += 1\n",
    "        else:\n",
    "            result.append(right[j])\n",
    "            j += 1\n",
    "    result += left[i:]\n",
    "    result += right[j:]\n",
    "    return result\n",
    "\n",
    "def mergesort(arr):\n",
    "    # if the array contains only 1 element, return it\n",
    "    if len(arr) < 2:\n",
    "        return arr\n",
    "    # otherwise recursively call this function to\n",
    "    # the array in half...\n",
    "    mid = len(arr) // 2\n",
    "    left_arr = mergesort(arr[:mid])\n",
    "    right_arr = mergesort(arr[mid:])\n",
    "    # ...and then merge it\n",
    "    return merge(left_arr, right_arr)\n",
    "\n",
    "array = [9,6,3,5,3,0,2,4,3,7,8,2,6,1]\n",
    "print(mergesort(array))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "authentic-press",
   "metadata": {},
   "source": [
    "While merging back the subarrays, this algorithm maintains the relative order of elements with the same value and therefore it is a stable algorithm.\n",
    "\n",
    "This algorithm will terminate for every input when the it is split in subarrays and then merged back together."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smooth-maria",
   "metadata": {},
   "source": [
    "**Time Complexity Analysis**\n",
    "\n",
    "This algorithm first needs to recursively divide the array ino subarrays containing only 1 element, and then merge and order them.\n",
    "<figure>\n",
    "    <img src=\"pics/mergeSort.png\" alt=\"Representation of MergeSort\" width=\"400\"/>\n",
    "    <figcaption>\n",
    "        Merge Sort. Image Source: <a href=\"https://en.wikipedia.org/wiki/Merge_sort#/media/File:Merge_sort_algorithm_diagram.svg\">Wikipedia</a>\n",
    "    </figcaption>\n",
    "</figure>\n",
    "\n",
    "To divide the arra into subarrays of 1 element each it takes a time complexity of $O(1)$, since we only need to calculate the the medium point of the array. Once the array is split, we need to perform $log(n)$ number of iterations to merge it back together by merging 2 subarrays at a time, while performing $n$ comparisons at each iteration to sort it. Because of this, the time complexity of this algorithm will be $Θ(log(n))$.\n",
    "\n",
    "An input array which is already sorted, sorted backwards, or in which all items are the same will not affect the time complexity of this algorithm, therefore its best and worst case time complexity will be $Ω = O(n·log(n))$\n",
    "\n",
    "**Space Complexity Analysis**\n",
    "\n",
    "The MergeSort Algorithm is not really space-efficient, and requires an auxiliary space of $O(n)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alien-catholic",
   "metadata": {},
   "source": [
    "<div id=\"heapSort\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "retained-policy",
   "metadata": {},
   "source": [
    "## HeapSort"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "urban-museum",
   "metadata": {},
   "source": [
    "The HeapSort Algorithm is an iterative sorting algorithm.\n",
    "\n",
    "Taken in input an unordered array, this algorithm turns that array into a Max Heap (an heap in which the value of a parent node is always bigger than that of its children and that can be represented as an array). It then swaps the last element in the heap with the first one (the root). Since the root is always the biggest element in a Max Heap, we know that that element is in its final position, and thus we can considered it to be in the sorted partition of the array. Then the algorithm \"heapifies\" (rearranges the elements so that the condition of the heap is met) the heap (or non-sorted partition of the array) and repeats the process with the last element of the heap (or non-sorted partition of the array) until there are no more elements in the heap and the array is sorted.\n",
    "\n",
    "Here we can see a python implementation of this algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "pregnant-importance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 2, 3, 3, 3, 4, 5, 6, 6, 7, 8, 9]\n"
     ]
    }
   ],
   "source": [
    "def heapify(heap, n, i):\n",
    "    '''order the heap'''\n",
    "    max = i\n",
    "    left = 2 * i + 1\n",
    "    right = 2 * i + 2\n",
    "    # check if the left and right children exist.\n",
    "    # If one of them is bigger than the parent, set it as max value.\n",
    "    if left < n and heap[max] < heap[left]:\n",
    "        max = left\n",
    "    if right < n and heap[max] < heap[right]:\n",
    "        max = right\n",
    "    # swap the max value with the parent\n",
    "    if max != i:\n",
    "        heap[i], heap[max] = heap[max], heap[i]\n",
    "        # heapify with the new parent node\n",
    "        heapify(heap, n, max)\n",
    "\n",
    "def build_max_heap(arr):\n",
    "    \"\"\"create an heap from an array\"\"\"\n",
    "    for i in range(len(arr)//2 - 1, -1, -1):\n",
    "        heapify(arr, len(arr), i)\n",
    "    return arr\n",
    "        \n",
    "def heapsort(arr):\n",
    "    build_max_heap(arr)\n",
    " \n",
    "    # perform the heapsort\n",
    "    for i in range(len(arr)-1, 0, -1):\n",
    "        arr[i], arr[0] = arr[0], arr[i]\n",
    "        heapify(arr, i, 0)\n",
    "\n",
    "array = [9,6,3,5,3,0,2,4,3,7,8,2,6,1]\n",
    "heapsort(array)\n",
    "print(array)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "removable-costs",
   "metadata": {},
   "source": [
    "The initial order of the array is changed once the Max Heap is created, so the relative order of elements with the same value will change too. This algorithm is therefore unstable.\n",
    "\n",
    "This algorithm will terminate for every input, since it ends once its main loops gets to the first item of the heap."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fifty-lithuania",
   "metadata": {},
   "source": [
    "<figure>\n",
    "    <img src=\"pics/HeapSort.gif\" alt=\"Heap Sort Animation\" width=\"400\"/>\n",
    "    <figcaption>\n",
    "        Heap Sort Animation. Gif source: <a href=\"https://www.codesdope.com/course/algorithms-heapsort/\">CodesDope</a>\n",
    "    </figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "historic-modification",
   "metadata": {},
   "source": [
    "**Time Complexity Analysis**\n",
    "\n",
    "We can start by analyzing the heapify function, which has a worst-case time complexity of $O(log(n))$, since it may be necessary for it to rearrange an element at each level of the Heap. \n",
    "\n",
    "The first step of this algorithm is turning the array into a Max Heap. Intuitively we may say that this operation will have a time complexity of $O(n·log(n))$, which is correct but is not an asymptotic tight bound. Since the runtime of the heapify function is different for each node, depending on its level, the worst-case time complexity of the make_max_heap function will be $O(n)$. A complete analysis of this operation can be found [here](http://www.cs.umd.edu/~meesh/351/mount/lectures/lect14-heapsort-analysis-part.pdf).\n",
    "\n",
    "We can see in the code implementation above that once turned the array into an heap, we need to call the heapify function $n-1$ times. For this reason, the time complexity of this algorithm in the best, average, and worst case will be $Ω=Θ=O(n·log(n))$.\n",
    "\n",
    "If we pass as input an array which is already sorted, sortad backwards, or in which all the items have the same value, the time complexity of this algorithm will not change.\n",
    "\n",
    "**Space Complexity Analysis**\n",
    "\n",
    "No auxiliary data structures are required by this algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "registered-component",
   "metadata": {},
   "source": [
    "<div id=\"countingSort\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prime-steel",
   "metadata": {},
   "source": [
    "### Counting Sort"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "numerical-guess",
   "metadata": {},
   "source": [
    "Counting Sort is a non-comparison, iterative sorting algorithm.\n",
    "\n",
    "Unlike all the other sorting algorithms we have seen, the Counting Sort algorithm works only on positive integers in a range $k$.\n",
    "\n",
    "Here is how the algorithm works: \n",
    "1. It counts the number of occurrences of each unique value $v$ in the array and stores the number of occurences at index $v$ in an auxiliary array;\n",
    "2. It then performs a cumulative count of the elements in the auxiliary array (e.g.: ```[0,2,1,1]``` becomes ```[0,2,3,4]``` after a cumulative count) so that each value $c$ in the auxiliary array is the last index + 1 at which the value $v$ (given by the index of $c$) will appear in the output array;\n",
    "3. Then, starting with the last element $e$ in the initial array, it decreases the cumulative count of the element in position $e$ in the auxiliary array, and inserts $e$ at the correct index (the value stored in the position $e$ of the auxiliary array) in the output array. At the end, it copies the output array in the original array.\n",
    "\n",
    "Here is a python implementation of this algorithm where the range $k$ is fixed to be 0-9:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "governmental-drinking",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 2, 3, 3, 3, 4, 5, 6, 6, 7, 8, 9]\n"
     ]
    }
   ],
   "source": [
    "def counting_sort(array):\n",
    "    size = len(array)\n",
    "    output = [0] * size \n",
    "    # Initialize count array\n",
    "    aux = [0] * 10\n",
    "    # Store the count of each elements in count array\n",
    "    for i in range(0, size):\n",
    "        aux[array[i]] += 1\n",
    "    # Store the cumulative count\n",
    "    for i in range(1, 10):\n",
    "        aux[i] += aux[i - 1]\n",
    "    # Decrease the cumulative count by one for each position\n",
    "    # Find the index of the element of the original array in aux array\n",
    "    # Place the elements in output array, at the right index\n",
    "    for i in range(size - 1, -1, -1):\n",
    "        aux[array[i]] -= 1\n",
    "        index = aux[array[i]]\n",
    "        output[index] = array[i]\n",
    "        \n",
    "    # Copy the sorted elements into original array\n",
    "    for i in range(0, size):\n",
    "        array[i] = output[i]\n",
    "\n",
    "array = [9, 6,3,5,3,0,2,4,3,7,8,2,6,1]\n",
    "counting_sort(array)\n",
    "print(array)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acceptable-processing",
   "metadata": {},
   "source": [
    "This algorithm is also stable because it maintains the relative order of elements with the same value when the array is sorted from the initial to the output array.\n",
    "\n",
    "This algorithm will also terminate for every input as soon as the loops are executed.\n",
    "\n",
    "<figure>\n",
    "    <img src=\"pics/CountingSort.gif\" alt=\"Counting Sort Animation\" width=\"400\"/>\n",
    "    <figcaption>\n",
    "        Counting Sort Animation. Gif source: <a href=\"https://brilliant.org/wiki/counting-sort/\">Brilliant</a>\n",
    "    </figcaption>\n",
    "</figure>\n",
    "\n",
    "**Time Complexity Analysis**\n",
    "\n",
    "We can analyze each step of this algorithm to find its time complexity:\n",
    "1. Counting the number of occurencies can be done with a time complexity of $O(n)$, where $n$ is the length of the array;\n",
    "2. Performing a cumulative count an be done with a time complexity of $O(k)$, where $k$ is the size of the range of the positive integer values of which the array is made of, and the size of the auxiliary array;\n",
    "3. Arranging the elements in ascending order in the output array can be done with a time complexity of $O(n)$.\n",
    "\n",
    "Since we have some operations that require a time complexity of $O(n)$ and another that requires a time complexity of $O(k)$, we can say that the time complexity of this algorithm is $O(n+k)$.\n",
    "\n",
    "In case that all the elements in the input array have the same value, the size of the range of positive integers $k$ in the array will be $1$, and thus its best-case time complexity will be $Ω(n)$. If instead the input array is already sorted, or sorted backwards, it won't affect the time complexity of this algorithms, so its average and worst-case time complexity will be $Θ=O(n+k)$.\n",
    "\n",
    "**Space Complexity Analysis**\n",
    "\n",
    "The auxiliary space required by this algorithm depends on the size of the array ($n$) and the size of the range of positive integeres that the array contains ($k$). Its space compelxity will therefore be $O(n+k)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ranging-costs",
   "metadata": {},
   "source": [
    "<div id=\"graphAlgorithms\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "structured-poker",
   "metadata": {},
   "source": [
    "## Graph Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unexpected-casting",
   "metadata": {},
   "source": [
    "Some algorithms are designed to perform some specific operations on graphs or on given subcategories of them. Some common operations carried out by graph algorithms include traversal (visiting all the vertices in a graph) and pathfinding (finding the shortest path between 2 vertices)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "offensive-affiliation",
   "metadata": {},
   "source": [
    "<div id=\"dps\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hazardous-bulgarian",
   "metadata": {},
   "source": [
    "### Depth-First Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "numerical-reminder",
   "metadata": {},
   "source": [
    "The Depth-First Search Algorithm (DFS) is a graph traversal algorithm. It can be implemented on both trees and graphs, and it can also be modified and used for searching or pathfinding on these data structures. We will see how the algorithm works when used for graph traversal.\n",
    "\n",
    "Starting from a given vertex (or the root node in case of a tree), this algorithm visits all the possible vertices in a specific branch before backtracking.\n",
    "\n",
    "The algorithm works by adding the vertex that is passed as an argument, $v$, to a data structure (usually a stack) – to keep track of the vertices that have already been visited. It then recursively calls itself on every neighbor of $v$ that has not been visited yet, so that all the branches are visited. Finally, it returns the list of visited vertices.\n",
    "\n",
    "Here is a python implementation of DFS used for graph traversal on a graph represented by an adjacency list in which each key of the dictionary is a vertex and its value is an array containing all the nodes to which it is connected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "hidden-division",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['C', 'B', 'A', 'D', 'E', 'F']\n"
     ]
    }
   ],
   "source": [
    "def dfs(graph, start, visited=None):\n",
    "    if visited == None:\n",
    "        visited = []\n",
    "    visited.append(start)\n",
    "    for vertex in graph[start]:\n",
    "        if vertex not in visited:\n",
    "            dfs(graph, vertex, visited)\n",
    "    return visited\n",
    "\n",
    "graph = {'A': ['B', 'D'],\n",
    "         'B': ['A', 'C', 'D'],\n",
    "         'C': ['B', 'F'],\n",
    "         'D': ['A', 'B', 'E', 'F'],\n",
    "         'E': ['D'],\n",
    "         'F': ['C', 'D']}\n",
    "\n",
    "print(dfs(graph, 'C'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fallen-business",
   "metadata": {},
   "source": [
    "This algorithm will not terminate if implemented for graph traversal on an infinite graph (and could not terminate even if implemented for searching on a infinite graph). Otherwise it will terminate once it has visited each vertex (or found a given vertex/path, depending on its implementation).\n",
    "\n",
    "We know that this algorithm is correct if it can correctly traverse a graph."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "electronic-cleveland",
   "metadata": {},
   "source": [
    "<figure>\n",
    "    <img src=\"pics/DFS.gif\" alt=\"Depth-First Search Animation\" width=\"400\"/>\n",
    "    <figcaption>\n",
    "        Depth-First Search on a tree. Gif source: <a href=\"https://commons.wikimedia.org/wiki/File:Depth-First-Search.gif\">Wikimedia</a>\n",
    "    </figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "digital-liability",
   "metadata": {},
   "source": [
    "**Time Complexity Analysis**\n",
    "\n",
    "We have seen that this algorithm needs to visit all the vertices, starting at the initial one, checking all of its neighbors (the vertices connected to it by an edge), and recursively call itself on the neighbors that have not yet been visited. Therefore, the total number of operations depends on the number of vertices $V$ and the number of edges $E$ and its time complexity, in the average and worst case, will be $Θ=O(|V|+|E|)$.\n",
    "\n",
    "When this algorithm is implemented on a tree (which is a graph with $v-1$ edges), its time complexity will be $Ω(V)$.\n",
    "\n",
    "**Space Complexity Analysis**\n",
    "\n",
    "The auxiliary space required by this algorithm depends on the number of vertices, so its space complexity will be $O(V)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stunning-brazil",
   "metadata": {},
   "source": [
    "<div id=\"bfs\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vocational-alabama",
   "metadata": {},
   "source": [
    "### Breadth-First Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "automated-necessity",
   "metadata": {},
   "source": [
    "The Breadth-First Search Algorithm (BFS) is another graph traversal algorithm. Just like DFS, it can be implemented on both trees and graphs, and it can be modified and used for searching or pathfinding on these data structures. Like for the DFS, we will see how the algorithm works when used for graph traversal.\n",
    "\n",
    "Starting from a given vertex (or the root node in case of a tree), this algorithm visits all of its neighbors, stores them in a queue and then repeats the process with the first vertex in the queue. \n",
    "\n",
    "The algorithm works by storing the start node in a queue (to keep track of the vertices whose neighbors haven't been visited) and in another data structure (to keep track of the vertices that have been visited). Then, while there are items in the queue, it pops the first item from the queue and checks its neighbors, and those who haven't been visited yet are added to both the queue and the other data structure. At the end, it returns the data structure containing the visited nodes.\n",
    "\n",
    "Here is a python implementation of BFS used for graph traversal on a graph represented by an adjacency list in which each key of the dictionary is a vertex and its value is an array containing all the nodes to which it is connected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "improved-manor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A', 'B', 'C', 'D', 'E', 'F', 'G']\n"
     ]
    }
   ],
   "source": [
    "def bfs(graph, start):\n",
    "    visited = []\n",
    "    queue = []\n",
    "    visited.append(start)\n",
    "    queue.append(start)\n",
    "    while queue:\n",
    "        current = queue.pop(0) \n",
    "        for vertex in graph[current]:\n",
    "            if vertex not in visited:\n",
    "                visited.append(vertex)\n",
    "                queue.append(vertex)\n",
    "    return visited\n",
    "\n",
    "graph = {'A': ['B', 'C', 'D'],\n",
    "         'B': ['A', 'E'],\n",
    "         'C': ['A', 'D', 'F'],\n",
    "         'D': ['A', 'C', 'E', 'G'],\n",
    "         'E': ['B', 'D', 'G'],\n",
    "         'F': ['C', 'G'],\n",
    "         'G': ['D', 'E', 'F']}\n",
    "\n",
    "print(bfs(graph, 'A'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "executed-matrix",
   "metadata": {},
   "source": [
    "This algorithm will not terminate if implemented for graph traversal on an infinite graph (but unlike DFS, it terminates if implemented for searching on a infinite graph). Otherwise it will terminate once it has visited each vertex (or found a given vertex/path, depending on its implementation).\n",
    "\n",
    "We know that this algorithm is correct if it can correctly traverse a graph."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "convenient-mattress",
   "metadata": {},
   "source": [
    "<figure>\n",
    "    <img src=\"pics/BFS.gif\" alt=\"Breadth-First Search Animation\" width=\"400\"/>\n",
    "    <figcaption>\n",
    "        Breadth-First Search on a graph. Gif source: <a href=\"https://www.codeabbey.com/index/task_view/breadth-first-search\">CodeAbbey</a>\n",
    "    </figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exterior-cooperation",
   "metadata": {},
   "source": [
    "**Time Complexity Analysis**\n",
    "\n",
    "We have seen that this algorithm needs to visit all the vertices, starting at the initial one, checking all of its neighbors (the vertices connected to it by an edge), and repeat the operation on the neighbors that have not yet been visited. Therefore, like for the DFS, the total number of operations depends on the number of vertices $V$ and the number of edges $E$, and its time complexity, in the average and worst case, will be $Θ=O(|V|+|E|)$.\n",
    "\n",
    "When this algorithm is implemented on a tree (which is a graph with $v-1$ edges), its time complexity will be $Ω(V)$.\n",
    "\n",
    "**Space Complexity Analysis**\n",
    "\n",
    "The auxiliary space required by this algorithm, like in the case of the DFS, depends on the number of vertices, so its space complexity will be $O(V)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "strange-bulletin",
   "metadata": {},
   "source": [
    "<div id=\"dijkstras\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "architectural-makeup",
   "metadata": {},
   "source": [
    "### Dijkstra's Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "framed-kentucky",
   "metadata": {},
   "source": [
    "The Dijkstra's Algorithm is an algorithm used to find the shortest distance between a starting vertex and all the other vertices in a weighted graph. This algorithm requires the weight of the edges to be positive to work.\n",
    "\n",
    "These are the steps of this algorithm:\n",
    "1. Initilaize 2 auxiliary data structures to keep track of the distance to each vertex (the initial value for the distances will 0 for the starting vertex and infinity for all the others) and the shortest path tree;\n",
    "2. Find the closest adjacent vertex $u$ that is not in the shortest path tree, and add it to the shortest path tree;\n",
    "3. Update the distance from $u$ to its neighbors, if the distance is positive, the neighbor is not in the shortest path tree and the value of the distance from $u$ to the starting vertex plus the the distance from $u$ to its neighbor is less than the current value of the distance from the starting vertex to the neighbor of $u$;\n",
    "4. Repeat steps 2 and 3 for the number of vertices in the graph (so that every vertex gets added to the shortest path tree);\n",
    "5. Return or print the result.\n",
    "\n",
    "Here is a python implementation of Dijkstra's Algorithm on a graph represented as an adjacency matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "organized-nomination",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vertex |Distance from 5\n",
      "_______|_______________\n",
      "0      |11\n",
      "1      |12\n",
      "2      |4\n",
      "3      |11\n",
      "4      |10\n",
      "5      |0\n",
      "6      |2\n",
      "7      |3\n",
      "8      |6\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    " \n",
    "class Graph():\n",
    "    def __init__(self, vertices):\n",
    "        ''' \n",
    "        Initialize a graph, represented as an adjacency matrix of V*V size\n",
    "        '''\n",
    "        self.V = vertices\n",
    "        self.graph = [[0 for column in range(vertices)]\n",
    "                      for row in range(vertices)]\n",
    "        \n",
    "    def printResult(self, dist, start):\n",
    "        print(\"Vertex |Distance from \" + str(start))\n",
    "        print(\"_______|_______________\")\n",
    "        for node in range(self.V):\n",
    "            print(str(node) + \"      |\" + str(dist[node]))\n",
    " \n",
    "    def minDistance(self, dist, shortestPathTree):\n",
    "        '''\n",
    "        Finds the closest vertex not yet added to the shortest path tree\n",
    "        '''\n",
    "        min = sys.maxsize\n",
    "        for v in range(self.V):\n",
    "            if dist[v] < min and shortestPathTree[v] == False:\n",
    "                min = dist[v]\n",
    "                min_index = v\n",
    "        return min_index\n",
    " \n",
    "    def dijkstra(self, start):\n",
    "        '''\n",
    "        Finds the shortest distance from one node to all the others\n",
    "        '''\n",
    "        # Initialize a distance adjacency matrix and assign infinity to each value\n",
    "        dist = [sys.maxsize] * self.V\n",
    "        # Assign a distance 0 to the starting vertex\n",
    "        dist[start] = 0\n",
    "        # Keep track of the vertices in the shortest path tree\n",
    "        shortestPathTree = [False] * self.V\n",
    " \n",
    "        for vertex in range(self.V):\n",
    "            # Find the closest vertex not yet added to the shortest path tree\n",
    "            # This vertex is the starting one in the first iteration \n",
    "            u = self.minDistance(dist, shortestPathTree)\n",
    "            # Add the closest vertex to the shortest path tree\n",
    "            shortestPathTree[u] = True\n",
    "            # Update the distance from the neighbors \n",
    "            for v in range(self.V):\n",
    "                if self.graph[u][v] > 0 and shortestPathTree[v] == False and dist[v] > dist[u] + self.graph[u][v]:\n",
    "                    dist[v] = dist[u] + self.graph[u][v]\n",
    " \n",
    "        self.printResult(dist, start)\n",
    "    \n",
    "g = Graph(9)\n",
    "g.graph = [[0, 4, 0, 0, 0, 0, 0, 8, 0],\n",
    "           [4, 0, 8, 0, 0, 0, 0, 11, 0],\n",
    "           [0, 8, 0, 7, 0, 4, 0, 0, 2],\n",
    "           [0, 0, 7, 0, 9, 14, 0, 0, 0],\n",
    "           [0, 0, 0, 9, 0, 10, 0, 0, 0],\n",
    "           [0, 0, 4, 14, 10, 0, 2, 0, 0],\n",
    "           [0, 0, 0, 0, 0, 2, 0, 1, 6],\n",
    "           [8, 11, 0, 0, 0, 0, 1, 0, 7],\n",
    "           [0, 0, 2, 0, 0, 0, 6, 7, 0]\n",
    "           ]\n",
    " \n",
    "g.dijkstra(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "binding-tonight",
   "metadata": {},
   "source": [
    "This algorithm will terminate after $v$ iterations, where $v$ is the number of vertices in the graph.\n",
    "\n",
    "The algorithm needs to correctly find the shortest path from one vertex to all the otehrs to be correct."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "experienced-julian",
   "metadata": {},
   "source": [
    "<figure>\n",
    "    <img src=\"pics/Dijkstra.gif\" alt=\"Dijkstra's algorithm Animation\" width=\"400\"/>\n",
    "    <figcaption>\n",
    "        Dijkstra's algorithm animation. Gif source: <a href=\"https://en.wikipedia.org/wiki/Dijkstra%27s_algorithm\">Wikipedia</a>\n",
    "    </figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "concerned-purse",
   "metadata": {},
   "source": [
    "**Time Complexity Analysis**\n",
    "\n",
    "We can analyze each step of this implementation of the algorithm to find its overall time complexity:\n",
    "1. Initializing the auxiliary data structures can be done with a time complexity of $O(|V|)$, where $V$ is the number of vertices;\n",
    "2. The time complexity for finding the closest adjacent vertex is $O(|V|)$;\n",
    "3. Updating the distance from a vertex to its neighbors has a worst case time complexity of $O(|E|)$, where $E$ is the number of edges (but in the implementation above, because we represented the graph as an adjacency matrix, the time complexity of this operation will be $O(V)$);\n",
    "4. Repeating the steps 2 and 3 $V$ times. For the algorithm above this means repeating a maximum of $|V| + |V|$ steps, so the time complexity of steps 2,3 and 4 will be $O(|V|^{2})$.\n",
    "5. Returning the value can happen in constant time.\n",
    "\n",
    "The overall worst-case time complexity of this specific implementation of Dijkstra's Algorithm will be $O(|V|) + O(|V|^{2}) = O(|V|^{2})$. However, there are more optimized implementations that have a worst-case time complexity of $O(|E|+|V|log|V|)$. More informations about this optimized version of Dijkstra's Algorithm can be found <a href=\"https://kbaile03.github.io/projects/fibo_dijk/fibo_dijk.html\">here</a>. \n",
    "\n",
    "**Space Complexity Analysis**\n",
    "\n",
    "This implementation of the algorithm uses two auxiliary data structures whose length depends on the number of vertices. The space complexity of this algorithm will therefore be $O(V)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "universal-reminder",
   "metadata": {},
   "source": [
    "> Algorithm analysis should include the following aspects\n",
    ">\n",
    "> - A detailed description of the algorithm\n",
    ">     - Goal of the algorithm\n",
    ">     - Termination condition\n",
    ">     - Criteria for determining that the algorithm is correct\n",
    ">     - Explanation of the steps of the algorithm\n",
    "> - A specific example illustrating how the algorithm works\n",
    "> - Detailed calculation of the time complexities, including best, worst and average cases\n",
    "> - Pseudo-code for complex algorithms (this excludes simple searching and sorting algorithms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "preliminary-garlic",
   "metadata": {},
   "source": [
    "<div id=\"sources\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "upset-bacon",
   "metadata": {},
   "source": [
    "# Sources\n",
    "- Skiena, S. The Algorithm Design Manual. 1998. Springer.\n",
    "- [Udacity - Data Structures & Algorithms in Python](https://classroom.udacity.com/courses/ud513)\n",
    "- [Cambridge Dictionary - Algorithm](https://dictionary.cambridge.org/dictionary/english/algorithm)\n",
    "- [GeeksforGeeks - Space Complexity](https://www.geeksforgeeks.org/g-fact-86/)\n",
    "- [Khan Academy - Asymptotic Notation](https://www.khanacademy.org/computing/computer-science/algorithms/asymptotic-notation/a/asymptotic-notation)\n",
    "- [The Lean Blogs - Some common runtime complexities and their meanings](https://medium.com/learn-with-the-lean-programmer/some-common-runtime-complexities-and-their-meanings-5a2bf4320f48)\n",
    "- [GeeksforGeeks - Array](https://www.geeksforgeeks.org/array-data-structure/)\n",
    "- [GeeksforGeeks - Linked Lists](https://www.geeksforgeeks.org/data-structures/linked-list/)\n",
    "- [GeeksforGeeks - Doubly Linked Lists](https://www.geeksforgeeks.org/doubly-linked-list/)\n",
    "- [GeeksforGeeks - Stacks](https://www.geeksforgeeks.org/stack-data-structure/)\n",
    "- [GeeksforGeeks - Queues](https://www.geeksforgeeks.org/queue-data-structure/)\n",
    "- [CS Dojo - Hash Tables](https://www.youtube.com/watch?v=sfWyugl4JWA&list=PLBZBJbE_rGRV8D7XZ08LK6z-4zPoWzu5H&index=13)\n",
    "- [Ananda Gunawardena - CMU - Hash Table Conflict Resolution](http://www.cs.cmu.edu/~ab/15-121N11/lectures/lecture16.pdf)\n",
    "- [Typeocaml - Height, Depth and Level of a Tree](http://typeocaml.com/2014/11/26/height-depth-and-level-of-a-tree/)\n",
    "- [GeeksforGeeks - Red-Black Trees](https://www.geeksforgeeks.org/red-black-tree-set-1-introduction-2/)\n",
    "- [GeeksforGeeks - Ropes](https://www.geeksforgeeks.org/ropes-data-structure-fast-string-concatenation/)\n",
    "- [Opengenus - Ropes](https://iq.opengenus.org/rope-data-structure/)\n",
    "- [GeeksforGeeks - Heaps](https://www.geeksforgeeks.org/heap-data-structure/)\n",
    "- [HackerRank - Heaps](https://www.youtube.com/watch?v=t0Cq6tVNRBA)\n",
    "- [Tutorialspoint - Graphs](https://www.tutorialspoint.com/data_structures_algorithms/graph_data_structure.htm)\n",
    "- [GeeksforGeeks - Types of Graphs](https://www.geeksforgeeks.org/graph-types-and-applications/)\n",
    "- [BigO complexities [pdf]](http://souravsengupta.com/cds2016/lectures/Complexity_Cheatsheet.pdf)\n",
    "- [GeeksforGeeks - Searching Algorithms](https://www.geeksforgeeks.org/searching-algorithms/)\n",
    "- [CS Dojo - QuickSort](https://www.youtube.com/watch?v=0SkOjNaO1XY&list=PLBZBJbE_rGRV8D7XZ08LK6z-4zPoWzu5H&index=12)\n",
    "- [DeepAI - QuickSort](https://deepai.org/machine-learning-glossary-and-terms/quicksort-algorithm)\n",
    "- [StudyTonight - MergeSort](https://www.studytonight.com/data-structures/merge-sort)\n",
    "- [GeeksforGeeks - HeapSort](https://www.geeksforgeeks.org/heap-sort/)\n",
    "- [University of Maryland - HeapSort Analysis](http://www.cs.umd.edu/~meesh/351/mount/lectures/lect14-heapsort-analysis-part.pdf)\n",
    "- [Back to Back SWE - Counting Sort](https://www.youtube.com/watch?v=1mh2vilbZMg)\n",
    "- [Edd Mann - DFS and BFS](https://eddmann.com/posts/depth-first-search-and-breadth-first-search-in-python/)\n",
    "- [GeeksforGeeks - Dijkstra's Algorithm](https://www.geeksforgeeks.org/python-program-for-dijkstras-shortest-path-algorithm-greedy-algo-7/)\n",
    "- [Kbaile03 - Optimized Dijkstra's Algorithm Analysis](https://kbaile03.github.io/projects/fibo_dijk/fibo_dijk.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
