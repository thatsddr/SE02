{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dramatic-sunrise",
   "metadata": {},
   "source": [
    "# Algorithms and Data Structures Portfolio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "absent-berlin",
   "metadata": {},
   "source": [
    "Domenico Di Ruocco,<br /> CODE University of Applied Sciences,<br/> SE02 Algorithms and Data Structures,<br /> Spring Semester 2021."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incredible-fraction",
   "metadata": {},
   "source": [
    "# Table of Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "significant-compensation",
   "metadata": {},
   "source": [
    "- [1. Introduction](#introductionT)\n",
    "    - [1.1. Time Complexity](#timeComplexity) \n",
    "    - [1.2. Space Complexity](#spaceComplexity)  \n",
    "    - [1.3. Asymptotic Notation](#asymptoticNotation)  \n",
    "        - [1.3.1. Big O](#bigO)  \n",
    "        - [1.3.2. Big Ω](#bigOmega)  \n",
    "        - [1.3.3. Big Θ](#bigTheta)  \n",
    "        - [1.3.4. Working with the Asymptotic Notation](#workingWithNotation)\n",
    "    - [1.4. Order of Dominance in the Asymptotic Limit](#orderOfDominance)\n",
    "    - [1.5. Algorithm Design Techniques](#adt)\n",
    "        - [1.5.1. Brute Force Algorithms](#bruteForce)\n",
    "        - [1.5.2. Divide and Conquer](#divideConquer)\n",
    "        - [1.5.3. Greedy Algorithms](#greedy)\n",
    "        - [1.5.4. Dynamic Programming](#dynamic)\n",
    "        - [1.5.5. Backtracking](#backtracking) \n",
    "- [2. Data Structures](#introductionDS)\n",
    "    - [2.1. Arrays](#arrays)\n",
    "    - [2.2. Linked Lists](#linkedLists)\n",
    "    - [2.3. Stacks](#stacks)\n",
    "    - [2.4. Queues](#queues)\n",
    "    - [2.5. Hash Tables](#hashTables)\n",
    "    - [2.6. Trees](#trees)\n",
    "        - [2.6.1. Binary Trees](#binaryTrees)\n",
    "            - [2.6.1.1. Binary Search Trees](#binarySearchTrees)\n",
    "            - [2.6.1.2. Red-Black Trees](#redBlackTrees)\n",
    "            - [2.6.1.3. Ropes](#ropes)\n",
    "        - [2.6.2. Heaps](#heaps)\n",
    "    - [2.7. Graphs](#graphs)\n",
    "- [3. Algorithms](#introductionA)\n",
    "    - [3.1. Searching Algorithms](#searching)\n",
    "        - [3.1.1. Linear Search](#linearSearch)\n",
    "        - [3.1.2. Binary Search](#binarySearch)\n",
    "    - [3.2. Sorting Algorithms](#sorting)\n",
    "        - [3.2.1. Selection Sort](#selectionSort)\n",
    "        - [3.2.2. Bubble Sort](#bubbleSort)\n",
    "        - [3.2.3. QuickSort](#quickSort)\n",
    "        - [3.2.4. MergeSort](#mergeSort)\n",
    "        - [3.2.5. HeapSort](#heapSort)\n",
    "        - [3.2.6. Counting Sort](#countingSort)\n",
    "    - [3.3. Graph Algorithms](#graphAlgorithms)\n",
    "        - [3.3.1. Depth-First Search](#dfs)\n",
    "        - [3.3.2. Breadth-First Search](#bfs)\n",
    "        - [3.3.3. Dijkstra's Algorithms](#dijkstras)\n",
    "        - [3.3.4. A* Algorithm](#a*)\n",
    "    - [3.4. Graph Algorithms In Action](#maps)\n",
    "        - [3.4.1. Street Networks In Code](#networks)\n",
    "        - [3.4.2. Searching for Paths](#osmnxSearching)\n",
    "            - [3.4.2.1. Shortest Path](#shortestDistance)\n",
    "            - [3.4.2.2. Fastest Path](#shortestTime)\n",
    "            - [3.4.2.3. Acknowledgements on the Examples](#acknowledgements)\n",
    "- [4. Sources](#sources)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "generic-emergency",
   "metadata": {},
   "source": [
    "___ \n",
    "<br />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "referenced-check",
   "metadata": {},
   "source": [
    "<div id=\"introductionT\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "informative-gibson",
   "metadata": {},
   "source": [
    "# 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tender-amino",
   "metadata": {},
   "source": [
    "In Computer Science, an algorithm is a set of instructions that must be followed in a fixed order to calculate an answer to a mathematical problem [Cambridge Dictionary]. Since it is common to find more than one algorithm that has been developed to solve the same problem, we need a way to analyze and compare them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "african-startup",
   "metadata": {},
   "source": [
    "<div id=\"timeComplexity\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "changed-karma",
   "metadata": {},
   "source": [
    "## 1.1. Time Complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "existing-platform",
   "metadata": {},
   "source": [
    "One way to compare two algorithms that solve the same problem, assuming that the solutions provided by both are correct, is to compare the time it takes each of them to get to the solution. The problem with this method is that it depends on the hardware where the algorithm runs. It is for this reason that for machine-independent algorithm design we consider our algorithm to be running on a hypothetical machine called the \"Random Access Machine\" or RAM.\n",
    "\n",
    "On the RAM, we consider each simple operation (+, *, -, =, if, call) and each memory access to take one time step, while loops and subroutines are considered to be the composition of many single-step operations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sorted-indication",
   "metadata": {},
   "source": [
    "<div id=\"spaceComplexity\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excellent-compilation",
   "metadata": {},
   "source": [
    "## 1.2. Space Complexity\n",
    "Another way to compare two algorithms is to compare the total space it takes them to get to the solution. The total space includes the size of the input and the auxiliary space, which is the extra or temporary space used by the algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "italic-flexibility",
   "metadata": {},
   "source": [
    "<div id=\"asymptoticNotation\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alleged-builder",
   "metadata": {},
   "source": [
    "## 1.3. Asymptotic Notation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "controversial-booking",
   "metadata": {},
   "source": [
    "We can use the RAM model to determine the number of steps it will take an algorithm to end with an input we choose, but estimating the worst, average, and best case runtime scenario with the RAM model can be inconvenint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "systematic-thinking",
   "metadata": {},
   "outputs": [],
   "source": [
    "def even_numbers_avg(array):\n",
    "    '''\n",
    "    This function returns either the average of the sum of even numbers,\n",
    "    or None.\n",
    "    '''\n",
    "    even_sum = 0                    #1 time step\n",
    "    even_count = 0                  #1 time step\n",
    "                                    #n times:\n",
    "    for n in array:                    #1 time step\n",
    "        if n % 2 == 0:                 #1 time step\n",
    "            even_sum += n                  #1 time step\n",
    "            even_count +=1                 #1 time step\n",
    "            \n",
    "    if even_count > 0:              #1 time step\n",
    "        return even_sum / even_count     #1 time step\n",
    "    else:                           #1 time step\n",
    "        return None                    #1 time step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bizarre-pride",
   "metadata": {},
   "source": [
    "In the example above, we can try to generalize the time complexity of this algorithm by counting every step, and we will find that in the worst case its time complexity will be: $T(n) = 5n + 6$. Its space complexity will be the size of the array n, plus the two variables we initialize."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "encouraging-chase",
   "metadata": {},
   "source": [
    "The problem with the notation we used above is that is difficult to work precisely with it. In the example above we can see that both return statements have been counted as well as every step in the for loop, which is correct for the worst but not for the average and best-case scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "romantic-dealer",
   "metadata": {},
   "source": [
    "Since we can approximate an algorithm to a mathematical function, we can also determine its growth as a function of the input and define an upper bound function (Big O), a lower bound function (Big Ω), or both (Big Θ) in order to understand how it grows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unique-bidder",
   "metadata": {},
   "source": [
    "In Asymptotic Notation, we only consider the fastest-growing term without any multiplicative constant. E.g.: in the example above $T(n) = 5n + 6$ is $O(n)$ and not $O(5n)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forward-university",
   "metadata": {},
   "source": [
    "<div id=\"bigO\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accompanied-transition",
   "metadata": {},
   "source": [
    "### 1.3.1. Big O"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expected-billion",
   "metadata": {},
   "source": [
    "The Big O of a function is its asymptotic upper bound. This means that the running time of a function $T$ will be always shorter than that of $f$. To generalize we can say that a function $T(n)$ is $O(f(n))$ if there is a constant $k$ such that $T(n) < k·f(n)$ for large enough $n$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hollywood-associate",
   "metadata": {},
   "source": [
    "<figure>\n",
    "    <img src=\"https://i.postimg.cc/CLwn679Y/Big-O.png\" alt=\"Big O\"/>\n",
    "    <figcaption>\n",
    "        Big O. Image source: <a href=\"https://www.khanacademy.org/computing/computer-science/algorithms/asymptotic-notation/a/big-o-notation\">Khan Academy</a>\n",
    "    </figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cardiac-flush",
   "metadata": {},
   "source": [
    "<div id=\"bigOmega\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fancy-fraction",
   "metadata": {},
   "source": [
    "### 1.3.2. Big Ω"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "christian-haiti",
   "metadata": {},
   "source": [
    "The Big Ω of a function is the asymptotic lower bound of that function. This means that the running time of a function $T$ will always be longer than that of $f$. To generalize we can say that a function $T(n)$ is $Ω(f(n))$ if there is a constant $k$ such that $T(n) > k·f(n)$ for large enough $n$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exotic-saturn",
   "metadata": {},
   "source": [
    "<figure>\n",
    "    <img src=\"https://i.postimg.cc/gk4L9RPh/Big-Omega.png\" alt=\"Big Omega\"/>\n",
    "    <figcaption>\n",
    "        Big Ω. Image source: <a href=\"https://www.khanacademy.org/computing/computer-science/algorithms/asymptotic-notation/a/big-big-omega-notation\">Khan Academy</a>\n",
    "    </figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "entitled-conspiracy",
   "metadata": {},
   "source": [
    "<div id=\"bigTheta\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "persistent-court",
   "metadata": {},
   "source": [
    "### 1.3.3. Big Θ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fourth-halifax",
   "metadata": {},
   "source": [
    "The Big Θ of a function is its asymptotic tight bound. This means that the function always runs in a time comprised between the run time of the two asymptotic bounds. To generalize we can say that a function $T(n)$ is $Θ(f(n))$ if there are two constants $k_{1}$ and $k_{2}$\n",
    "such that $T(n) ≥ k_{1}·f(n)$ and $T(n) ≤ k_{2}·f(n)$ for large enough $n$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expanded-strategy",
   "metadata": {},
   "source": [
    "<figure>\n",
    "    <img src=\"https://i.postimg.cc/520Yf0xR/Big-Theta.png\" alt=\"Big Theta\"/>\n",
    "    <figcaption>\n",
    "        Big Θ. Image source: <a href=\"https://www.khanacademy.org/computing/computer-science/algorithms/asymptotic-notation/a/big-big-theta-notation\">Khan Academy</a>\n",
    "    </figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "administrative-defensive",
   "metadata": {},
   "source": [
    "<div id=\"workingWithNotation\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fatal-substitute",
   "metadata": {},
   "source": [
    "### 1.3.4. Working with the Asymptotic Notation\n",
    "\n",
    "1. Addition:\n",
    "\n",
    "    We can sum two functions together and the result will be the dominant one:\n",
    "    $f(n) + g(n) = Θ(max(f(n), g(n)))$.\n",
    "2. Multiplication:\n",
    "\n",
    "    Multiplying a function by a constant will result in the constant being ignored. If instead, we are multiplying two functions, we proceed as follows: $Θ(f(n))·Θ(g(n)) = Θ(f(n)·g(n))$.\n",
    "    \n",
    "\n",
    "**The same rules also apply to Big O and Big Ω**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polish-possibility",
   "metadata": {},
   "source": [
    "<div id=\"orderOfDominance\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sized-script",
   "metadata": {},
   "source": [
    "## 1.4. Order of Dominance in the Asymptotic Limit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "applied-horizontal",
   "metadata": {},
   "source": [
    "Let's consider some common asymptotic growths, valid for both space and time complexity:\n",
    "- Constant: $O(1)$\n",
    "- Logarithmic: $O(log(n))$\n",
    "- Linear: $O(n)$\n",
    "- Quasilinear: $O(n·log(n))$\n",
    "- Quadratic: $O(n^{2})$\n",
    "- Exponential: $O(2^{n})$\n",
    "- Factorial: $O(n!)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "purple-repair",
   "metadata": {},
   "source": [
    "To understand the difference between some of the most common time complexities, take a look at the graph below, where the x-axis represents the size of the input of the functions and the y-axis represents the result of the functions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "billion-portable",
   "metadata": {},
   "source": [
    "<img src=\"https://i.postimg.cc/8kZTNbjT/Comparison.png\" alt=\"Functions Compared\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bizarre-figure",
   "metadata": {},
   "source": [
    "We can see that the dominance order of these functions is: $$n! >> 2^n >> n^2 >> n·logn >> n >> log(n) >> 1$$\n",
    "\n",
    "It is also clear that an efficient algorithm can really make the difference in terms of time and space efficiency, especially as the input size grows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adverse-station",
   "metadata": {},
   "source": [
    "<div id=\"adt\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "modified-taiwan",
   "metadata": {},
   "source": [
    "## 1.5. Algorithm Design Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "digital-serum",
   "metadata": {},
   "source": [
    "Another way to classify algorithms is based on the technique used to implement them. We will now examinate some common techniques and their pros and cons."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "departmental-fishing",
   "metadata": {},
   "source": [
    "<div id=\"bruteForce\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "allied-workshop",
   "metadata": {},
   "source": [
    "### 1.5.1. Brute Force Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fuzzy-duplicate",
   "metadata": {},
   "source": [
    "Brute Force Algorithms are unefficient algorithms that instead of using other techniques to improve their efficiency, rely on computing power to try every possible combination. They are usually easy to implement, and the first solution that may come to mind when trying to solve a problem.<br/> Examples of Brute Force Algorithms include the [Selection Sort](#selectionSort) and [Bubble Sort](#bubbleSort)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regional-conservation",
   "metadata": {},
   "source": [
    "<div id=\"divideConquer\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "refined-millennium",
   "metadata": {},
   "source": [
    "### 1.5.2. Divide and Conquer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "freelance-glance",
   "metadata": {},
   "source": [
    "Algorithms that use the divide and Conquer technique divide the problem they are trying to solve into sub-problems, recursively solve these, and then recombine them to get the final answer. \n",
    "\n",
    "We can calculate the time efficiency of these algorithms, and any other recursive algorithm with the Master Theorem, using the following formula: $T(n) = aT(n/b) + f(n)$, where: $n$ is the size of the input; $a$ is the number of subproblems in the recursion; $n/b$ is the size of each subproblem. All subproblems are assumed to have the same size; and $f(n)$ is the cost of the work done outside the recursive call.\n",
    "\n",
    "One example of algorithm that uses the Divide and Conquer technique is the [MergeSort](#mergeSort)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "equivalent-sperm",
   "metadata": {},
   "source": [
    "<div id=\"greedy\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "olive-cooperation",
   "metadata": {},
   "source": [
    "### 1.5.3. Greedy Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sharp-amino",
   "metadata": {},
   "source": [
    "Greedy Algorithms are used in optimization problems. They always make the 'local optimum' choice to optimize a given objective. They do not guarantee the optimal solution to the problem they are given.\n",
    "\n",
    "An example of greedy algorithm is [Dijkstra's Algorithm](#dijkstras)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "waiting-breed",
   "metadata": {},
   "source": [
    "<div id=\"dynamic\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "actual-presence",
   "metadata": {},
   "source": [
    "### 1.5.4. Dynamic Programming "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "absent-balloon",
   "metadata": {},
   "source": [
    "Dynamic Programming can be considered an optimization of recursion. Whenever an algorithm that uses this technique calls a recursive function, it stores the returned value in a data structure, so that if later the same calculation will be needed again, the algorithm will just look up the result in the data structure. This can improve the time complexity of a probel from exponential to polynomial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spread-terrace",
   "metadata": {},
   "source": [
    "<div id=\"backtracking\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "olive-chester",
   "metadata": {},
   "source": [
    "### 1.5.5. Backtracking Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "knowing-april",
   "metadata": {},
   "source": [
    "Backtracking Algorithms are usually recursive algorithms that try different possibilities until they find the right one. If a piece of the solution computed by the algorithm does not lead to a full solution, these algorithms remove that piece of the solution and backtract to the point where there was another possible alternative.\n",
    "\n",
    "An example of algorithm that uses backtracking is the [Depth-First Search](#dfs) Algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "instrumental-neighbor",
   "metadata": {},
   "source": [
    "___\n",
    "<br />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "marked-bible",
   "metadata": {},
   "source": [
    "<div id=\"introductionDS\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reasonable-fleet",
   "metadata": {},
   "source": [
    "# 2. Data Structures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comparative-panel",
   "metadata": {},
   "source": [
    "Data Structures are constructs that allow you to store and manage data values and provide you with methods to access or manipulate such data. There are different kinds of data structures available, each with its pros and cons. It is important to understand the strengths and weaknesses of different Data Structures in order to choose the right one for your use case and make your software more efficient.\n",
    "\n",
    "Data structures can be classified into \"contiguous\" and \"linked\". The former are based upon arrays and the latter on pointers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adjusted-characteristic",
   "metadata": {},
   "source": [
    "<div id=\"arrays\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "failing-royalty",
   "metadata": {},
   "source": [
    "## 2.1. Arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "split-covering",
   "metadata": {
    "tags": []
   },
   "source": [
    "An Array is an example of contiguous data structure. They are collections of data of fixed size allocated in contiguous memory locations, which make accessing the data values by index really efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "transsexual-friend",
   "metadata": {},
   "source": [
    "<figure>\n",
    "    <img src=\"https://i.postimg.cc/brxbSFMV/Array.png\" alt=\"Example of array in memory\" width=\"450\"/>\n",
    "    <figcaption>\n",
    "        Array. Image source: <a href=\"https://www.geeksforgeeks.org/array-data-structure\">Geeks for Geeks</a>\n",
    "    </figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cardiac-update",
   "metadata": {},
   "source": [
    "Analysis of common operations on arrays:\n",
    "\n",
    "- Access:\n",
    "\n",
    "    Since the values are indexed and this is a data structure contiguous in memory it is possible to access data in the array with worst-case time complexity of $O(1)$ (constant time complexity);\n",
    "- Search:\n",
    "\n",
    "    The data stored in an array is not always sorted, so we need to assume that every memory slot could be searched before finding the element we are looking for, therefore the time complexity of this operation is $O(n)$ (linear time complexity);\n",
    "- Insertion:\n",
    "\n",
    "    To insert an element at a specific index we might, in the worst case scenario, need to shift all the other elements in the array, so the time complexity for this operation will be $O(n)$;\n",
    "- Deletion:\n",
    "\n",
    "    To delete an element in an array we may run into the same issue of insertion, hence the time complexity will be, again, $O(n)$.\n",
    "\n",
    "The space efficiency of arrays is also one of its major strengths since arrays are only made up of pure data, no space is wasted.\n",
    "\n",
    "Another advantage of arrays is their memory locality, which takes full advantage of the speed of cache memory.\n",
    "\n",
    "Their main disadvantage is the impossibility of changing their size while the program is running. It is however possible to avoid this limitation using dynamic arrays, arrays whose size doubles every time they are full.\n",
    "\n",
    "Arrays are very common data structures, and they can be used to store basically every kind of data. Other data structures, like Hash Tables, Graphs, and Heaps, are also based on them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "manufactured-bolivia",
   "metadata": {},
   "source": [
    "<div id=\"linkedLists\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "molecular-dover",
   "metadata": {},
   "source": [
    "## 2.2. Linked Lists"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "injured-suicide",
   "metadata": {},
   "source": [
    "Linked lists are an example of linked data structures. They are collections of data not allocated in contiguous memory locations but in which the elements are linked using a pointer to the next value in the case of a Singly Linked List or to both the previous and the next in the case of a Doubly Linked List.\n",
    "Linked lists are made up of \"Nodes\", the first one of which is called Head. Every node has a pointer that points to the next node (or to a null value in case it is the last element), while in case they are doubly linked list they also have a pointer to the previous value (or to a null value in case of the first element)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continental-horse",
   "metadata": {},
   "source": [
    "<figure>\n",
    "    <img src=\"https://i.postimg.cc/SKLp56BF/Linked-List.png\" alt=\"Representation of singly linked lists\" width=\"500\"/>\n",
    "    <figcaption>\n",
    "        Singly Linked List. Image source: <a href=\"https://www.geeksforgeeks.org/data-structures/linked-list/\">Geeks for Geeks</a>\n",
    "    </figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lesser-pearl",
   "metadata": {},
   "source": [
    "<figure>\n",
    "    <img src=\"https://i.postimg.cc/VLT1bLqT/Doubly-Linked-List.png\" alt=\"Representation of doubly linked lists\" width=\"500\"/>\n",
    "    <figcaption>\n",
    "        Doubly Linked List. Image source: <a href=\"https://www.geeksforgeeks.org/doubly-linked-list/\">Geeks for Geeks</a>\n",
    "    </figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aware-monaco",
   "metadata": {},
   "source": [
    "Analysis of common operations on linked lists:\n",
    "\n",
    "- Access / Search:\n",
    "\n",
    "    Linked Lists (both Singly and Dobly Linked) are not indexed data structures. This means that we cannot access one value directly, but we need to start from the Head (or also from the last element if we are in a Doubly Linked List) and follow the pointers until we find the element we are looking for or we find a null value. For this reason, the time complexity of this operation is $O(n)$;\n",
    "\n",
    "- Insertion:\n",
    "\n",
    "    To insert a new node \"B\" in a Singly Linked List, between two nodes \"A\" and \"C\", we just need to make sure that the node \"A\" points to node \"B\" and that node \"B\" points to node \"C\". If the list is a Doubly Linked one, we also need to make sure that the backward pointer of node \"C\" and \"B\" is set correctly. \n",
    "    We can also add an element at the beginning of a Linked List by simply making it the new head and set its pointer to the previous head, and in case it is a Doubly Linked List we also need to point the backward pointer of the previous Head to the new Head. The time complexity of this operation, after you know the position where you want to insert a new element is $O(1)$, since you only need to change the value of the pointer(s);\n",
    "- Deletion:\n",
    "\n",
    "    The same concept from the insertion operation applies here, except that instead of changing the pointer(s) to include a new element we change them to exclude one. For this reason, the time complexity of this operation is also $O(1)$.\n",
    "\n",
    "Linked lists are not really space-efficient since they need to store the pointers and the extra space needed will be $O(n)$.\n",
    "\n",
    "As we have seen, their main disadvantages are that they are slow in searching and occupy more memory than arrays. Moreover, another disadvantage is that they don't benefit from the speed of cache memory since they are not stored contiguously.\n",
    "\n",
    "Lists, just like arrays, are also very common data structures. They are mainly used where dynamic memory allocation is required. Trees are an example of data structure implemented using a modified version of linked lists."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recent-third",
   "metadata": {},
   "source": [
    "<div id=\"stacks\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prescription-trigger",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2.3. Stacks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complicated-bumper",
   "metadata": {},
   "source": [
    "A Stack is a linear data structure. They allow two operations: insertion at the top (push) and read and removal at the top (pop). For this reason, stacks follow the \"Last In First Out\" or \"LIFO\" order."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "optimum-raleigh",
   "metadata": {},
   "source": [
    "<figure>\n",
    "    <img src=\"https://i.postimg.cc/bwXWhGHz/Stack.png\" alt=\"Representation of a stack\" width=\"500\"/>\n",
    "    <figcaption>\n",
    "        Stacks. Image source: <a href=\"https://www.geeksforgeeks.org/stack-data-structure/\">Geeks for Geeks</a>\n",
    "    </figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "floppy-handbook",
   "metadata": {},
   "source": [
    "Analysis of common operations on Stacks:\n",
    "    \n",
    "- Access / Search:\n",
    "\n",
    "    Access an element in a Stack entails that we need to read and remove the top item until we reach the one we're looking for or we are left with an empty Stack. Because of this, the time complexity of this operation will depend on its size and will therefore be $O(n)$;\n",
    "\n",
    "- Insertion:\n",
    "\n",
    "    We can only insert an element at the top of the Stack, and for this reason, this operation will have a time complexity of $O(1)$ (constant time);\n",
    "- Deletion:\n",
    "\n",
    "    Just like insertion, we can only delete the top element of a Stack and thus the time complexity of this operation will also be $O(1)$.\n",
    "    \n",
    "\n",
    "Depending on how Stacks are implemented they can be more or less space-efficient.\n",
    "\n",
    "Their main advantage is that they are easy to implement and that that the insertion and deletion operations are really time efficient.\n",
    "\n",
    "Uses of Stacks include situations in which the order in which the elements are inserted/deleted is not important, or when you specifically need to retrieve the last elements first. An example of implementation could be a social media feed or a chat app, because in both cases you need to retrieve the latest information first."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forty-policy",
   "metadata": {},
   "source": [
    "<div id=\"queues\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "planned-steel",
   "metadata": {},
   "source": [
    "## 2.4. Queues"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brilliant-upper",
   "metadata": {},
   "source": [
    "A Queue is a linear data structure similar to the Stack but that supports a different set of operations, Enqueue (inserting an item at the rear of the queue) and Dequeue (reading and deleting an item from the front of the queue). Instead of the LIFO, Queues follow the FIFO order (First In, First Out)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ethical-inflation",
   "metadata": {},
   "source": [
    "<figure>\n",
    "    <img src=\"https://i.postimg.cc/tJYwXWMg/Queue.png)\" alt=\"Representation of a queue\" width=\"500\"/>\n",
    "    <figcaption>\n",
    "        Queues. Image source: <a href=\"https://www.geeksforgeeks.org/queue-data-structure/\">Geeks for Geeks</a>\n",
    "    </figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "serious-meditation",
   "metadata": {},
   "source": [
    "Analysis of common operations on Queues:\n",
    "    \n",
    "- Access / Search:\n",
    "\n",
    "    Access an element in a Queue is similar to the access operation in a Stack, we need to read and remove (dequeue) the front item until we find the one we are looking for or no elements are left in the Queue. The time complexity of this operation will hence be $O(n)$;\n",
    "\n",
    "- Insertion:\n",
    "\n",
    "    We can only insert an element at the rear of the Queue, and for this reason, this operation will have a time complexity of $O(1)$ (constant time);\n",
    "- Deletion:\n",
    "\n",
    "    We can only delete the front element of a Queue and thus the time complexity of this operation will also be $O(1)$.\n",
    "\n",
    "The space complexity of stacks also depends on how they are implemented.\n",
    "\n",
    "Just like Stacks, they are also easy to implement and that the insertion and deletion operations are really time efficient.\n",
    "\n",
    "It is worth mentioning that another type of Queues, Priority Queues, are also widely used in computer science. They are different from a normal Queue because they don't follow the FIFO order, but they prioritize elements with a higher (or lower) priority. Priority Queues are usually implemented using the [Heap Data Structure](#heaps), so the time complexity to insert an element in a priority queue is the same of insertin an element in a Heap ($O(n*log(n))$). We will see them in action while analyzing the [A* Algorithm](#a*).\n",
    "\n",
    "Queues are useful in situations in which the order in which the elements are retrieved matters. An example of implementation is every service that needs to handle the users' requests in the order that they were made (like for online payments)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aware-semester",
   "metadata": {},
   "source": [
    "<div id=\"hashTables\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "massive-distribution",
   "metadata": {},
   "source": [
    "## 2.5. Hash Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "common-traveler",
   "metadata": {},
   "source": [
    "A Hash Table is a data structure in which a key-value data pair is stored. It is usually implemented with an array and it works by hashing (generating a unique value, integer in this case) the key and storing the key-value data at the index returned by the hash function. In this way, given the key, it is really efficient to locate its value.\n",
    "\n",
    "The main challenge of Hash Tables is to remain memory-efficient while avoiding collisions (having 2 or more elements at the same index)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "passive-recognition",
   "metadata": {},
   "source": [
    "<figure>\n",
    "    <img src=\"https://i.postimg.cc/7LrkmsQn/Hash-Table.png\" alt=\"Representation of a hash table\" width=\"400\"/>\n",
    "    <figcaption>\n",
    "        Hash Table. Image source: <a href=\"https://en.wikipedia.org/wiki/Hash_table#/media/File:Hash_table_3_1_1_0_1_0_0_SP.svg/\">Wikipedia</a>\n",
    "    </figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cardiac-haven",
   "metadata": {},
   "source": [
    "Analysis of common operations on Hash Tables:\n",
    "\n",
    "- Search / Access:\n",
    "\n",
    "    Since the data stored in a Hash Table is indexed, it will take constant time to search or access it ($O(1)$), even if depending on the hashing algorithm, it could depend on the size of the key. Other edge cases include collision, and we will take such situation into account later;\n",
    "\n",
    "- Insertion:\n",
    "\n",
    "    To insert an element in a Hash table, we just need to execute the hash function and insert the data in the array, which happens, depending on the hashing function, wither with a time complexity of $O(1)$ (constant time), or with time complexity dependent on the key size. This can change in case of collisions;\n",
    "- Deletion:\n",
    "\n",
    "    Deleting an element in a Hash Table is a process similar to searching for it, except that instead of reading it, it gets deleted. Normally this process happened with time complexity of $O(1)$, but also this operation can be slowed down by collisions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continuing-solid",
   "metadata": {},
   "source": [
    "**Collisions** are more frequent as the array fills up because the empty slots are fewer. It is common to use the variable $a=n/m$ (where $n$ is the number of elements and $m$ the length of the array) to measure how full is an array. Once a collision happens there are different ways to deal with it:\n",
    "- Chaining:\n",
    "\n",
    "    We add more than 1 key-value pair to the same index. In this way inserting a new element has always a time complexity of $O(1)$ while searching for an element a time complexity of $(O(1+a))$;\n",
    "    \n",
    "- Open Addressing:\n",
    "\n",
    "    We store the elements in the same array without using additional data structures. Ways to find a new index include:\n",
    "    \n",
    "    - Linear Probing:\n",
    "\n",
    "        We store the colliding element in the first available slot in the array. This method, however, can be really inefficient. Inserting and searching for an element can have a time complexity of $O(n)$;\n",
    "    - Quadratic Probing:\n",
    "        \n",
    "        We find a new index by adding an arbitrary number that increases quadratically. Just like Linear probing, this method can be really inefficient with inserting and sorting operations that can have a time complexity of $O(n)$;\n",
    "    - Double Hashing:\n",
    "\n",
    "        We generate a new hash if a collision is detected. We can choose between different functions to implement this technique, but generally, this method is more time-efficient compared to the other 2, especially in searching."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "postal-surrey",
   "metadata": {},
   "source": [
    "Hash Table are not really space-efficient.\n",
    "\n",
    "Their main advantage is the efficiency with which insert the insert operation can be carried out.\n",
    "\n",
    "Hash Tables are usually implemented with built-in data structures like python dictionaries and can be used for a variety of things, from the implementation of an actual dictionary to entries of a NoSQL database."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incorrect-license",
   "metadata": {},
   "source": [
    "<div id=\"trees\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "future-appeal",
   "metadata": {},
   "source": [
    "## 2.6. Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "textile-checklist",
   "metadata": {},
   "source": [
    "Trees are non-linear data structures that can be considered an extension of a linked list. In a Tree, each node points to some \"children\" nodes or a null value, creating a hierarchical data structure.\n",
    "\n",
    "Trees have a lot of properties, understand them we are going to take into consideration the Binary Tree in the image below (a Binary Tree is a Tree in which each node has at most 2 children)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "technological-denial",
   "metadata": {},
   "source": [
    "<figure>\n",
    "    <img src=\"https://i.postimg.cc/jjnDJFn6/Binary-Tree.png\" alt=\"Representation of a binary tree\" width=\"500\"/>\n",
    "    <figcaption>\n",
    "        Binary Tree. Image source: <a href=\"https://www.geeksforgeeks.org/binary-tree-data-structure//\">GeeksforGeeks</a>\n",
    "    </figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bulgarian-official",
   "metadata": {},
   "source": [
    "**Properties and Terminology of Trees:**\n",
    "\n",
    "- **Node**: an element of the Tree, contains data and pointers to its children;\n",
    "- **Edge**: The \"link\" between 2 nodes, every Tree has a maximum of $N-1$ edges, where $N$ is the number of nodes;\n",
    "- **Parent Node**: the predecessor of a node, or the node that points to another. In the example above, among the others, \"1\" is the parent of \"2\" and \"3\", and \"2\" is the parent of \"4\" and \"5\";\n",
    "- **Child node**: the descendant of a node. In the example above, among the others, \"11\" is a child of \"2\" and \"3\" is a child of \"1\";\n",
    "- **Root Node**: The first element of the Tree and the only node without a parent node, in the example above the node is \"1\";\n",
    "- **Siblings Nodes**: nodes that are children of the same parent node. In the example above, \"4\" and \"5\" and \"8\" and \"9\" are examples of siblings;\n",
    "- **Leaf**: a node without children, like \"11\" or \"14\" in the example above.\n",
    "- **Internal Node**: a node with at least 1 child. \"7\", \"4\" and \"1\" are internal nodes in the example above;\n",
    "- **Degree**: the number of children that a node has. In a binary tree, this number is never greater than 2. The \"Degree of Tree\" is the Degree of the node with the highest Degree;\n",
    "- **Level**: the \"distance\" of a Node from the Root Node, starting at 1. In the example above, the level of \"1\" is 1, the level of \"3\" is 2, the level of \"5\" is 3 and the level of \"9\" is 4;\n",
    "- **Height**: is the \"distance\" between the furthest descending leaf and a node, starting at 0 for the leaves. In the example above, the height of \" 10\" is 0, the height of \"4\" is 1, the height of \"3\" is 2 and the Height of \"1\" is \"4\". The Height of the Root Node is also the Height of the Tree.\n",
    "- **Depth**: the number of edges between a Node and the Root Node. For example, it is 0 for \"1\" and 2 for \"7\" in the Tree above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "another-sunrise",
   "metadata": {},
   "source": [
    "Analysis of common operations on trees:\n",
    "\n",
    "- Access:\n",
    "    \n",
    "    Since a Tree is not an indexed data structure, in the worst case it is possible that we need to look through all the nodes until we find the one we are looking for. For this reason, the time complexity of this operation is $O(n)$;\n",
    "- Search:\n",
    "    \n",
    "    Trees are not ordered data structures, or at least not all of them. For this reason, searching in a tree could also mean that we need to search through all the other nodes, either with a Depth-First-Search or a Breadth-First-Search approach (we will look at both of these algorithms in the algorithms section of this portfolio) with a time complexity that in both cases is of $O(n)$;\n",
    "- Insertion:\n",
    "    \n",
    "    Inserting a Node in a tree may require changing the positions of the other nodes as well to keep the properties of the tree. For this reason, this operation also has a time complexity of $O(n)$;\n",
    "- Deletion:\n",
    "\n",
    "    Deletion, just like insertion, may require rearranging all the other Nodes and so this operation also happens in $O(n)$ time complexity.\n",
    "\n",
    "The space complexity of trees is $O(n)$, since they need to store a number of pointers that grows linearly with the number of nodes.\n",
    "\n",
    "Given that there are a lot of subcategories of trees, they can be used in a lot of different ways. We will now look at some specific types of trees and discuss the real-world implementations of each."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "crazy-basement",
   "metadata": {},
   "source": [
    "<div id=\"binaryTrees\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "raising-store",
   "metadata": {},
   "source": [
    "### 2.6.1. Binary Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "centered-single",
   "metadata": {},
   "source": [
    "As I mentioned before, a Binary Tree is a Tree in which is node can have a maximum of 2 children, therefore each node contains some data, a pointer to its left child and a pointer to its right child."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mature-leader",
   "metadata": {},
   "source": [
    "A binary tree is not an ordered tree by definition, so the time complexity of common operations is the same as that of a normal tree.\n",
    "\n",
    "There are, however, a lot of different implementations of Binary Trees that make them more efficient in those common operations:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cooked-scholarship",
   "metadata": {},
   "source": [
    "<div id=\"binarySearchTrees\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "occupational-julian",
   "metadata": {},
   "source": [
    "### 2.6.1.1. Binary Search Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "convertible-stanley",
   "metadata": {},
   "source": [
    "Binary Search Trees (BST) are Binary Trees in which the left child of a node (and all of its children) have a value smaller than that of the parent node and the right child of a node (and all of its children) have a value bigger than that of the parent node. As the name suggests, BSTs are used to implement the  Binary Search algorithm on them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "about-queensland",
   "metadata": {},
   "source": [
    "<figure>\n",
    "    <img src=\"https://i.postimg.cc/P5RLJjJr/BST.png\" alt=\"Representation of a binary search tree\" width=\"400\"/>\n",
    "    <figcaption>\n",
    "        Binary Tree. Image source: <a href=\"https://www.geeksforgeeks.org/binary-search-tree-data-structure/\">GeeksforGeeks</a>\n",
    "    </figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "distinguished-halifax",
   "metadata": {},
   "source": [
    "Analysis of common operations on BSTs:\n",
    "\n",
    "- Search / Access:\n",
    "    \n",
    "    Binary Search Trees are not indexed data structure, so to access an element we need to search for it. BSTs are ordered data structures that work really well with the Binary Search algorithm (an analysis of this algorithm can be found in the algorithms part of this portfolio). Because of the fact that we can use Binary Search with this data structure, we can find an element with time complexity of $O(h)$, where $h$ is also the height of the tree;\n",
    "    \n",
    "- Insertion:\n",
    "    \n",
    "    Inserting a Node in a BST may require us to change the order of all the other nodes, so in the worst-case scenario the time complexity of this operation will be $O(n)$, but in the average case the time complexity of this operation will depend on the height of the tree, so $Θ(h)$;\n",
    "- Deletion:\n",
    "\n",
    "    Deleting a Node has the same impact as inserting a Node. the time complexity of this operation will therefore be $Θ(h)$ in the average case and $O(n)$ in the worst.\n",
    "    \n",
    "An implementation of a BST could be using it together with a binary search algorithm.\n",
    "\n",
    "The main problem with BSTs is that they can be unbalanced when the nodes skew to one of the sides of the tree. In that case, the height of the tree is $n$ and the operations in the tree become inefficient. To avoid this and have a height of $log(n)$ we can use a self-balancing Binary Tree."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "original-effects",
   "metadata": {},
   "source": [
    "<div id=\"redBlackTrees\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pleased-remainder",
   "metadata": {},
   "source": [
    "### 2.6.1.2. Red-Black Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reported-hawaiian",
   "metadata": {},
   "source": [
    "Red-Black trees are a common kind of self-balancing Binary Trees in which: \n",
    "- Each node has a color property (either red or black); \n",
    "- The root is always black; \n",
    "- A Node cannot have the parent or children of its the same color, except if one or both of its children are leaves; \n",
    "- Its leaves have a null value and are considered black; \n",
    "- And every path from a node to any of its null descendants contains the same number of black nodes.\n",
    "\n",
    "Red-Black Trees, just like BSTs, are used to implement the Binary Search algorithm on them, but these are usually more efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "possible-crazy",
   "metadata": {},
   "source": [
    "<figure>\n",
    "    <img src=\"https://i.postimg.cc/Xv4PrTzm/RedBlack.png\" alt=\"Representation of a red black tree\" width=\"500\"/>\n",
    "    <figcaption>\n",
    "        Red Black Tree. Image source: <a href=\"https://en.wikipedia.org/wiki/Red–black_tree#/media/File:Red-black_tree_example.svg\">Wikipedia</a>\n",
    "    </figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ignored-relation",
   "metadata": {},
   "source": [
    "Analysis of common operations on Red-Black Trees:\n",
    "\n",
    "- Search / Access:\n",
    "    \n",
    "    It is possible to perform a Binary Search on a Red-Black Tree, which as we have seen before allows us to search an element with a time complexity of $O(h)$. Since Red-Black trees are a balanced data structure, $h$ will be $log(n)$ and therefore this operation will happen with time complexity of $O(log(n))$;\n",
    "    \n",
    "- Insertion:\n",
    "    \n",
    "    Since a Red-Black Tree is a balanced Tree, and the insertion operation depends on the height of the tree, we can say that this operation will have a worst-case time complexity of $O(log(n))$;\n",
    "- Deletion:\n",
    "\n",
    "    Deleting a node can have the same impact as inserting a node, and this operation depends on the height of the Tree too. Since the height of a Red-Black tree is $O(log(n))$, this operation will have a time complexity of $O(log(n))$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coordinated-simon",
   "metadata": {},
   "source": [
    "<div id=\"ropes\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "perfect-surgeon",
   "metadata": {},
   "source": [
    "### 2.6.1.3. Ropes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solar-variable",
   "metadata": {},
   "source": [
    "A Rope is a Binary Tree used for string manipulation. In a rope, each leaf holds a substring and each inner node the total length of the substrings that are descendants of its left child.\n",
    "\n",
    "In the example below, the value of root node is the total length of the string, which is not a mandatory feature but can be useful in common operations, as we will see below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "relative-vegetarian",
   "metadata": {},
   "source": [
    "<figure>\n",
    "    <img src=\"https://i.postimg.cc/m21JJ39X/Rope.jpg\" alt=\"Representation of a rope\" width=\"500\"/>\n",
    "    <figcaption>\n",
    "        Rope. Image source: <a href=\"https://www.geeksforgeeks.org/ropes-data-structure-fast-string-concatenation/\">GeeksForGeeks</a>\n",
    "    </figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cardiovascular-registrar",
   "metadata": {},
   "source": [
    "Common operations that can be done on a rope are different than those done on other trees. These operations include:\n",
    "\n",
    "- Index:\n",
    "\n",
    "    Searching an element by their index is a very common operation in string manipulation, and thanks to the value of the inner nodes this operation can be done on Ropes with time complexity of $O(log(n))$. \n",
    "    \n",
    "    To understand how that is possible let's consider an example, finding the character with at the position $i=8$ in the rope in the image above. We start by comparing $i$ to the value of the root (which in this case holds the value of the total length), and we quickly determine if the index is part of the string. Since it is smaller than the value of A, we move to A's left child (B). we compare $i$ to the value of B and since 8 is smaller than 9 we move to B's left child. We compare $i$ to the value of C (6) and since 8 is bigger, we move to  C's right child (F) and since we're moving to the right we update $i$ to be $i-C$ (8-6 = 2), and since F is a leaf we access the child at position $i$ (which is now 2) of the substring (y), which is the 8th character of the whole string.\n",
    "\n",
    "- Concat:\n",
    "\n",
    "    To concatenate 2 Ropes we just need to assign them to a new common root node with the value equal to the sum of the length of the substrings that descend from its new left child. This operation can be made in $O(1)$ time complexity, but computing the value for the new root node is an operation that has a time complexity of $O(h)$, where $h$ is the height of the tree and is equal to $log(n)$ in a balanced tree.\n",
    "\n",
    "- Split:\n",
    "\n",
    "    When splitting a string starting from an index we need to make a distinction between 2 major cases: we need to start splitting after the last character of a leaf, or we need to start splitting starting from a middle character of a leaf. If our case is the latter, we assign 2 children to the leaf (which becomes an inner node), the left one containing the character that we don't need to split, and the right one containing the characters that we need to split. \n",
    "    \n",
    "    After finding the leaf from which we need to split the Rope, we separate the nodes at the right of that leaf and we fix the weight of the inner nodes that were ancestors of the nodes we separated. We then assign the split nodes to a new common root node.\n",
    "    \n",
    "    At this point, it may be necessary to rebalance both Ropes. \n",
    "    \n",
    "    This operation has a time complexity of $O(log(n))$ since it is the sum of the time complexities of the operations that are needed to complete this operation;\n",
    "\n",
    "- Insert:\n",
    "\n",
    "    Inserting a Rope in the middle of another Rope is an operation that can be done by splitting the original Rope, concatenate the Rope we need to insert, and then concatenate the right part of the node we originally split. Rebalancing the tree may be also required. The time complexity of this operation will be the sum of the time complexities of 1 split operation and 2 concatenation operations $(O(log(n))$;\n",
    "\n",
    "- Delete:\n",
    "\n",
    "    To delete a substring at the middle of a Rope, we need to split the original rope starting at the first character that we want to delete. We then split the resulting right Rope starting after the last character that we need to delete, and we finally concatenate the left rope of the first split operation with the right Rope of the last split operation. This operation will also have a time complexity given by the sum of the operations that it uses, which will result in time complexity of $O(log(n))$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "informative-platinum",
   "metadata": {},
   "source": [
    "Ropes are widely used in softwares that need to manage large texts, such as text editors and email clients, because of their performances in managing strings, especially compared with a traditional string implementation (an array of characters) and because they do not require contiguos memory allocation. Some of the disadvantages of ropes include the fact that they occupy more space than an array of string and their complexity, which can often lead to bugs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "valid-increase",
   "metadata": {},
   "source": [
    "<div id=\"heaps\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "integral-nebraska",
   "metadata": {},
   "source": [
    "### 2.6.2. Heaps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solved-representative",
   "metadata": {},
   "source": [
    "Heaps are trees in which the parent node always stores a value smaller than that of its children (in the case of a Min Heap) or bigger than that of its children (in case of a Max Heap). In this way, the root node always stores the smallest value (in a Min Heap) or the biggest value (in a Max Heap).\n",
    "\n",
    "Heaps do not need to be Binary Trees, but they need to be complete (every level should have the maximum amount of nodes) and if they are not, new elements are added to the incomplete level from left to right. Because of this last property, Heaps are usually stored as arrays."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fiscal-dynamics",
   "metadata": {},
   "source": [
    "<figure>\n",
    "    <img src=\"https://i.postimg.cc/02LRyXx1/Heap.png\" alt=\"Representation of a heap\" width=\"500\"/>\n",
    "    <figcaption>\n",
    "        Heap. Image source: <a href=\"https://www.geeksforgeeks.org/heap-data-structure\">GeeksForGeeks</a>\n",
    "    </figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "developing-sugar",
   "metadata": {},
   "source": [
    "Analysis of common operations on Heaps:\n",
    "\n",
    "- Search / Access:\n",
    "\n",
    "    Searching an element that is not the root node (the node with the max value in a Max Heap or the node with a min value in a Min Heap), we may need to search through all the nodes to find the one we're looking for. Because of this, this operation will have a time complexity of $O(n)$;\n",
    "- Insertion:\n",
    "    \n",
    "    \n",
    "   To insert an element in its correct position in a Heap we need to start by appending it to the last level, which can be done with average time complexity of $O(1)$ (if the heap is stored in an array). We then need to switch it with its parent node (in case the parent node is smaller and our heap is a Max Heap or in case the parent node is bigger and our Heap is a Min Heap) until it satisfies the properties of the heap. This second operation has a time complexity of $O(log(n))$;\n",
    "- Deletion:\n",
    "   \n",
    "   \n",
    "   To delete an element from a Heap we may also need to rearrange its nodes until the properties of the heap are satisfied. To do this we may need to switch an element for every level of the Heap and since the number of levels is given by $log(n)$ the time complexity of this operation will be $O(log(n))$.\n",
    "\n",
    "\n",
    "Heaps are mainly used as an auxiliary data structure in various algorithms, like the Heapsort algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sustainable-faculty",
   "metadata": {},
   "source": [
    "<div id=\"graphs\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mobile-pixel",
   "metadata": {},
   "source": [
    "## 2.7. Graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "revised-relief",
   "metadata": {},
   "source": [
    "Graphs are non-linear data structures made of vertices (or nodes) that store data and edges (that can also store data). Graphs are used to represent the relationships between its nodes or vertices. We have already examined a subset of Graphs, Trees."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quarterly-devon",
   "metadata": {},
   "source": [
    "<figure>\n",
    "    <img src=\"https://i.postimg.cc/283r6BZh/Graph.png\" alt=\"Representation of a graph\" width=\"500\"/>\n",
    "    <figcaption>\n",
    "        Graph. Image source: <a href=\"https://www.geeksforgeeks.org/graph-data-structure-and-algorithms/\">GeeksForGeeks</a>\n",
    "    </figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "revised-slovakia",
   "metadata": {},
   "source": [
    "Graphs terminology:\n",
    "\n",
    "- **Vertex** (or Node), an element of the graph that always contains some data;\n",
    "- **Edge**, the relationship between 2 vertices, can also contain information;\n",
    "- **Adjacency**, two nodes connected via an edge;\n",
    "- **Path**, a sequence of edges between 2 vertices;\n",
    "- **Eulerian Path**, a path that visits every edge once (but can visit vertices more than once) and ends up in a vertex which is not the starting one;\n",
    "- **Eulerian Cycle**: a path that visits every edge once (but can visit vertices more than once) and ends up in the starting vertex;\n",
    "- **Hamiltonian Path**, a path that visits every vertex only once (but can visit edges more than once) and ends up in a vertex which is not the starting one;\n",
    "- **Hamiltonian Cycle**: a path that visits every vertex only once (but can visit edges more than once) and ends up in the starting vertex;\n",
    "- **Parallel Edges**, two or more edges that connect the same vertices;\n",
    "- **Loop**, an edge that connects a node to itself."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "linear-timeline",
   "metadata": {},
   "source": [
    "Types of Graphs:\n",
    "\n",
    "- **Finite**. A finite Graph contains a finite number of edges and vertices;\n",
    "- **Infinite**. An infinite Graph contains an infinite number of vertices and edges;\n",
    "- **Trivial**. A trivial Graph contains only one vertex and no edges;\n",
    "- **Simple**. A simple Graph contains only one edge between a pair of vertices;\n",
    "- **Non Simple**. A non-simple Graph contains more than one edge between a pair of vertices;\n",
    "- **Multi-Graph**. A multi-graph contains some parallel edges but no loops;\n",
    "- **Pseudo-Graph**. a pseudo-graph is a graph with at least a loop and a parallel edge;\n",
    "- **Null**. A null graph contains vertices but no edges;\n",
    "- **Complete** (or Full Graph). In a complete graph every vertex is adjacent to all the others;\n",
    "- **Unweighted**. In an unweighted graph the edges do not store data;\n",
    "- **Weighted**. In a weighted graph the edges store data;\n",
    "- **Directed**. In a directed graph the edges connect 2 vertices only in one direction;\n",
    "- **Undirected**. In an undirected graph the edges connect 2 vertices in both directions;\n",
    "- **Topological**. In Topological Graphs, the vertices are represented by distinct points in space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pressing-chance",
   "metadata": {},
   "source": [
    "Ways of representing a graph:\n",
    "\n",
    "- **Adjacency List:**\n",
    "\n",
    "    With an Adjacency List, we use an array to store information about the graph. In an Adjacency List, the array element with the same index as the id of a vertex contains information about its adjacent vertices. An Adjacency List for the graph in the image above will look like this: ```[[1,4],[0,2,3,4],[1,3],[1,2,4],[0,1,3]]```.\n",
    "    \n",
    "    Analysis of common operations on Adjacency Lists:\n",
    "    \n",
    "    - Storage:\n",
    "    \n",
    "        Storing a graph as an Adjacency List can be done with time complexity of $O(|V| + |E|)$, where $V$ is the number of vertices and also the length of the array, and $E$ is the number of edges;\n",
    "        \n",
    "    - Add Vertex:\n",
    "    \n",
    "        Adding a vertex to a graph represented as an Adjacency List can be done with time complexity of $O(1)$ since we just need to store a new element to the list;\n",
    "    \n",
    "    - Add Edge:\n",
    "    \n",
    "        Adding an edge to a graph represented as an Adjacency List can be done with time complexity of $O(1)$, since we just need to add to the arrays representing the two nodes 1 value;\n",
    "    \n",
    "    - Remove Vertex:\n",
    "    \n",
    "        Removing a vertex from a graph represented as an Adjacency List can be done with time complexity of $O(|V|+|E|)$, since we need to remove the edges to that vertex from all the other vertices as well;\n",
    "    \n",
    "    - Remove Edge:\n",
    "    \n",
    "        Removing an edge from a graph represented as an Adjacency List can be done with time complexity of $O(|E|)$, since we need to search and remove the edge from the list of edges of the two nodes that it connects.\n",
    "<br/>\n",
    "The space complexity of an Adjacency List is $O(|V|+|E|)$.\n",
    "    \n",
    "    \n",
    "- **Adjacency Matrix:**\n",
    "\n",
    "    With an Adjacency Matrix, we use a 2D matrix to store information about the graph. In an Adjacency Matrix, the array element with the same index as the id of a vertex contains an array that indicates if a vertex is connected to another or not (1 if it is, 0 if it is not). An Adjacency Matrix for the graph in the image above will look like this:\n",
    "    \n",
    "    ```\n",
    "    [\n",
    "    [0,1,0,0,1],\n",
    "    [1,0,1,1,1],\n",
    "    [0,1,0,1,0],\n",
    "    [0,1,1,0,1],\n",
    "    [1,1,0,1,0]\n",
    "    ]\n",
    "    ```\n",
    "    \n",
    "    Analysis of common operations on Adjacency Matrices:\n",
    "    \n",
    "    - Storage:\n",
    "    \n",
    "        Storing a graph as an Adjacency matrix can be done with time complexity of $O(|V|^{2})$, where $V$ is the number of vertices, the number of arrays in the matrix, and the length of each array;\n",
    "\n",
    "    - Add Vertex:\n",
    "    \n",
    "        Adding a vertex to a graph represented as an Adjacency Matrix can be done with time complexity of $O(|V|^{2})$ since we need to update all the arrays of which the matrix is made up as well as adding a new array;\n",
    "        \n",
    "    - Add Edge:\n",
    "        \n",
    "        Adding an edge to a graph represented as an Adjacency Matrix can be done with time complexity of $O(1)$, since we just need to add to the update 2 values in the matrix;\n",
    "   \n",
    "    - Remove Vertex:\n",
    "    \n",
    "        Removing a vertex from a graph represented as an Adjacency Matrix can be done with time complexity of $O(|V|^{2})$, because we need to update all the other arrays of which the matrix is made up;\n",
    "\n",
    "    - Remove Edge:\n",
    "    \n",
    "        Removing an edge from a graph represented as an Adjacency Matrix can be done with time complexity of $O(1)$, since we need to just update 2 values in the matrix.\n",
    "<br/>\n",
    "The space complexity of an Adjacency Matrix is $O(|V|^{2})$.\n",
    "\n",
    "\n",
    "Because of their space complexity, it makes sense to use Adjacency Matrices either for small Graphs or Graphs with a lot of edges.\n",
    "\n",
    "Graphs can be used everywhere we need to store relationships of any kind between elements. Since Trees are a subset of Graphs, all the real-world implementations of Trees are also real-world implementations of graphs. Another possible real-world implementation of Graphs is for road maps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "overhead-curtis",
   "metadata": {},
   "source": [
    "___\n",
    "<br />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "first-oracle",
   "metadata": {},
   "source": [
    "<div id=\"introductionA\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "waiting-display",
   "metadata": {},
   "source": [
    "# 3. Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "latter-windows",
   "metadata": {},
   "source": [
    "We have seen what is an algorithm and how to analyze one in the first part of the portfolio. In this section, we will analyze in-depth some common algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acknowledged-reverse",
   "metadata": {},
   "source": [
    "<div id=\"searching\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "julian-musical",
   "metadata": {},
   "source": [
    "## 3.1. Searching Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fifteen-anderson",
   "metadata": {},
   "source": [
    "Searching Algorithms are algorithms designed to find an element in the data structure for which the algorithm has been designed. We can find 2 main categories of Searching Algorithms:\n",
    "\n",
    "- **Sequential Searching Algorithms**, designed to search for an item in unsorted data structures, check every item in the data structure;\n",
    "- **Interval Searching Algorithms**, designed to search for an item in sorted data structures, only check some items of the data structure. This allows them to be more efficient than Sequential Searching Algorithms.\n",
    "\n",
    "These algorithms are correct if they can correctly find an item in the data structure for which they were designed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "constitutional-bible",
   "metadata": {},
   "source": [
    "<div id=\"linearSearch\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compatible-prevention",
   "metadata": {},
   "source": [
    "### 3.1.1. Linear Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hidden-credit",
   "metadata": {},
   "source": [
    "The Linear Search Algorithm is a Sequential Searching Algorithm.\n",
    "\n",
    "It takes in input an array and a value to search and iterates through the array to search for the value. It usually returns the index of the element (if it is in the array) or $-1$ (if the element is not in the array).\n",
    "\n",
    "Here is a python example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "varied-center",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "def linear_search(arr, val):\n",
    "    for i in range(len(arr)):\n",
    "        if (arr[i] == val):\n",
    "            return i\n",
    "    return -1\n",
    "\n",
    "array = [9,6,3,5,3,0,2,4,3,7,8,2,6,1]\n",
    "print(linear_search(array, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "artificial-dividend",
   "metadata": {},
   "source": [
    "This algorithm terminates for every input, either when it finds the element it is looking for, or when it iterates through the whole array. It can also be considered correct since it will always return the index of the element, or $-1$ if the element is not present."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "engaged-processing",
   "metadata": {},
   "source": [
    "<figure>\n",
    "    <img src=\"https://i.postimg.cc/bJzjjGPG/Linear-Search.gif\" alt=\"Linear Search Animation\" width=\"400\"/>\n",
    "    <figcaption>\n",
    "        Linear Search Animation. Gif source: <a href=\"https://www.tutorialspoint.com/data_structures_algorithms/linear_search_algorithm.htm\">tutorialspoint</a>\n",
    "    </figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "running-donor",
   "metadata": {},
   "source": [
    "**Time Complexity Analysis**\n",
    "\n",
    "We have seen that this algorithm searches an element by iterating through the array, so in the case that the element is not present or is the last element of the array, it needs to go through the whole array of $n$ elements. In the average case, the number of elements through which it needs to iterate also depend on $n$. Its time complexity in the average and worst-case will therefore be $Θ = O(n)$.\n",
    "\n",
    "In the best case, the element to search will be the first one, and thus the time complexity of this algorithm in the best case will be $Ω(1)$.\n",
    "\n",
    "**Space Complexity Analysis**\n",
    "\n",
    "No auxiliary data structures are required by this algorithm, so the auxiliary space complexity will be $O(0)$ while the total space complexity will be $O(n)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alleged-thing",
   "metadata": {},
   "source": [
    "<div id=\"binarySearch\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "southwest-hampshire",
   "metadata": {},
   "source": [
    "### 3.1.2. Binary Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pointed-berkeley",
   "metadata": {},
   "source": [
    "The Binary Search Algorithm is an Interval Searching Algorithm.\n",
    "\n",
    "It takes in input a sorted array and the value to search in it. It works by finding the median point of the array and returning the index of that point if that element contains the value we are searching for or recursively calls itself on the half of the array which could contain the element (the right part if the value of the main element was smaller than that of the value we are looking for or the left part if the value of the median element was greater). The process is recursively repeated until the element is found or there are no subarrays left to search.\n",
    "\n",
    "Here is a python example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "athletic-aquarium",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "def binary_search (arr, left, right, val):\n",
    "    # check that the subarray is not empty\n",
    "    if right >= left:\n",
    "        # find the index of the median value\n",
    "        mid = left + (right - left) // 2\n",
    "        # return the median value if it is the one we were searching for\n",
    "        if arr[mid] == val:\n",
    "            return mid \n",
    "        # if the median value is greater, recursively search the left part of the array.\n",
    "        elif arr[mid] > val: \n",
    "            return binary_search(arr, left, mid-1, val)\n",
    "        # otherwise recursively search the right part.\n",
    "        else: \n",
    "            return binary_search(arr, mid + 1, right, val)\n",
    "    # if the subarray is empty return -1\n",
    "    else:  \n",
    "        return -1\n",
    "array = [0, 2, 5, 7, 8, 11, 12, 13, 19, 23, 26, 32, 41]\n",
    "print(binary_search(array, 0, len(array), 26))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "still-promise",
   "metadata": {},
   "source": [
    "This algorithm terminates for every input, either when it finds the element and returns its index or when it tries to search an empty subarray and returns $-1$. Because of this, it can also be considered correct."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compatible-hacker",
   "metadata": {},
   "source": [
    "<figure>\n",
    "    <img src=\"https://i.postimg.cc/ydqkhy0M/Binary-Search.gif\" alt=\"Binary Search Animation\" width=\"400\"/>\n",
    "    <figcaption>\n",
    "        Binary Search Animation. Gif source: <a href=\"https://brilliant.org/wiki/binary-search/\">Brilliant</a>\n",
    "    </figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "treated-belief",
   "metadata": {},
   "source": [
    "**Time Complexity Analysis**\n",
    "\n",
    "Because at each recursive call we split the array of size $n$ in half, it will take an amount of steps $k$ to get to an array that will either contain 1 or 0 elements (depending on if the element we are searching for is in the array). Hence, we know that $n=2^k$ and at this point we can easily calculate $k = log_{2}(n)$.\n",
    "\n",
    "Because of this, the time complexity of this algorithm in its average and worst case is $Θ=O(log(n))$.\n",
    "\n",
    "In the best case, the element we are looking for is in the middle of the array and thus will be the first value we will search. So the time complexity of this algorithm in the best case will be $Ω(1)$.\n",
    "\n",
    "**Space Complexity Analysis**\n",
    "\n",
    "No auxiliary data structures are required by this algorithm, so the auxiliary space complexity will be $O(0)$ while the total space complexity will be $O(n)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "narrative-government",
   "metadata": {},
   "source": [
    "<div id=\"sorting\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "taken-guyana",
   "metadata": {},
   "source": [
    "## 3.2. Sorting Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "first-immunology",
   "metadata": {},
   "source": [
    "Sorting Algorithms are algorithms specifically designed to order elements in ascending or descending order, in the data structure for which the algorithm has been designed. \n",
    "\n",
    "Sorting algorithms may vary a lot for both time and space efficiency, but also for their behavior in edge cases.\n",
    "\n",
    "We can also distinguish between stable and unstable Sorting Algorithms: Stable Algorithms maintain the relative order of elements with the same value, while Unstable Algorithms do not. Every Unstable Algorithm can become Stable if we add the initial index of the element as the second sorting key.\n",
    "\n",
    "We can say that a sorting algorithm is correct if it is capable of sorting the items in the data structure for which it has been designed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outstanding-vancouver",
   "metadata": {},
   "source": [
    "<div id=\"selectionSort\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "julian-liechtenstein",
   "metadata": {},
   "source": [
    "### 3.2.1. Selection Sort"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pretty-banner",
   "metadata": {},
   "source": [
    "The Selection Sort Algorithm is an in-place, iterative sorting algorithm. \n",
    "\n",
    "It works by comparing the first item in an array to all the other items with a greater index and swaps it with the smallest it finds. It then moves to the next element and repeats the process until it gets to the last item of the array. In this way, after each iteration, one more item is sorted.\n",
    "\n",
    "Here we can see a simple implementation in python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "integral-shopper",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 2, 3, 3, 3, 4, 5, 6, 6, 7, 8, 9]\n"
     ]
    }
   ],
   "source": [
    "def selection_sort(arr):\n",
    "    for i in range(len(arr)-1):\n",
    "        # find the index of the smallest element\n",
    "        min_number_index = i\n",
    "        for j in range(i, len(arr)):\n",
    "            if arr[min_number_index] > arr[j]:\n",
    "                min_number_index = j\n",
    "        # swap the elements\n",
    "        arr[i], arr[min_number_index] = arr[min_number_index], arr[i]\n",
    "\n",
    "array = [9,6,3,5,3,0,2,4,3,7,8,2,6,1]\n",
    "selection_sort(array)\n",
    "print(array)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compliant-accuracy",
   "metadata": {},
   "source": [
    "Because this implementation of the algorithm swaps the elements in their correct positions, it does not maintain the relative order of the elements and it is therefore unstable.\n",
    "\n",
    "We can see that this algorithm terminates for every input since its loops will stop at the end of the array. Even if this algorithm is not really time efficient, we know that its output will always be correct because it will compare each item in the array against all the others in the unsorted section of the array."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enclosed-witch",
   "metadata": {},
   "source": [
    "<figure>\n",
    "    <img src=\"https://i.postimg.cc/nhv5cH0q/Selection-Sort.gif\" alt=\"Selection Sort Animation\" width=\"500\"/>\n",
    "    <figcaption>\n",
    "        Selection Sort Animaiton. Blue = current index, Red = current minimum, Yellow = already sorted. Gif source: <a href=\"https://rcsole.gitbooks.io/apprenticeship/content/year-one/data-structures-and-algorithms/02-sorting-algorithms.html\">GitBooks</a>\n",
    "    </figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "immediate-surgeon",
   "metadata": {},
   "source": [
    "**Time Complexity Analysis**\n",
    "\n",
    "As we can see from the code, the algorithm compares the element in the innermost of two nested loops, the outermost one will run $n$ times, while the innermost loop will run $n-1$ times the first times it is called, and this number will decrease at each call until it runs just $1$ time. The operations inside the innermost loop will run a number of times defined by the arithmetic series $\\frac{n(n+1)}{2}$.\n",
    "\n",
    "From the arithmetic series above we can easily find out that the dominant term is $n^2$, therefore, the average, best, and worst-case-scenario time complexity will be $Θ = Ω = O(n^{2})$.\n",
    "\n",
    "This means that this algorithm will take the same amount of steps also in every edge case (array already sorted, array sorted backward, array where all the items are the same).\n",
    "\n",
    "**Space Complexity Analysis**\n",
    "\n",
    "Because this is an in-place algorithm, it does not require any auxiliary space, so the auxiliary space complexity will be $O(0)$ while the total space complexity will be $O(n)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "static-bread",
   "metadata": {},
   "source": [
    "<div id=\"bubbleSort\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "convinced-reservation",
   "metadata": {},
   "source": [
    "### 3.2.2. Bubble Sort"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "global-equivalent",
   "metadata": {},
   "source": [
    "The Bubble Sort Algorithm is another example of an in-place, iterative sorting algorithm. \n",
    "\n",
    "Here is how this algorithm works: it starts by comparing the first element in the array to the next. If the first element is bigger than the next, it swaps them before moving to the next element and repeats the process, until it gets to the end of the array. It then starts a new iteration with one less item to be compared, until the array is sorted.\n",
    "\n",
    "Here we can see a simple implementation in python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "greater-world",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 2, 3, 3, 3, 4, 5, 6, 6, 7, 8, 9]\n"
     ]
    }
   ],
   "source": [
    "def bubble_sort(arr):\n",
    "    # check if any items have been swapped\n",
    "    has_swapped = True\n",
    "    # keep track of the index\n",
    "    i = 0\n",
    "    while(has_swapped and i<len(arr)):\n",
    "        has_swapped = False\n",
    "        for j in range(len(arr) - i - 1):\n",
    "            if arr[j] > arr[j+1]:\n",
    "                arr[j], arr[j+1] = arr[j+1], arr[j]\n",
    "                has_swapped = True\n",
    "        i += 1\n",
    "    \n",
    "array = [9,6,3,5,3,0,2,4,3,7,8,2,6,1]\n",
    "bubble_sort(array)\n",
    "print(array)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bearing-stuff",
   "metadata": {},
   "source": [
    "Because at each iteration this algorithm moves the item with the largest value at the end, the relative order of elements with the same value remains the same and thus is algorithm is stable.\n",
    "\n",
    "This algorithm terminates for every input, as soon as there is an iteration in which no elements of the array need to be sorted."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chief-halifax",
   "metadata": {},
   "source": [
    "<figure>\n",
    "    <img src=\"https://i.postimg.cc/hGmGGQNp/Bubble-Sort.gif\" alt=\"Bubble Sort Animation\" width=\"300\"/>\n",
    "    <figcaption>\n",
    "        Bubble Sort Animation. Gif source: <a href=\"https://medium.com/madhash/bubble-sort-in-a-nutshell-how-when-where-4965e77910d8\">Medium</a>\n",
    "    </figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bulgarian-profession",
   "metadata": {},
   "source": [
    "**Time Complexity Analysis**\n",
    "\n",
    "Just like the selection sort algorithm, this algorithm compares the element in the innermost of two nested loops, the outermost one will run $n$ times, while the innermost loop will run $n-1$ times the first times it is called, and this number will decrease at each call until it runs just $1$ time. The operations inside the innermost loop will run a number of times defined by the arithmetic series $\\frac{n(n+1)}{2}$.\n",
    "\n",
    "From the arithmetic series we will find out that the asymptotic growth of this algorithm is quadratic. The average and worst-case-scenario time complexity will be $Θ = O(n^{2})$.\n",
    "\n",
    "Since the algorithm checks if any swaps have been performed at each iteration, in the case that the input array is already sorted or all the items in the array are the same (no item is greater than the previous one), the time complexity of this algorithm will be $Ω(n)$.\n",
    "\n",
    "If instead the input array is sorted backward, the time complexity will be $O(n^2)$, since it will need to perform swaps at each operation.\n",
    "\n",
    "**Space Complexity Analysis**\n",
    "\n",
    "Because this is an in-place algorithm, it does not require any auxiliary space, so the auxiliary space complexity will be $O(0)$ while the total space complexity will be $O(n)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "twenty-franchise",
   "metadata": {},
   "source": [
    "<div id=\"quickSort\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "satisfied-stupid",
   "metadata": {},
   "source": [
    "### 3.2.3. QuickSort"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "primary-meaning",
   "metadata": {},
   "source": [
    "The QuickSort Algorithm is a recursive sorting algorithm.\n",
    "\n",
    "The algorithm starts by choosing a \"pivot\" (there are different ways to do this) and puts all the elements smaller than the pivot at its left and the elements bigger than the pivot at its right. In this way, the pivot is in the correct position and the operation is recursively repeated on the 2 sub-arrays, with new pivots. The operation continues until the subarrays contain only one element.\n",
    "\n",
    "Here we can see an example of implementation in python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "second-workshop",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 2, 3, 3, 3, 4, 5, 6, 6, 7, 8, 9]\n"
     ]
    }
   ],
   "source": [
    "def quicksort(arr):\n",
    "    # auxiliary arrays\n",
    "    less = []\n",
    "    equal = []\n",
    "    greater = []\n",
    "    if len(arr) > 1:\n",
    "        # pick the first element as pivot\n",
    "        pivot = arr[0]\n",
    "        for x in arr:\n",
    "            if x < pivot:\n",
    "                less.append(x)\n",
    "            elif x == pivot:\n",
    "                equal.append(x)\n",
    "            elif x > pivot:\n",
    "                greater.append(x)\n",
    "        # recursively repeat the operation\n",
    "        return quicksort(less)+equal+quicksort(greater)\n",
    "    else:\n",
    "        return arr\n",
    "    \n",
    "array = [9,6,3,5,3,0,2,4,3,7,8,2,6,1]\n",
    "print(quicksort(array))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "relative-gross",
   "metadata": {},
   "source": [
    "In this implementation, the pivot is always the first item of the array/subarray. Other ways of choosing the pivot include:\n",
    "- picking always the last item in the array;\n",
    "- picking a random value;\n",
    "- picking three values (using one of the methods above) and choosing the median value as the pivot.\n",
    "\n",
    "Because in this implementation we store the elements with the same value in a third array, we pick the pivot to be the first element of the subarrays, the relative order of the elements with the same value will not change. This specific implementation of QuickSort is stable, but it is not always the case.\n",
    "\n",
    "This algorithm terminates for every input as soon as the subarrays contain 1 or 0 elements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "frank-twenty",
   "metadata": {},
   "source": [
    "**Time Complexity Analysis**\n",
    "\n",
    "We know that this algorithm will need to go through all the elements in the array, split it in half, and repeat the process on the halved arrays until it is left with subarrays of only one element. Assuming that the subarrays are of equal size, we can recursively split them in half $log_{2}(n)$ times. This happens because we know that it will take $k$ steps to divide the array in subarrays of 1 element if at each step we halve the size of the array. So we can write the size of the array as $n=2^k$ and at this point we can easily calculate $k = log_{2}(n)$.\n",
    "<figure>\n",
    "    <img src=\"https://i.postimg.cc/tTr8Q2Df/Quick-Sort.png\" alt=\"Representation of QuickSort\" width=\"400\"/>\n",
    "    <figcaption>\n",
    "        Quick Sort. Image Source: <a href=\"https://deepai.org/machine-learning-glossary-and-terms/quicksort-algorithm\">DeepAI</a>\n",
    "    </figcaption>\n",
    "</figure>\n",
    "We can see in the image above how an array of 9 elements is recursively divided 3 times ($≈log_{2}(9)$) before it is sorted. The number of comparisons in every iteration decreases as the size of the subarrays decreases, but we can see that it depends on the size of the array ($n$). Therefore, we will have in the average case a total number of operations, and thus its time complexity, that will grow in a quasilinear way ($Θ(n·log(n))$).\n",
    "<br/><br/>\n",
    "\n",
    "In case that the input array contains only elements with the same value, our implementation will only need to go through the array once, and since no element is bigger or smaller it will return two recursive functions called on empty arrays and the array of elements with the same value as the pivot. Its best-case time complexity will therefore be $Ω(n)$, but depending on how the algorithm is implemented, this may not be true in every case.\n",
    "\n",
    "\n",
    "If instead the input array is already sorted or sorted backward, the time complexity of the algorithm will be $O(n^2)$, at least if the pivot is chosen to be always the first or always the last value of the array since we will have a situation similar to that of the image below, where after each \"iteration\" $i$, the array is not split enough but in an array $n-i-1$ elements:\n",
    "<figure>\n",
    "    <img src=\"https://i.postimg.cc/j5qBFG6P/Quick-Sort-Worst.png\" alt=\"Representation of QuickSort in worst case scenario\" width=\"400\"/>\n",
    "    <figcaption>\n",
    "        Quick Sort in worst-case time complexity. Image Source: <a href=\"https://www.khanacademy.org/computing/computer-science/algorithms/quick-sort/a/analysis-of-quicksort\">Khan Academy</a>\n",
    "    </figcaption>\n",
    "</figure>\n",
    "This situation can be avoided by picking a random pivot (which does not guarantee that this situation will not happen in other cases, although extremely unlikely especially for large enough inputs) or picking a median input.\n",
    "<br/><br/>\n",
    "\n",
    "**Space Complexity Analysis**\n",
    "\n",
    "This implementation of the QuickSort algorithm requires an auxiliary space equal to the size of the array that it is sorting, so it will have an auxiliary space complexity of $O(n)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "internal-height",
   "metadata": {},
   "source": [
    "<div id=\"mergeSort\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collectible-phrase",
   "metadata": {},
   "source": [
    "### 3.2.4. MergeSort"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "postal-layer",
   "metadata": {},
   "source": [
    "The MergeSort Algorithm is another example of a recursive sorting algorithm.\n",
    "\n",
    "This algorithm works in a way that is conceptually really simple: it recursively divides an array in half until only arrays containing 1 element are left. It then starts to merge these arrays together while sorting them.\n",
    "\n",
    "Here we can see a python implementation of this algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "suitable-stream",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 2, 3, 3, 3, 4, 5, 6, 6, 7, 8, 9]\n"
     ]
    }
   ],
   "source": [
    "def merge(left, right):\n",
    "    '''\n",
    "    helper function to merge the array in an ordered way\n",
    "    '''\n",
    "    result = []\n",
    "    i = j = 0\n",
    "    while i < len(left) and j < len(right):\n",
    "        if left[i] <= right[j]:\n",
    "            result.append(left[i])\n",
    "            i += 1\n",
    "        else:\n",
    "            result.append(right[j])\n",
    "            j += 1\n",
    "    result += left[i:]\n",
    "    result += right[j:]\n",
    "    return result\n",
    "\n",
    "def mergesort(arr):\n",
    "    # if the array contains only 1 element, return it\n",
    "    if len(arr) < 2:\n",
    "        return arr\n",
    "    # otherwise recursively call this function to\n",
    "    # the array in half...\n",
    "    mid = len(arr) // 2\n",
    "    left_arr = mergesort(arr[:mid])\n",
    "    right_arr = mergesort(arr[mid:])\n",
    "    # ...and then merge it\n",
    "    return merge(left_arr, right_arr)\n",
    "\n",
    "array = [9,6,3,5,3,0,2,4,3,7,8,2,6,1]\n",
    "print(mergesort(array))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prompt-blank",
   "metadata": {},
   "source": [
    "While merging back the subarrays, this algorithm maintains the relative order of elements with the same value, and therefore it is a stable algorithm.\n",
    "\n",
    "This algorithm will terminate for every input when the input is split into subarrays and then merged back together."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interior-malaysia",
   "metadata": {},
   "source": [
    "**Time Complexity Analysis**\n",
    "\n",
    "This algorithm first needs to recursively divide the array into subarrays containing only 1 element, and then merge and order them.\n",
    "<figure>\n",
    "    <img src=\"https://i.postimg.cc/3rcztfk0/Merge-Sort.png\" alt=\"Representation of MergeSort\" width=\"400\"/>\n",
    "    <figcaption>\n",
    "        Merge Sort. Image Source: <a href=\"https://en.wikipedia.org/wiki/Merge_sort#/media/File:Merge_sort_algorithm_diagram.svg\">Wikipedia</a>\n",
    "    </figcaption>\n",
    "</figure>\n",
    "\n",
    "Splitting an array or subarray in half takse constant time, since in this case it is only a matter of computing the middle point of the array or subarray.\n",
    "As we have seen with the binary search, it takses $k$ steps to divide the array into subarrays of 1 element each, so we know that $n=2^k$ and we can easily calculate $k = log_{2}(n)$. The same amount of steps is taken to build the array back. In this case, however, at each step we will need to perform a number of comparisons that depends on the size of the array ($n$) to make sure that the array is sorted \n",
    "Because of this, the time complexity of this algorithm will be $Θ(log(n))$.\n",
    "\n",
    "An input array that is already sorted, sorted backward, or in which all items are the same will not affect the time complexity of this algorithm, therefore its best and worst-case time complexity will be $Ω = O(n·log(n))$\n",
    "\n",
    "**Space Complexity Analysis**\n",
    "\n",
    "The MergeSort Algorithm is not really space-efficient and requires an auxiliary space equal to the size of the array that it is sorting, so it will have an auxiliary space complexity of $O(n)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "female-cooking",
   "metadata": {},
   "source": [
    "<div id=\"heapSort\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "revised-virus",
   "metadata": {},
   "source": [
    "## 3.2.5. HeapSort"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collect-running",
   "metadata": {},
   "source": [
    "The HeapSort Algorithm is an iterative sorting algorithm.\n",
    "\n",
    "Taken in input an unordered array, this algorithm turns that array into a Max Heap (n heap in which the value of a parent node is always bigger than that of its children and that can be represented as an array). It then swaps the last element in the heap with the first one (the root). Since the root is always the biggest element in a Max Heap, we know that that element is in its final position, and thus we can consider it to be in the sorted partition of the array. Then the algorithm \"heapifies\" (rearranges the elements so that the condition of the heap is met) the heap (or non-sorted partition of the array) and repeats the process with the last element of the heap (or non-sorted partition of the array) until there are no more elements in the heap and the array is sorted.\n",
    "\n",
    "Here we can see a python implementation of this algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bound-wrong",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 2, 3, 3, 3, 4, 5, 6, 6, 7, 8, 9]\n"
     ]
    }
   ],
   "source": [
    "def heapify(heap, n, i):\n",
    "    '''order the heap'''\n",
    "    max = i\n",
    "    left = 2 * i + 1\n",
    "    right = 2 * i + 2\n",
    "    # check if the left and right children exist.\n",
    "    # If one of them is bigger than the parent, set it as max value.\n",
    "    if left < n and heap[max] < heap[left]:\n",
    "        max = left\n",
    "    if right < n and heap[max] < heap[right]:\n",
    "        max = right\n",
    "    # swap the max value with the parent\n",
    "    if max != i:\n",
    "        heap[i], heap[max] = heap[max], heap[i]\n",
    "        # heapify with the new parent node\n",
    "        heapify(heap, n, max)\n",
    "\n",
    "def build_max_heap(arr):\n",
    "    \"\"\"create an heap from an array\"\"\"\n",
    "    for i in range(len(arr)//2 - 1, -1, -1):\n",
    "        heapify(arr, len(arr), i)\n",
    "    return arr\n",
    "        \n",
    "def heapsort(arr):\n",
    "    build_max_heap(arr)\n",
    " \n",
    "    # perform the heapsort\n",
    "    for i in range(len(arr)-1, 0, -1):\n",
    "        arr[i], arr[0] = arr[0], arr[i]\n",
    "        heapify(arr, i, 0)\n",
    "\n",
    "array = [9,6,3,5,3,0,2,4,3,7,8,2,6,1]\n",
    "heapsort(array)\n",
    "print(array)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pediatric-celebration",
   "metadata": {},
   "source": [
    "The initial order of the array is changed once the Max Heap is created, so the relative order of elements with the same value will change too. This algorithm is therefore unstable.\n",
    "\n",
    "This algorithm will terminate for every input since it ends once its main loops gets to the first item of the heap."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alike-collectible",
   "metadata": {},
   "source": [
    "<figure>\n",
    "    <img src=\"https://i.postimg.cc/SKfwVy9m/HeapSort.gif\" alt=\"Heap Sort Animation\" width=\"400\"/>\n",
    "    <figcaption>\n",
    "        Heap Sort Animation. Gif source: <a href=\"https://www.codesdope.com/course/algorithms-heapsort/\">CodesDope</a>\n",
    "    </figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "processed-farmer",
   "metadata": {},
   "source": [
    "**Time Complexity Analysis**\n",
    "\n",
    "We can start by analyzing the heapify function. In the worst-case scenario, this function will need to rearrange one element in each level of the tree. In the case of a binary heap, given that the heap is a balanced tree, its height will always be $h = log(n)$. Therefore, the worst-case time complexity of this function is $O(log(n))$. \n",
    "\n",
    "The first step of this algorithm is turning the array into a Max Heap. Intuitively we may say that this operation will have a time complexity of $O(n·log(n))$, which is correct but is not an asymptotic tight bound. Since the runtime of the heapify function is different for each node, depending on its level, the worst-case time complexity of the make_max_heap function will be $O(n)$. A complete analysis of this operation can be found [here](http://www.cs.umd.edu/~meesh/351/mount/lectures/lect14-heapsort-analysis-part.pdf).\n",
    "\n",
    "We can see in the code implementation above that once turned the array into a heap, we need to call the heapify function $n-1$ times. For this reason, the time complexity of this algorithm in the best, average, and worst-case will be $Ω=Θ=O(n·log(n))$.\n",
    "\n",
    "If we pass as input an array that is already sorted, sorted backward, or in which all the items have the same value, the time complexity of this algorithm will not change.\n",
    "\n",
    "**Space Complexity Analysis**\n",
    "\n",
    "No auxiliary data structures are required by this algorithm, so the auxiliary space complexity will be $O(0)$ while the total space complexity will be $O(n)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "contemporary-gospel",
   "metadata": {},
   "source": [
    "<div id=\"countingSort\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rubber-shuttle",
   "metadata": {},
   "source": [
    "### 3.2.6. Counting Sort"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comparative-casino",
   "metadata": {},
   "source": [
    "Counting Sort is a non-comparison, iterative sorting algorithm.\n",
    "\n",
    "Unlike all the other sorting algorithms we have seen, the Counting Sort algorithm works only on positive integers in a range $k$.\n",
    "\n",
    "Here is how the algorithm works: \n",
    "1. It counts the number of occurrences of each unique value $v$ in the array and stores the number of occurrences at index $v$ in an auxiliary array;\n",
    "2. It then performs a cumulative count of the elements in the auxiliary array (e.g.: ```[0,2,1,1]``` becomes ```[0,2,3,4]``` after a cumulative count) so that each value $c$ in the auxiliary array is the last index + 1 at which the value $v$ (given by the index of $c$) will appear in the output array;\n",
    "3. Then, starting with the last element $e$ in the initial array, it decreases the cumulative count of the element in position $e$ in the auxiliary array, and inserts $e$ at the correct index (the value stored in the position $e$ of the auxiliary array) in the output array. In the end, it copies the output array in the original array.\n",
    "\n",
    "Here is a python implementation of this algorithm where the range $k$ is fixed to be 0-9:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "weighted-bundle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 2, 3, 3, 3, 4, 5, 6, 6, 7, 8, 9]\n"
     ]
    }
   ],
   "source": [
    "def counting_sort(array):\n",
    "    size = len(array)\n",
    "    output = [0] * size \n",
    "    # Initialize count array\n",
    "    aux = [0] * 10\n",
    "    # Store the count of each element in count array\n",
    "    for i in range(0, size):\n",
    "        aux[array[i]] += 1\n",
    "    # Store the cumulative count\n",
    "    for i in range(1, 10):\n",
    "        aux[i] += aux[i - 1]\n",
    "    # Decrease the cumulative count by one for each position\n",
    "    # Find the index of the element of the original array in aux array\n",
    "    # Place the elements in output array, at the right index\n",
    "    for i in range(size - 1, -1, -1):\n",
    "        aux[array[i]] -= 1\n",
    "        index = aux[array[i]]\n",
    "        output[index] = array[i]\n",
    "        \n",
    "    # Copy the sorted element into original array\n",
    "    for i in range(0, size):\n",
    "        array[i] = output[i]\n",
    "\n",
    "array = [9, 6,3,5,3,0,2,4,3,7,8,2,6,1]\n",
    "counting_sort(array)\n",
    "print(array)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compliant-calculator",
   "metadata": {},
   "source": [
    "This algorithm is also stable because it maintains the relative order of elements with the same value when the array is sorted from the initial to the output array.\n",
    "\n",
    "This algorithm will also terminate for every input as soon as the loops are executed.\n",
    "\n",
    "<figure>\n",
    "    <img src=\"https://i.postimg.cc/kGg9W2hr/Counting-Sort.gif\" alt=\"Counting Sort Animation\" width=\"400\"/>\n",
    "    <figcaption>\n",
    "        Counting Sort Animation. Gif source: <a href=\"https://brilliant.org/wiki/counting-sort/\">Brilliant</a>\n",
    "    </figcaption>\n",
    "</figure>\n",
    "\n",
    "**Time Complexity Analysis**\n",
    "\n",
    "We can analyze each step of this algorithm to find its time complexity:\n",
    "1. Counting the number of occurences can be done with time complexity of $O(n)$, where $n$ is the length of the array;\n",
    "2. Performing a cumulative count can be done with time complexity of $O(k)$, where $k$ is the size of the range of the positive integer values of which the array is made of, and the size of the auxiliary array;\n",
    "3. Arranging the elements in ascending order in the output array can be done with time complexity of $O(n)$.\n",
    "\n",
    "Since we have some operations that require a time complexity of $O(n)$ and another that requires a time complexity of $O(k)$, we can say that the time complexity of this algorithm is $O(n+k)$.\n",
    "\n",
    "In case that all the elements in the input array have the same value, the size of the range of positive integers $k$ in the array will be $1$, and thus its best-case time complexity will be $Ω(n)$. If instead the input array is already sorted or sorted backward, it won't affect the time complexity of this algorithms, so its average and worst-case time complexity will be $Θ=O(n+k)$.\n",
    "\n",
    "**Space Complexity Analysis**\n",
    "\n",
    "The auxiliary space required by this algorithm depends on the size of the array ($n$) and the size of the range of positive integers that the array contains ($k$). Its space complexity will therefore be $O(n+k)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "swiss-design",
   "metadata": {},
   "source": [
    "<div id=\"graphAlgorithms\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "requested-reunion",
   "metadata": {},
   "source": [
    "## 3.3. Graph Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "parental-compensation",
   "metadata": {},
   "source": [
    "Some algorithms are designed to perform some specific operations on graphs or on given subcategories of them. Some common operations carried out by graph algorithms include traversal (visiting all the vertices in a graph) and pathfinding (finding the shortest path between 2 vertices)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "steady-campus",
   "metadata": {},
   "source": [
    "<div id=\"dfs\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lasting-invalid",
   "metadata": {},
   "source": [
    "### 3.3.1. Depth-First Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "grave-atlanta",
   "metadata": {},
   "source": [
    "The Depth-First Search Algorithm (DFS) is a graph traversal algorithm. It can be implemented on both trees and graphs, and it can also be modified and used for searching or pathfinding on these data structures. We will see how the algorithm works when used for graph traversal.\n",
    "\n",
    "Starting from a given vertex (or the root node in the case of a tree), this algorithm visits all the possible vertices in a specific branch before backtracking.\n",
    "\n",
    "The algorithm works by adding the vertex that is passed as an argument, $v$, to a data structure (usually a stack) – to keep track of the vertices that have already been visited. It then recursively calls itself on every neighbor of $v$ that has not been visited yet, so that all the branches are visited. Finally, it returns the list of visited vertices.\n",
    "\n",
    "Here is a python implementation of DFS used for graph traversal on a graph represented by an adjacency list in which each key of the dictionary is a vertex and its value is an array containing all the nodes to which it is connected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "pursuant-inquiry",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['C', 'B', 'A', 'D', 'E', 'F']\n"
     ]
    }
   ],
   "source": [
    "def dfs(graph, start, visited=None):\n",
    "    if visited == None:\n",
    "        visited = []\n",
    "    visited.append(start)\n",
    "    for vertex in graph[start]:\n",
    "        if vertex not in visited:\n",
    "            dfs(graph, vertex, visited)\n",
    "    return visited\n",
    "\n",
    "graph = {'A': ['B', 'D'],\n",
    "         'B': ['A', 'C', 'D'],\n",
    "         'C': ['B', 'F'],\n",
    "         'D': ['A', 'B', 'E', 'F'],\n",
    "         'E': ['D'],\n",
    "         'F': ['C', 'D']}\n",
    "\n",
    "print(dfs(graph, 'C'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brazilian-disney",
   "metadata": {},
   "source": [
    "This algorithm will not terminate if implemented for graph traversal on an infinite graph (and could not terminate even if implemented for searching on an infinite graph). Otherwise, it will terminate once it has visited each vertex (or found a given vertex/path, depending on its implementation).\n",
    "\n",
    "We know that this algorithm is correct if it can correctly traverse a graph."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "offensive-rhythm",
   "metadata": {},
   "source": [
    "<figure>\n",
    "    <img src=\"https://i.postimg.cc/GtXLfFsh/DFS.gif\" alt=\"Depth-First Search Animation\" width=\"400\"/>\n",
    "    <figcaption>\n",
    "        Depth-First Search on a tree. Gif source: <a href=\"https://commons.wikimedia.org/wiki/File:Depth-First-Search.gif\">Wikimedia</a>\n",
    "    </figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mediterranean-logging",
   "metadata": {},
   "source": [
    "**Time Complexity Analysis**\n",
    "\n",
    "We have seen that this algorithm needs to visit all the vertices, starting at the initial one, checking all of its neighbors (the vertices connected to it by an edge), and recursively call itself on the neighbors that have not yet been visited. Therefore, the total number of operations depends on the number of vertices $V$ and the number of edges $E$ and its time complexity, in the average and worst case, will be $Θ=O(|V|+|E|)$.\n",
    "\n",
    "When this algorithm is implemented on a tree (which is a graph with $v-1$ edges), its time complexity will be $Ω(V)$.\n",
    "\n",
    "**Space Complexity Analysis**\n",
    "\n",
    "The auxiliary space required by this algorithm depends on the number of vertices, so its space complexity will be $O(V)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wooden-sapphire",
   "metadata": {},
   "source": [
    "<div id=\"bfs\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "short-blend",
   "metadata": {},
   "source": [
    "### 3.3.2. Breadth-First Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "peaceful-dressing",
   "metadata": {},
   "source": [
    "The Breadth-First Search Algorithm (BFS) is another graph traversal algorithm. Just like DFS, it can be implemented on both trees and graphs, and it can be modified and used for searching or pathfinding on these data structures. As for the DFS, we will see how the algorithm works when used for graph traversal.\n",
    "\n",
    "Starting from a given vertex (or the root node in the case of a tree), this algorithm visits all of its neighbors stores them in a queue, and then repeats the process with the first vertex in the queue. \n",
    "\n",
    "The algorithm works by storing the start node in a queue (to keep track of the vertices whose neighbors haven't been visited) and in another data structure (to keep track of the vertices that have been visited). Then, while there are items in the queue, it pops the first item from the queue and checks its neighbors, and those who haven't been visited yet are added to both the queue and the other data structure. In the end, it returns the data structure containing the visited nodes.\n",
    "\n",
    "Here is a python implementation of BFS used for graph traversal on a graph represented by an adjacency list in which each key of the dictionary is a vertex and its value is an array containing all the nodes to which it is connected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "blessed-organizer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A', 'B', 'C', 'D', 'E', 'F', 'G']\n"
     ]
    }
   ],
   "source": [
    "def bfs(graph, start):\n",
    "    visited = []\n",
    "    queue = []\n",
    "    visited.append(start)\n",
    "    queue.append(start)\n",
    "    while queue:\n",
    "        current = queue.pop(0) \n",
    "        for vertex in graph[current]:\n",
    "            if vertex not in visited:\n",
    "                visited.append(vertex)\n",
    "                queue.append(vertex)\n",
    "    return visited\n",
    "\n",
    "graph = {'A': ['B', 'C', 'D'],\n",
    "         'B': ['A', 'E'],\n",
    "         'C': ['A', 'D', 'F'],\n",
    "         'D': ['A', 'C', 'E', 'G'],\n",
    "         'E': ['B', 'D', 'G'],\n",
    "         'F': ['C', 'G'],\n",
    "         'G': ['D', 'E', 'F']}\n",
    "\n",
    "print(bfs(graph, 'A'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "governing-agency",
   "metadata": {},
   "source": [
    "This algorithm will not terminate if implemented for graph traversal on an infinite graph (but unlike DFS, it terminates if implemented for searching on an infinite graph). Otherwise, it will terminate once it has visited each vertex (or found a given vertex/path, depending on its implementation).\n",
    "\n",
    "We know that this algorithm is correct if it can correctly traverse a graph."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "disturbed-debut",
   "metadata": {},
   "source": [
    "<figure>\n",
    "    <img src=\"https://i.postimg.cc/kXyKnzWB/BFS.gif\" alt=\"Breadth-First Search Animation\" width=\"400\"/>\n",
    "    <figcaption>\n",
    "        Breadth-First Search on a graph. Gif source: <a href=\"https://www.codeabbey.com/index/task_view/breadth-first-search\">CodeAbbey</a>\n",
    "    </figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "floral-brooks",
   "metadata": {},
   "source": [
    "**Time Complexity Analysis**\n",
    "\n",
    "We have seen that this algorithm needs to visit all the vertices, starting at the initial one, checking all of its neighbors (the vertices connected to it by an edge), and repeat the operation on the neighbors that have not yet been visited. Therefore, like for the DFS, the total number of operations depends on the number of vertices $V$ and the number of edges $E$, and its time complexity, in the average and worst case, will be $Θ=O(|V|+|E|)$.\n",
    "\n",
    "When this algorithm is implemented on a tree (which is a graph with $v-1$ edges), its time complexity will be $Ω(V)$.\n",
    "\n",
    "**Space Complexity Analysis**\n",
    "\n",
    "The auxiliary space required by this algorithm, like in the case of the DFS, depends on the number of vertices, so its space complexity will be $O(V)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "virgin-assist",
   "metadata": {},
   "source": [
    "<div id=\"dijkstras\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "emerging-charlotte",
   "metadata": {},
   "source": [
    "### 3.3.3. Dijkstra's Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "million-january",
   "metadata": {},
   "source": [
    "Dijkstra's Algorithm is an algorithm used to find the shortest distance between two vertices in a weighted graph. This algorithm requires the weight of the edges to be positive to work. It can also be modified to find the distance between one vertex and all the others.\n",
    "\n",
    "These are the steps of this algorithm:\n",
    "1. Initialize 3 auxiliary data structures to keep track of the vertices to visit (starting from the starting vertex), from which adjacent vertex a vertex has been visited (None for the starting vertex); and the cost to get from the starting vertex to that one (0 for the starting vertex). We will call these data structures $a$, $b$, and $c$ respectively for simplicity;\n",
    "2. Set the current vertex to be the one with the lowest \"cost\" in $a$ and remove it from $a$ (in the first iteration it will be the starting vertex);\n",
    "3. If the current vertex is the vertex to find, reconstruct and return the path to that vertex;\n",
    "4. Otherwise, for each of its neighbors $n$, calculate the \"cost\" to get there and if they are not in $c$, or if they are but this new \"cost\" is lower than the previous one: update its value in $c$, add it to $a$ with its new \"cost\" as priority, and set it as visited from the current node in $b$.\n",
    "5. Repeat steps 2, 3, and 4 until there are elements in $a$ or as long as you don't have found the vertex to find.\n",
    "\n",
    "This algorithm is optimal, which means that it will always find the best solution.\n",
    "\n",
    "Here is a python implementation of Dijkstra's Algorithm on a graph where each vertex has a set of spatial coordinates and a list of neighbors and the heuristic function returns the distance between 2 vertices. To keep track of the vertex to visit we will use a Priority Queue implemented with a Min-Heap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "mature-observer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['LAX', 'JFK', 'LHR', 'BER'], 'The cost is 573')\n"
     ]
    }
   ],
   "source": [
    "import heapq\n",
    "\n",
    "class PriorityQueue:\n",
    "    def __init__(self):\n",
    "        self.elements = []\n",
    "    \n",
    "    def empty(self):\n",
    "        return not self.elements\n",
    "    \n",
    "    def put(self, item, priority):\n",
    "        heapq.heappush(self.elements, (priority, item))\n",
    "    \n",
    "    def get(self):\n",
    "        return heapq.heappop(self.elements)[1]\n",
    "\n",
    "class Graph():\n",
    "    def __init__(self, graph):\n",
    "        ''' \n",
    "        Initialize a graph, represented as an adjacency list\n",
    "        '''\n",
    "        self.graph = graph\n",
    "    \n",
    "    def dijkstra(self, start, to_find):\n",
    "        to_visit = PriorityQueue()\n",
    "        to_visit.put(start, 0)\n",
    "        # Dictionary in which we store from which vertex another vertex has been visited\n",
    "        from_v = {}\n",
    "        # Dictionary in which we store the cost to get to a vertex\n",
    "        cost_to_vertex= {}\n",
    "        # Initialize the start node in both the from_v and cost_to_vertex dicts\n",
    "        from_v[start] = None  # It is none because it is the first node\n",
    "        cost_to_vertex[start] = 0 # It is 0 because it is the first node\n",
    "\n",
    "        while not to_visit.empty():\n",
    "            # Get the vertex with the least cost in the queue\n",
    "            current = to_visit.get()\n",
    "            # If the vertex is the one to find, return the path to that vertex and the cost to get there\n",
    "            if current == to_find:\n",
    "                path = []\n",
    "                while current != start:\n",
    "                    path.append(current)\n",
    "                    current = from_v[current]\n",
    "                path.append(start)\n",
    "                path.reverse()\n",
    "                return path, \"The cost is \" + str(cost_to_vertex[to_find])\n",
    "            \n",
    "            for neighbor, weight in self.graph[current][\"neighbors\"]:\n",
    "                # determine the new cost\n",
    "                new_cost = cost_to_vertex[current] + weight\n",
    "                # If the vartex hasn't been explored or is being explored from a more convenient path\n",
    "                if neighbor not in cost_to_vertex or new_cost < cost_to_vertex[neighbor]:\n",
    "                    cost_to_vertex[neighbor] = new_cost\n",
    "                    # calculate the priority\n",
    "                    priority = new_cost\n",
    "                    # add it in the queue\n",
    "                    to_visit.put(neighbor, priority)\n",
    "                    # add it to the path, assign it as value the node current, from which we got there\n",
    "                    from_v[neighbor] = current\n",
    "\n",
    "        return\n",
    "\n",
    "\n",
    "graph = {\n",
    "    \"JFK\" : {\"coords\": (40.730, -73.935), \"neighbors\": [(\"LAX\", 92), (\"LHR\", 427), (\"GRU\", 419)]},\n",
    "    \"BER\" : {\"coords\": (52.520, 13.404), \"neighbors\": [(\"FCO\", 33), (\"LHR\", 54), (\"SVO\", 122)]},\n",
    "    \"FCO\" : {\"coords\": (41.773, 12.239), \"neighbors\": [(\"BER\", 33), (\"LHR\", 95), (\"JNB\", 351), (\"DEL\", 358)]},\n",
    "    \"LHR\" : {\"coords\": (51.509, 0.118), \"neighbors\": [(\"JFK\", 427),(\"BER\", 54), (\"FCO\", 95), (\"GRU\", 521)]},\n",
    "    \"LAX\" : {\"coords\": (34.052, -118.24), \"neighbors\": [(\"JFK\", 92), (\"HNL\", 129), (\"ICN\", 567), (\"SYD\", 890)]},\n",
    "    \"GRU\" : {\"coords\" : (-23.588, -46.658), \"neighbors\" : [(\"JFK\", 419), (\"LHR\", 521), (\"JNB\", 545)]},\n",
    "    \"JNB\" : {\"coords\" : (-26.134, 28.240), \"neighbors\" : [(\"FCO\", 351), (\"GRU\", 545), (\"DEL\", 454)]},\n",
    "    \"SVO\" : {\"coords\" : (55.751, 37.618), \"neighbors\" : [(\"BER\", 122), (\"PEK\", 621)]},\n",
    "    \"SYD\" : {\"coords\" : (-33.947, 151.179), \"neighbors\" : [(\"LAX\", 890), (\"ICN\", 554), (\"PEK\", 445), (\"HNL\", 467)]},\n",
    "    \"PEK\" : {\"coords\" : (40.072, 116.597), \"neighbors\" : [(\"SVO\", 621), (\"SYD\", 445), (\"ICN\", 454), (\"DEL\", 779)]},\n",
    "    \"DEL\" : {\"coords\" : (28.644, 77.216), \"neighbors\" : [(\"FCO\", 358), (\"PEK\", 779), (\"JNB\", 454)]},\n",
    "    \"HNL\" : {\"coords\" : (21.315, -157.858), \"neighbors\" : [(\"LAX\", 129), (\"SYD\", 467)]},\n",
    "    \"ICN\" : {\"coords\" : (37.532, 127.024), \"neighbors\" : [(\"LAX\", 567), (\"SYD\", 554), (\"PEK\", 454)]},\n",
    "}\n",
    "g = Graph(graph)\n",
    "start = \"LAX\"\n",
    "end = \"BER\"\n",
    "print(g.dijkstra(start, end))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "finnish-relief",
   "metadata": {},
   "source": [
    "This algorithm will terminate after $v$ iterations, where $v$ is the number of vertices in the graph.\n",
    "\n",
    "The algorithm needs to correctly find the shortest path from one vertex to all the others to be correct."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chicken-robertson",
   "metadata": {},
   "source": [
    "<figure>\n",
    "    <img src=\"https://i.postimg.cc/zvzDptvc/Dijkstra.gif\" alt=\"Dijkstra's algorithm Animation\" width=\"400\"/>\n",
    "    <figcaption>\n",
    "        Dijkstra's algorithm animation. Gif source: <a href=\"https://en.wikipedia.org/wiki/Dijkstra%27s_algorithm\">Wikipedia</a>\n",
    "    </figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ultimate-tradition",
   "metadata": {},
   "source": [
    "**Time Complexity Analysis**\n",
    "\n",
    "We can analyze each step of this implementation of the algorithm to find its overall time complexity:\n",
    "1. Initializing the auxiliary data structures is done in constant time;\n",
    "2. Getting the current node from the heap can be done with a worst-case time complexity of $O(log(V))$\n",
    "3. Returning the value can happen in constant time, but generally depends on the size of the path, which can never be more than $V$;\n",
    "4. We need to check the cost to all the neighbors of the current vertex to update the priority queue, so this operation will be dependant on the number of edges $O(E)$;\n",
    "5. Repating the steps 2, 3, and 4 until we have found the correct vertex means that we will need to extract the current vertex from a priority queue that in the worst case contains all the vertices, an operation that can be done with a time complexity of $O(log(V))$. In the worst-case scenario, this operation will be repeated $E$ times.\n",
    "\n",
    "The overall worst-case time complexity of this specific implementation of Dijkstra's Algorithm will therefore be $O(|V|) + O(|E| ⋅ log(|V|)) = O(|V| + |E|⋅log(|V|))$. \n",
    "\n",
    "**Space Complexity Analysis**\n",
    "\n",
    "This implementation of the algorithm uses two auxiliary data structures whose length depends on the number of vertices. The space complexity of this algorithm will therefore be $O(V)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "freelance-thousand",
   "metadata": {},
   "source": [
    "<div id=\"a*\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "improving-dayton",
   "metadata": {},
   "source": [
    "### 3.3.4. A* Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "united-collector",
   "metadata": {},
   "source": [
    "The A* Algorithm is an informed search algorithm. An informed search algorithm uses some additional information to determine its best next move. <br/>\n",
    "This additional information is calculated in this case by a function $f(n)=g(n)+h(n)$, where $n$ is the next vertex, $g(n)$ is the cost to get from the start vertex to the next, and $h(n)$ is an heuristic function that approximates the cost to get from $n$ to the vertex to find, and never overestimates the real cost between $n$ and the vertex to find. The algorithm will then pick its next move based on which one has the lowest value of $f(n)$.\n",
    "\n",
    "Here is how this algorithm works:\n",
    "1. Initialize some additional data structures to keep track of: the vertices to visit (starting from the starting vertex); from which adjacent vertex a vertex has been visited (None for the starting vertex); and the cost to get from the starting vertex to that one (0 for the starting vertex). We will call these data structures $a$, $b$, and $c$ respectively for simplicity;\n",
    "2. Set the current vertex to be the one with the lowest value of $f(n)$ in $a$ and remove it from $a$;\n",
    "3. If the current vertex is the vertex to find, reconstruct and return the path to that vertex;\n",
    "4. Otherwise, for each of its neighbors $n$, calculate $g(n)$ and if they are not in $c$, or if they are but this new value of $g(n)$ is lower than the previous one: update its value in $c$, add it to $a$ with priority $f(n)$, and set it as visited from the current node in $b$.\n",
    "5. Repeat steps 2, 3, and 4 until there are elements in $a$ or as long as you don't have found the vertex to find.\n",
    "\n",
    "This algorithm is optimal, which means that it will always find the best solution.\n",
    "\n",
    "Here is a python implementation of the A* algorithm, on a graph where each vertex has a set of spatial coordinates and a list of neighbors and the heuristic function returns the distance between 2 vertices. To keep track of the vertex to visit we will use a Priority Queue implemented with a Min-Heap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "generous-lloyd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['LAX', 'JFK', 'LHR', 'BER'], 'The cost is 573')\n"
     ]
    }
   ],
   "source": [
    "import heapq\n",
    "\n",
    "class PriorityQueue:\n",
    "    def __init__(self):\n",
    "        self.elements = []\n",
    "    \n",
    "    def empty(self):\n",
    "        return not self.elements\n",
    "    \n",
    "    def put(self, item, priority):\n",
    "        heapq.heappush(self.elements, (priority, item))\n",
    "    \n",
    "    def get(self):\n",
    "        return heapq.heappop(self.elements)[1]\n",
    "\n",
    "class Graph():\n",
    "    def __init__(self, graph):\n",
    "        ''' \n",
    "        Initialize a graph, represented as an adjacency list\n",
    "        '''\n",
    "        self.graph = graph\n",
    "    \n",
    "    def heuristic(self, current, to_find):\n",
    "        (x1, y1) = self.graph[current][\"coords\"]\n",
    "        (x2, y2) = self.graph[to_find][\"coords\"]\n",
    "        dist =  abs(x1 - x2)**2 + abs(y1 - y2)**2\n",
    "        return dist\n",
    "    \n",
    "    def astar(self, start, to_find):\n",
    "        to_visit = PriorityQueue()\n",
    "        to_visit.put(start, 0)\n",
    "        # Dictionary in which to each key corresponds the previous vertex\n",
    "        from_v = {}\n",
    "        # Dictionary in which to each key corresponds the cost to get there\n",
    "        cost_to_vertex= {}\n",
    "\n",
    "        # Initialize the start node in both the from_v and cost_to_vertex dicts\n",
    "        from_v[start] = None  # It is none because it is the first node\n",
    "        cost_to_vertex[start] = 0 # It is 0 because it is the first node\n",
    "\n",
    "        while not to_visit.empty():\n",
    "            # Get the vertex with the most priority from the queue\n",
    "            current = to_visit.get()\n",
    "            # If the vertex is the one to find, return the path to that vertex and the cost ot get there\n",
    "            if current == to_find:\n",
    "                path = []\n",
    "                while current != start:\n",
    "                    path.append(current)\n",
    "                    current = from_v[current]\n",
    "                path.append(start)\n",
    "                path.reverse()\n",
    "                return path, \"The cost is \" + str(cost_to_vertex[to_find])\n",
    "            \n",
    "            for neighbor, weight in self.graph[current][\"neighbors\"]:\n",
    "                # determine the new cost\n",
    "                new_cost = cost_to_vertex[current] + weight\n",
    "                # If the vartex hasn't been explored or is being explored from a more convenient path\n",
    "                if neighbor not in cost_to_vertex or new_cost < cost_to_vertex[neighbor]:\n",
    "                    # update the cost\n",
    "                    cost_to_vertex[neighbor] = new_cost\n",
    "                    # calculate the priority\n",
    "                    priority = new_cost + self.heuristic(neighbor, to_find)\n",
    "                    # add it in the queue\n",
    "                    to_visit.put(neighbor, priority)\n",
    "                    # add it to the path, assign it as value the node current, from which we got there\n",
    "                    from_v[neighbor] = current\n",
    "\n",
    "        return \"No path\"\n",
    "\n",
    "\n",
    "graph = {\n",
    "    \"JFK\" : {\"coords\": (40.730, -73.935), \"neighbors\": [(\"LAX\", 92), (\"LHR\", 427), (\"GRU\", 419)]},\n",
    "    \"BER\" : {\"coords\": (52.520, 13.404), \"neighbors\": [(\"FCO\", 33), (\"LHR\", 54), (\"SVO\", 122)]},\n",
    "    \"FCO\" : {\"coords\": (41.773, 12.239), \"neighbors\": [(\"BER\", 33), (\"LHR\", 95), (\"JNB\", 351), (\"DEL\", 358)]},\n",
    "    \"LHR\" : {\"coords\": (51.509, 0.118), \"neighbors\": [(\"JFK\", 427),(\"BER\", 54), (\"FCO\", 95), (\"GRU\", 521)]},\n",
    "    \"LAX\" : {\"coords\": (34.052, -118.24), \"neighbors\": [(\"JFK\", 92), (\"HNL\", 129), (\"ICN\", 567), (\"SYD\", 890)]},\n",
    "    \"GRU\" : {\"coords\" : (-23.588, -46.658), \"neighbors\" : [(\"JFK\", 419), (\"LHR\", 521), (\"JNB\", 545)]},\n",
    "    \"JNB\" : {\"coords\" : (-26.134, 28.240), \"neighbors\" : [(\"FCO\", 351), (\"GRU\", 545), (\"DEL\", 454)]},\n",
    "    \"SVO\" : {\"coords\" : (55.751, 37.618), \"neighbors\" : [(\"BER\", 122), (\"PEK\", 621)]},\n",
    "    \"SYD\" : {\"coords\" : (-33.947, 151.179), \"neighbors\" : [(\"LAX\", 890), (\"ICN\", 554), (\"PEK\", 445), (\"HNL\", 467)]},\n",
    "    \"PEK\" : {\"coords\" : (40.072, 116.597), \"neighbors\" : [(\"SVO\", 621), (\"SYD\", 445), (\"ICN\", 454), (\"DEL\", 779)]},\n",
    "    \"DEL\" : {\"coords\" : (28.644, 77.216), \"neighbors\" : [(\"FCO\", 358), (\"PEK\", 779), (\"JNB\", 454)]},\n",
    "    \"HNL\" : {\"coords\" : (21.315, -157.858), \"neighbors\" : [(\"LAX\", 129), (\"SYD\", 467)]},\n",
    "    \"ICN\" : {\"coords\" : (37.532, 127.024), \"neighbors\" : [(\"LAX\", 567), (\"SYD\", 554), (\"PEK\", 454)]},\n",
    "}\n",
    "g = Graph(graph)\n",
    "print(g.astar(\"LAX\", \"BER\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "trained-scale",
   "metadata": {},
   "source": [
    "This algorithm will terminate for every input, either because it finds the shortest path to a vertex or because no elements are left to visit, and no path exists.\n",
    "\n",
    "This algorithm is correct if it can find the shortest path between 2 vertices in a graph."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "numerical-lawrence",
   "metadata": {},
   "source": [
    "<figure>\n",
    "    <img src=\"https://i.postimg.cc/Qdx5VmqS/A.gif\" alt=\"A* algorithm Animation\" width=\"500\"/>\n",
    "    <figcaption>\n",
    "        A* algorithm animation in a real-world scenario: finding the shortest path from Washington, DC to Los Angeles, CA. Gif source: <a href=\"https://en.wikipedia.org/wiki/A*_search_algorithm#/media/File:A*_Search_Example_on_North_American_Freight_Train_Network.gif\">Wikipedia</a>\n",
    "    </figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interracial-journalism",
   "metadata": {},
   "source": [
    "**Time Complexity Analysis**\n",
    "\n",
    "In general, the time complexity of this algorithm depends on the number of vertices that need to be visited before finding the vertex that we were looking for and thus on the efficiency of the heuristic function.\n",
    "\n",
    "With an uninformative heuristic function, this algorithm will behave like Dijkstra's algorithm, and potentially visit every vertex and edge in the graph, an operation that has a time complexity of $O(|V|+|E|)$. This time complexity can also be expressed as $O(b^{d})$, where $b$ is the average number of adjacent vertices and $d$ is the number of edges between the starting node and the node to find.\n",
    "\n",
    "In the best case for this algorithm, the heuristic function is so efficient that only the nodes in the path between the two vertices are ever explored, its time complexity, in this case, will be $Ω(d)$, where $d$ is the number of edges between the 2 nodes.\n",
    "\n",
    "**Space Complexity Analysis**\n",
    "\n",
    "This Algorithm keeps track of all the vertices it visits, so in the worst case, its space complexity will be of $O(V)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "twenty-speaker",
   "metadata": {},
   "source": [
    "<div id=\"maps\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "north-leader",
   "metadata": {},
   "source": [
    "## 3.4. Graph Algorithms In Action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "developmental-degree",
   "metadata": {},
   "source": [
    "One common use of graph algorithms is finding the shortest route on a street network. In this section, we will look at one way to represent street networks with graphs, how to use the Dijkstra's and the A* algorithms to find the shortest path, and then compare their efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aboriginal-tolerance",
   "metadata": {},
   "source": [
    "<div id=\"networks\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "charged-independence",
   "metadata": {},
   "source": [
    "### 3.4.1. Street Networks In Code "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "subtle-indicator",
   "metadata": {},
   "source": [
    "To understand how we can represent streets with code we need to introduce the concept of network. <br/> While a graph is an abstract mathematical representation of elements and their connections, a network may be thought of as a real-world graph. Networks inherit the terminology of graph theory. [...]. A complex network is one with a nontrivial topology (the configuration and structure of its nodes and edges) – that is, the topology is neither fully regular nor fully random (Boeing, 2017).<br/> According to this definition, a complex network seems like a good choice for our case, but we also want our graph to have nodes and edges ebedded in space. A complex network with nodes and edges embedded in space is called a complex spatial network, and that is what we will use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brutal-boards",
   "metadata": {},
   "source": [
    "We will look at different examples and implementations that are specific to python and Geoff Boeing's [OSMnx library](https://github.com/gboeing/osmnx). There may be other ways to represent and interact with street networks but for the purposes of this portfolio this one should be enough. All the maps are downloaded by the library from [OpenStreetMap](https://www.openstreetmap.org)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mysterious-invalid",
   "metadata": {},
   "source": [
    "A street network is represented by the OSMnx library, or in general by OpenStreetMap, with a series of vertices that represent intersections, the beginning or end of a road, or a change in the topography of a road.<br/>\n",
    "The properties of each vertex include: a unique id, a set of coordinates, the type of road to which it belongs, optionally the max speed (in mph or km/h) and the road name, a list of adjacent vertices with all of their properties and the distance in meters from the current vertex. OSMnx uses another Library, [NetworkX](https://networkx.org), to store the all of these informations.<br/>\n",
    "To make the maps more clear, in the examples that we will see in this section, we will simplify the map view to hide the nodes that are not interesctions or the beginning or end of a road. You can see the difference in the images below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spare-declaration",
   "metadata": {},
   "source": [
    "<div style=\"display:flex; flex-direction:row;\">\n",
    "<figure>\n",
    "    <img src=\"https://i.postimg.cc/nhB8jKcj/Hollywood.png\" alt=\"Unsimplified Street Representation\" width=\"400\"/>\n",
    "    <figcaption>\n",
    "        Unsimplified representation of a street network in Hollywood, CA.\n",
    "    </figcaption>\n",
    "</figure>\n",
    "<figure>\n",
    "    <img src=\"https://i.postimg.cc/jjbpzWL8/Hollywood-simplified.png\" alt=\"Simplified Street Representation\" width=\"400\"/>\n",
    "    <figcaption>\n",
    "        Simplified representation of a street network in Hollywood, CA.\n",
    "    </figcaption>\n",
    "</figure>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "saved-bread",
   "metadata": {},
   "source": [
    "<div id=\"osmnxSearching\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "formal-complex",
   "metadata": {},
   "source": [
    "### 3.4.2. Searching for Paths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "educated-mother",
   "metadata": {},
   "source": [
    "Searching for a path between 2 vertices of a network is not much more complicated than searching for a path between 2 vertices in a graph. However, in this section we will assume that we will start searching for a path by having the ids of the 2 nodes, otherwise if we only had the names of two places, it would be necessary to convert the names to coordinates and then look for the nearest vertices to these in our graph. <br /><br />\n",
    "The OSMnx library makes it really easy to find the shortest path between 2 points with a built-in method, but we are not going to use it. We will instead use the Dijkstra's and the A* algorithms that we analyzed above, with some slight changes to make them interact with the network returned from OSMnx and to return the number of vertices and edges explored by each algorithm. Moreover, we will analyze both algorithm in 2 different scenarios, finding the shortes and the fastest path, before comparing their performances.\n",
    "<br/><br/>\n",
    "Since the code for each one of these algorithms is rather long and cannot run natively on a jupyter notebook, there will be a link to a GitHub Gist with the code and some instructions to run it for each of the algorithms.\n",
    "<br/><br/>\n",
    "All the algorithms will share some similar code, indeed the only code that will change will be the ```Graph``` class. The common code contains:\n",
    "* A ```PriorityQueue``` class, that we have already seen while analyzing the Dijkstra's and A* algorithms;\n",
    "* The configuration for the OSMnx library;\n",
    "* 2 calls to the ```geocode``` method, to get the coordinates of 2 points;\n",
    "* Some code the determine the boundaries of the graph that we are requiring based on the coordinates of the 2 points;\n",
    "* A call to the ```graph_from_bbox``` method, to request the graph (or get if from the caches if already downloaded);\n",
    "* 2 calls to the ```get_nearest_node``` method, to get the nearest nodes to the 2 points;\n",
    "* The initialization of the ```Graph``` Class;\n",
    "* A call to either the ```dijkstras``` or ```astar``` method of the ```Graph``` class, to compute the shortest path;\n",
    "* A ```print``` statement to show in the console the how many vertices and edges have been explored by the called method;\n",
    "* A call to the ```plot_graph_route``` method to plot the graph downloaded by the library with the route computed by on eof my algorithms.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "detailed-harrison",
   "metadata": {},
   "source": [
    "<div id=\"shortestDistance\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "personal-president",
   "metadata": {},
   "source": [
    "### 3.4.2.1. Shortest Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "disciplinary-bunny",
   "metadata": {},
   "source": [
    "Finding the shortest path means using the distance between the vertices as the cost to get there. Because the vertices store the distance between them and their neighbors, retrieving the cose is really simple and does not require additional operations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spiritual-pleasure",
   "metadata": {},
   "source": [
    "**With Dijkstra's Algorithm**\n",
    "\n",
    "> [Code on GitHub](https://gist.github.com/thatsddr/af9c419118575a9aa59c52b942643dc1)\n",
    "\n",
    "We can see from the code that the algorithm is really similar to the one we analyzed before. The only thing that differs is the way the neighbors and the cost to get to a vertex are retrieved, but only because it is working with a different kind of graph. This algorithm is also the one used by default by NetworkX, and for extent by OSMnx, to calculate the shortest path on weighted graphs, as we can see on the library's [source code](https://networkx.org/documentation/networkx-1.10/_modules/networkx/algorithms/shortest_paths/generic.html#shortest_path).\n",
    "\n",
    "Because the only 2 operations that changed (getting the neighbors and determining the cose to reach it) require the same time complexity as the the operations in the example of Dijkstra's algorithm that we analyzed before, the time complexity of this algorithm will also be $O(|V| + |E|⋅log(|V|))$.\n",
    "\n",
    "<b>With A* Algorithm</b>\n",
    "\n",
    "> [Code on GitHub](https://gist.github.com/thatsddr/f1fa54d355bc7562bcfd42791d93a43a)\n",
    "\n",
    "We can see that the code of this algorithm is also really similar to the one we analyzed before, but the difference here is not only in the different way to get the neighbors and their cost, but also in the heuristic function. The heuristic in this case also calculates the distance, but not in meters instead of coordinates units. To do this, we use the [Haversine Formula](https://rosettacode.org/wiki/Haversine_formula).\n",
    "\n",
    "The only 2 operation that changed here are the same that changed in Dijkstra's algorithm, but we have seen that the time complexity of this algorithm depends on its heuristic funciton. Like we saw while analyzing this algorithm, its worst-case time complexity remains $O(b^{d})$.\n",
    "\n",
    "**Results**\n",
    "\n",
    "If we run both scripts, the paths found by the two algorithms on an example route from Cupertino, CA to Mountain View, CA, will be the following:\n",
    "<div style=\"display:flex; flex-direction:row;\">\n",
    "<figure>\n",
    "    <img src=\"https://i.postimg.cc/90g2bcvD/DDistance.png\" alt=\"Result of Dijkstra's Algorithm with distance as cost\" width=\"400\"/>\n",
    "    <figcaption>\n",
    "        The path found by Dijkstra's algorithm.\n",
    "    </figcaption>\n",
    "</figure>\n",
    "<figure>\n",
    "    <img src=\"https://i.postimg.cc/k4xDTRcB/A-Distance.png\" alt=\"Result of the A* Algorithm with distance as cost\" width=\"400\"/>\n",
    "    <figcaption>\n",
    "        The path found by the A* algorithm.\n",
    "    </figcaption>\n",
    "</figure>\n",
    "</div>\n",
    "\n",
    "The path found by the two algorithms is the same but let's take a look at the output of both algorithms in the terminal:\n",
    "\n",
    "<div>\n",
    "<figure>\n",
    "    <img src=\"https://i.postimg.cc/DZgnWr6d/Dijkstra-DTerminal.png\" alt=\"Result of Dijkstra's Algorithm with distance as cost\" width=\"1000\"/>\n",
    "    <figcaption>\n",
    "        Terminal Algorithm of Dijkstra's algorithm.\n",
    "    </figcaption>\n",
    "</figure>\n",
    "<figure>\n",
    "    <img src=\"https://i.postimg.cc/d3Pr1KWT/A-DTerminal.png\" alt=\"Result of the A* Algorithm with distance as cost\" width=\"1000\"/>\n",
    "    <figcaption>\n",
    "        Terminal Output of the A* algorithm.\n",
    "    </figcaption>\n",
    "</figure>\n",
    "</div>\n",
    "\n",
    "We can see that the A* Algorithms explored 56.58% less vertices and 55.33% less edges. We can see here that the A* algorithm can really be more efficient than Dijkstra's, but still this is not the best it can do."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tutorial-tennessee",
   "metadata": {},
   "source": [
    "<div id=\"shortestTime\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "written-mainstream",
   "metadata": {},
   "source": [
    "### 3.4.2.2. Fastest Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interstate-communications",
   "metadata": {},
   "source": [
    "To find the shortest path we consider the time needed to travel between two vertices as the cost to get to a vertex. However, the time needed to reach an adjacent vertex is not among the properties of our nodes. We therefore need to calculate it using the ```distance``` property, which can be easily retrieved, and either the ```maxspeed``` property (for a more precise result) or the ```highway``` property, which returns the road type and we can estimate the maximum speed allowed on that using some default values. If for some reason both properties are not present, we will use a default value. Once we have the distance and the speed, we can easily calculate the time.\n",
    "\n",
    "\n",
    "Both of the algorithms in this case will store the default speed for different types of road and will have 3 additional methods: ```get_speed```, ```is_mph```, and ```to_ms```. These methods determine the speed of the current road, check if the speed is in miles per hour and convert the speed to meters/second respectively.\n",
    "\n",
    "**With Dijkstra's Algorithm**\n",
    "\n",
    "> [Code on GitHub](https://gist.github.com/thatsddr/30619e345c266b7f4a734b15c0781608)\n",
    "\n",
    "We can see from the code that apart from the new methods and the different way to calculate the cost, this algorithm is also basically the same as the one we analyzed before.\n",
    "\n",
    "What changes here is the way the cost to get to an adjacent vertex is calculated (using the 3 methods), but since these mathods all have a worst-case time complexity of $O(1)$, the overall time complexity of this algorithm will always be $O(|V| + |E|⋅log(|V|))$.\n",
    "\n",
    "<b>With A* Algorithm</b>\n",
    "\n",
    "> [Code on GitHub](https://gist.github.com/thatsddr/3c5c3b6807cd66ba62af6f842445cc4a)\n",
    "\n",
    "This algorithm differs from his shortest-path counterpart because of the methods needed to retirieve and convert the speed, the different way in which the cost to get to a vertex is calculated, and for the heuristic function. The heuristic function in this algorithm also calculates the distance between 2 points using the haversine function, but it also divides that value by the highest possible value of ```maxspeed``` so that the estimated cost is always an underestimate. In this way, we make sure that the path is always optimal.\n",
    "\n",
    "Like we saw before, the time complexity of the A* algorithm depends on its heuristic and cannot be worst than $O(b^{d})$.\n",
    "\n",
    "**Results**\n",
    "\n",
    "By running both scripts, the paths found by the two algorithms on the same example route from Cupertino, CA to Mountain View, CA, will be the following:\n",
    "\n",
    "<div style=\"display:flex; flex-direction:row;\">\n",
    "<figure>\n",
    "    <img src=\"https://i.postimg.cc/sghzPdK2/DTime.png\" alt=\"Result of Dijkstra's Algorithm with distance as cost\" width=\"400\"/>\n",
    "    <figcaption>\n",
    "        The path found by Dijkstra's algorithm.\n",
    "    </figcaption>\n",
    "</figure>\n",
    "<figure>\n",
    "    <img src=\"https://i.postimg.cc/SRkR3QTV/A-Time.png\" alt=\"Result of the A* Algorithm with distance as cost\" width=\"400\"/>\n",
    "    <figcaption>\n",
    "        The path found by the A* algorithm.\n",
    "    </figcaption>\n",
    "</figure>\n",
    "</div>\n",
    "\n",
    "We can see that in this case the fastest path is different from the shortest path and that, even in this case, both algorithms have found the same path.\n",
    "\n",
    "<div>\n",
    "<figure>\n",
    "    <img src=\"https://i.postimg.cc/63L5rJWZ/Dijkstra-TTerminal.png\" alt=\"Result of Dijkstra's Algorithm with time as cost\" width=\"1000\"/>\n",
    "    <figcaption>\n",
    "        Terminal Algorithm of Dijkstra's algorithm.\n",
    "    </figcaption>\n",
    "</figure>\n",
    "<figure>\n",
    "    <img src=\"https://i.postimg.cc/Hsmb1dcq/A-TTerminal.png\" alt=\"Result of the A* Algorithm with time as cost\" width=\"1000\"/>\n",
    "    <figcaption>\n",
    "        Terminal Output of the A* algorithm.\n",
    "    </figcaption>\n",
    "</figure>\n",
    "</div>\n",
    "\n",
    "By looking at the terminal ouput of both algorithms, however, we can see a smaller difference. The A* Algorithm explored 19.74% less vertices and 18.93% less edges while still finding the same path.<br/>This heuristic function can be improved in different ways, but it is important that its time complexity remains constant. If we really care about the performance of our algorithm but not about its optimality, we could use an heuristic function that does not guarantee optimality (in this case the fastest path) but explores less vertices. If we know the characteristics of the street network, or of a graph in general, we could create an heuristic that prioritizes some paths over others. In this case, we may want to prioritize faster roads, so what I came up with is an heuristic that looks like this: \n",
    "\n",
    "$$h(currentV,finalV,speed) = \\frac{haversine(currentV,finalV)}{maxSpeed} * \\frac{thresholdSpeed}{speed}$$\n",
    "\n",
    "Basically this heuristic function calculates how much time it would take to get from the current vertex to the final vertex, and multiply this value by a treshold speed divided by the current speed. In this way, if we set our treshold to $25m/s$ ($90km/h$), all the vertices in roads with a higher max speed will have an even lower priority (which means that they will explored before) because we will multiply the estimate by a number which is less than 1. For all the roads with a max speed less than the treshold, the priority will be higher (and so they will be explored later). This heuristic could lead to overestimates in many different situations, but especially to find two vertices that are really far away, this could make the algorithm really more efficient.\n",
    "Let's try to apply this heuristic to our previous scenario to see what happens:\n",
    "<figure>\n",
    "    <img src=\"https://i.postimg.cc/PfY17Pxb/A-N-OTerminal.png\" alt=\"Result of the non-optimal A* Algorithm with distance as cost\" width=\"1000\"/>\n",
    "    <figcaption>\n",
    "        Terminal Output of the Non-Optimal A* algorithm.\n",
    "    </figcaption>\n",
    "</figure>\n",
    "\n",
    "As we can see, in this case we still get the optimal result and compared to Dijkstra's algorithm it has explored 85.31% less vertices and 86.43% less edges. If we want to run our algorithm on very large maps, it could also make sense to dynamically choose between the two heuristics based on the distance of a vertex from the destination one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "municipal-chain",
   "metadata": {},
   "source": [
    "<div id=\"acknowledgements\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "palestinian-terrain",
   "metadata": {},
   "source": [
    "### 3.4.2.3. Acknowledgemnts on the Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tropical-installation",
   "metadata": {},
   "source": [
    "Some info about the example scenario: the map used in both cases has an area of ~60km$^{2}$ and contains 3791 vertices. The fastest path contains 45 vertices while the shortest 55. \n",
    "\n",
    "Using this info, another way to determine how well the algorithms did could be comparing the number of explored vertices to the number of vertices in the path. We could divide the number of explored vertices by the number of vertices in the path, and the closest the number is to 1.0, the more efficient is the algorithm. Alternatively, we could also divide the number of explored vertices by the numebr of total vertices in the map, In this case, the lower the number, the more efficient the algorithm.\n",
    "\n",
    "The efficiency comparisons are done on the same map but these Algorithms have been tested on a limited amount of maps, for this reason, the performance indicated here is not of scientific precision.\n",
    "\n",
    "The default values for the speed of the road should be different based on the traffic laws of the country/state/administrative region in which the map is located to get more precise estimates of the travel time.\n",
    "\n",
    "The heuristic function of the A* algorithm for the fastest path should use the maximum speed limit available among the roads present in the map to be even more efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minimal-quality",
   "metadata": {},
   "source": [
    "___\n",
    "<br /> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unexpected-phone",
   "metadata": {},
   "source": [
    "<div id=\"sources\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "motivated-familiar",
   "metadata": {},
   "source": [
    "# Sources\n",
    "- Skiena, S. The Algorithm Design Manual. 1998. Springer.\n",
    "- [Udacity - Data Structures & Algorithms in Python](https://classroom.udacity.com/courses/ud513)\n",
    "- [Cambridge Dictionary - Algorithm](https://dictionary.cambridge.org/dictionary/english/algorithm)\n",
    "- [GeeksforGeeks - Space Complexity](https://www.geeksforgeeks.org/g-fact-86/)\n",
    "- [Khan Academy - Asymptotic Notation](https://www.khanacademy.org/computing/computer-science/algorithms/asymptotic-notation/a/asymptotic-notation)\n",
    "- [The Lean Blogs - Some common runtime complexities and their meanings](https://medium.com/learn-with-the-lean-programmer/some-common-runtime-complexities-and-their-meanings-5a2bf4320f48)\n",
    "- [GeeksforGeeks - Design Techniques](https://www.geeksforgeeks.org/algorithms-design-techniques/)\n",
    "- [Geeksforgeeks - Backtracking](https://www.geeksforgeeks.org/backtracking-introduction/)\n",
    "- [Programiz - Master Theorem](https://www.programiz.com/dsa/master-theorem)\n",
    "- [GeeksforGeeks - Array](https://www.geeksforgeeks.org/array-data-structure/)\n",
    "- [GeeksforGeeks - Linked Lists](https://www.geeksforgeeks.org/data-structures/linked-list/)\n",
    "- [GeeksforGeeks - Doubly Linked Lists](https://www.geeksforgeeks.org/doubly-linked-list/)\n",
    "- [GeeksforGeeks - Stacks](https://www.geeksforgeeks.org/stack-data-structure/)\n",
    "- [GeeksforGeeks - Queues](https://www.geeksforgeeks.org/queue-data-structure/)\n",
    "- [CS Dojo - Hash Tables](https://www.youtube.com/watch?v=sfWyugl4JWA&list=PLBZBJbE_rGRV8D7XZ08LK6z-4zPoWzu5H&index=13)\n",
    "- [Ananda Gunawardena - CMU - Hash Table Conflict Resolution](http://www.cs.cmu.edu/~ab/15-121N11/lectures/lecture16.pdf)\n",
    "- [Typeocaml - Height, Depth and Level of a Tree](http://typeocaml.com/2014/11/26/height-depth-and-level-of-a-tree/)\n",
    "- [GeeksforGeeks - Red-Black Trees](https://www.geeksforgeeks.org/red-black-tree-set-1-introduction-2/)\n",
    "- [GeeksforGeeks - Ropes](https://www.geeksforgeeks.org/ropes-data-structure-fast-string-concatenation/)\n",
    "- [Opengenus - Ropes](https://iq.opengenus.org/rope-data-structure/)\n",
    "- [GeeksforGeeks - Heaps](https://www.geeksforgeeks.org/heap-data-structure/)\n",
    "- [HackerRank - Heaps](https://www.youtube.com/watch?v=t0Cq6tVNRBA)\n",
    "- [Tutorialspoint - Graphs](https://www.tutorialspoint.com/data_structures_algorithms/graph_data_structure.htm)\n",
    "- [GeeksforGeeks - Types of Graphs](https://www.geeksforgeeks.org/graph-types-and-applications/)\n",
    "- [BigO complexities [pdf]](http://souravsengupta.com/cds2016/lectures/Complexity_Cheatsheet.pdf)\n",
    "- [GeeksforGeeks - Searching Algorithms](https://www.geeksforgeeks.org/searching-algorithms/)\n",
    "- [CS Dojo - QuickSort](https://www.youtube.com/watch?v=0SkOjNaO1XY&list=PLBZBJbE_rGRV8D7XZ08LK6z-4zPoWzu5H&index=12)\n",
    "- [DeepAI - QuickSort](https://deepai.org/machine-learning-glossary-and-terms/quicksort-algorithm)\n",
    "- [StudyTonight - MergeSort](https://www.studytonight.com/data-structures/merge-sort)\n",
    "- [GeeksforGeeks - HeapSort](https://www.geeksforgeeks.org/heap-sort/)\n",
    "- [University of Maryland - HeapSort Analysis](http://www.cs.umd.edu/~meesh/351/mount/lectures/lect14-heapsort-analysis-part.pdf)\n",
    "- [Back to Back SWE - Counting Sort](https://www.youtube.com/watch?v=1mh2vilbZMg)\n",
    "- [Edd Mann - DFS and BFS](https://eddmann.com/posts/depth-first-search-and-breadth-first-search-in-python/)\n",
    "- [GeeksforGeeks - Dijkstra's Algorithm](https://www.geeksforgeeks.org/dijkstras-algorithm-for-adjacency-list-representation-greedy-algo-8/)\n",
    "- [Stack Abuse - A* Algorithm](https://stackabuse.com/basic-ai-concepts-a-search-algorithm/)\n",
    "- [University of British Columbia - A*](https://www.cs.ubc.ca/~kevinlb/teaching/cs322%20-%202008-9/Lectures/Search5.pdf)\n",
    "- Boeing, G. 2017. “OSMnx: New Methods for Acquiring, Constructing, Analyzing, and Visualizing Complex Street Networks.” Computers, Environment and Urban Systems. 65, 126-139. [doi:10.1016/j.compenvurbsys.2017.05.004](doi:10.1016/j.compenvurbsys.2017.05.004)\n",
    "- [RosettaCode - The Haversine Formula](https://rosettacode.org/wiki/Haversine_formula)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
